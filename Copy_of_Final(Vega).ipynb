{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oapAlkyBO2Ie"
   },
   "source": [
    "Domain Question 1: How do temperature patterns in Chicago evolve during the winter-to-spring transition period?\n",
    "\n",
    "Domain Question 2: How do environmental sensors in Chicago capture spatial variations in temperature?\n",
    "\n",
    "Domain Question 3: What relationships exist between temperature, humidity, and precipitation in Chicago?\n",
    "\n",
    "Domain Question 4: How does soil moisture respond to precipitation events in Chicago?\n",
    "\n",
    "Domain Question 5: How do wind patterns vary across different parts of Chicago, and how do they correlate with temperature?\n",
    "\n",
    "Domain Question 6: How do daily cycles of temperature and humidity change throughout the winter-to-spring transition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (2.2.3)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Collecting geopandas\n",
      "  Using cached geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting altair\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.10.0-cp312-cp312-macosx_12_0_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting pyproj>=3.3.0 (from geopandas)\n",
      "  Downloading pyproj-3.7.1-cp312-cp312-macosx_13_0_x86_64.whl.metadata (31 kB)\n",
      "Collecting shapely>=2.0.0 (from geopandas)\n",
      "  Downloading shapely-2.1.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from altair) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from altair) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair)\n",
      "  Downloading narwhals-1.36.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from altair) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from jsonschema>=3.0->altair) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from jsonschema>=3.0->altair) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from jsonschema>=3.0->altair) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from jsonschema>=3.0->altair) (0.23.1)\n",
      "Requirement already satisfied: certifi in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages (from jinja2->altair) (3.0.2)\n",
      "Using cached pandas-2.2.3-cp312-cp312-macosx_10_9_x86_64.whl (12.5 MB)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-macosx_10_13_x86_64.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-macosx_10_13_x86_64.whl (271 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp312-cp312-macosx_10_13_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-macosx_10_13_x86_64.whl (66 kB)\n",
      "Downloading narwhals-1.36.0-py3-none-any.whl (331 kB)\n",
      "Downloading pillow-11.2.1-cp312-cp312-macosx_10_13_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyogrio-0.10.0-cp312-cp312-macosx_12_0_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading pyproj-3.7.1-cp312-cp312-macosx_13_0_x86_64.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.1.0-cp312-cp312-macosx_10_13_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, shapely, pyproj, pyparsing, pyogrio, pillow, narwhals, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, geopandas, altair\n",
      "Successfully installed altair-5.5.0 contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 geopandas-1.0.1 kiwisolver-1.4.8 matplotlib-3.10.1 narwhals-1.36.0 pandas-2.2.3 pillow-11.2.1 pyogrio-0.10.0 pyparsing-3.2.3 pyproj-3.7.1 shapely-2.1.0 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib geopandas altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_Fys8QYwx9Fs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Success) Block 1: Setup & Imports complete. Directories 'data' and 'specs' ensured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Block 1: Setup & Imports ---\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.dates as mdates\n",
    "import json # Needed for handling GeoJSON dict\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# For better visualization (optional, mainly for matplotlib)\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Import Altair and enable rendering/fusion\n",
    "import altair as alt\n",
    "# Enable VegaFusion for potentially large datasets\n",
    "alt.data_transformers.enable('vegafusion')\n",
    "# For better rendering in Google Colab (if applicable)\n",
    "try:\n",
    "    alt.renderers.enable('colab')\n",
    "except Exception as e:\n",
    "    print(f\"Colab renderer not available: {e}\")\n",
    "    alt.renderers.enable('default')\n",
    "\n",
    "# Ensure necessary directories exist for saving data and specs\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('specs', exist_ok=True)\n",
    "\n",
    "print(\"(Success) Block 1: Setup & Imports complete. Directories 'data' and 'specs' ensured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGaVST8ByA4F",
    "outputId": "cbe38db5-35b2-4155-b3b3-571fcf9f6ec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from API: https://data.cityofchicago.org/resource/ggws-77ih.json?$query=SELECT%20measurement_title,%20measurem...\n",
      "Data successfully retrieved! Rows: 605620\n",
      "Fetching data from API: https://data.cityofchicago.org/resource/ggws-77ih.json?$query=SELECT%20measurement_title%2C%20measur...\n",
      "Data successfully retrieved! Rows: 360895\n",
      "API URL 1 result: Success with 605620 rows\n",
      "API URL 2 result: Success with 360895 rows\n",
      "Block 2: Data Fetching complete. Combined dataset has 916230 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 2: Data Fetching from API ---\n",
    "\n",
    "import urllib.parse\n",
    "import pandas as pd # Ensure pandas is imported here as well\n",
    "\n",
    "def fetch_data(url):\n",
    "    \"\"\"Fetches data from a given JSON API URL.\"\"\"\n",
    "    print(f\"Fetching data from API: {url[:100]}...\") # Print snippet of URL\n",
    "    # After the fetch_data function calls:\n",
    "    try:\n",
    "        df = pd.read_json(url)\n",
    "        print(f\"Data successfully retrieved! Rows: {df.shape[0]}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from {url[:50]}...: {e}\")\n",
    "        return None\n",
    "\n",
    "# API URL 1 (Jan–Mar 2017 - broader selection)\n",
    "api_url1 = (\"https://data.cityofchicago.org/resource/ggws-77ih.json?\"\n",
    "    \"$query=SELECT%20measurement_title,%20measurement_description,%20measurement_type,%20\"\n",
    "    \"measurement_medium,%20measurement_time,%20measurement_value,%20units,%20\"\n",
    "    \"units_abbreviation,%20measurement_period_type,%20data_stream_id,%20resource_id,%20\"\n",
    "    \"measurement_id,%20record_id,%20latitude,%20longitude,%20location%20\"\n",
    "    \"WHERE%20measurement_time%20BETWEEN%20%272017-01-01T00:00:00%27::floating_timestamp%20\"\n",
    "    \"AND%20%272017-03-31T23:59:59%27::floating_timestamp%20\"\n",
    "    \"ORDER%20BY%20measurement_time%20DESC%20NULL%20FIRST,%20data_stream_id%20ASC%20NULL%20LAST%20\"\n",
    "    \"LIMIT%201000000\")\n",
    "\n",
    "# API URL 2 (Jan–Jun 2017 - focused on Temp, Atmosphere, Celsius)\n",
    "# This URL is more specific and might pull less data but target relevant types\n",
    "raw_query2 = \"\"\"\n",
    "SELECT measurement_title, measurement_description, measurement_type, measurement_medium,\n",
    "       measurement_time, measurement_value, units, units_abbreviation,\n",
    "       measurement_period_type, data_stream_id, resource_id, measurement_id, record_id,\n",
    "       latitude, longitude, location\n",
    "WHERE measurement_time BETWEEN '2017-01-01T00:00:00'::floating_timestamp\n",
    "  AND '2017-06-30T23:45:00'::floating_timestamp\n",
    "  AND caseless_contains(units, 'degrees Celsius')\n",
    "  AND caseless_contains(measurement_type, 'Temperature')\n",
    "  AND caseless_contains(measurement_medium, 'atmosphere')\n",
    "ORDER BY data_stream_id ASC NULL LAST\n",
    "LIMIT 400000\n",
    "\"\"\"\n",
    "encoded_query2 = urllib.parse.quote(raw_query2.strip(), safe='') # Use strip() to remove leading/trailing whitespace\n",
    "api_url2 = f\"https://data.cityofchicago.org/resource/ggws-77ih.json?$query={encoded_query2}\"\n",
    "\n",
    "# Fetch data\n",
    "df1 = fetch_data(api_url1)\n",
    "df2 = fetch_data(api_url2)\n",
    "\n",
    "print(f\"API URL 1 result: {'Success with ' + str(len(df1)) + ' rows' if df1 is not None else 'Failed'}\")\n",
    "print(f\"API URL 2 result: {'Success with ' + str(len(df2)) + ' rows' if df2 is not None else 'Failed'}\")\n",
    "\n",
    "# Combine and use df as the base\n",
    "if df1 is not None and df2 is not None:\n",
    "    # The 'location' column from the API is likely a dictionary representation of a point.\n",
    "    # pandas.drop_duplicates can struggle with unhashable types like dictionaries.\n",
    "    # Drop the 'location' column before dropping duplicates.\n",
    "    if 'location' in df1.columns:\n",
    "        df1 = df1.drop(columns=['location'])\n",
    "    if 'location' in df2.columns:\n",
    "        df2 = df2.drop(columns=['location'])\n",
    "\n",
    "    df = pd.concat([df1, df2], ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "elif df1 is not None:\n",
    "    if 'location' in df1.columns:\n",
    "        df1 = df1.drop(columns=['location'])\n",
    "    df = df1.drop_duplicates().reset_index(drop=True)\n",
    "elif df2 is not None:\n",
    "    if 'location' in df2.columns:\n",
    "        df2 = df2.drop(columns=['location'])\n",
    "    df = df2.drop_duplicates().reset_index(drop=True)\n",
    "else:\n",
    "    raise Exception(\"Failed to fetch data from both API URLs. Exiting.\") # Exit if no data is fetched\n",
    "\n",
    "print(f\"Block 2: Data Fetching complete. Combined dataset has {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "29AGHPZRyA0r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 3: Starting initial data cleaning...\n",
      "  No dictionary columns found or converted.\n",
      " (success) Block 3: Initial cleaning complete. Removed 0 duplicate rows. Dataset size: 916230\n",
      "\n",
      "Missing Values Summary after Block 3 Cleaning:\n",
      "measurement_title               0\n",
      "measurement_type                0\n",
      "measurement_medium              0\n",
      "measurement_time                0\n",
      "measurement_value               0\n",
      "units                           0\n",
      "units_abbreviation              0\n",
      "measurement_period_type         0\n",
      "data_stream_id                  0\n",
      "resource_id                     0\n",
      "measurement_id                  0\n",
      "record_id                       0\n",
      "latitude                        0\n",
      "longitude                       0\n",
      "measurement_description    204040\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Block 3: Initial Cleaning, Type Conversion, and Duplicate Removal ---\n",
    "\n",
    "print(\"\\nBlock 3: Starting initial data cleaning...\")\n",
    "\n",
    "# Ensure measurement_time is datetime\n",
    "df['measurement_time'] = pd.to_datetime(df['measurement_time'], errors='coerce')\n",
    "\n",
    "# Convert measurement_value, latitude, longitude to numeric\n",
    "df['measurement_value'] = pd.to_numeric(df['measurement_value'], errors='coerce')\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "# Handle dictionary columns if any remain\n",
    "# Check a sample to confirm if necessary before applying broadly\n",
    "dict_columns = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        if not df[col].dropna().empty:\n",
    "             # Check first non-null value\n",
    "             sample_val = df[col].dropna().iloc[0]\n",
    "             if isinstance(sample_val, dict):\n",
    "                dict_columns.append(col)\n",
    "                print(f\"  Column '{col}' contains dictionary values (sample: {sample_val}). Converting to string.\")\n",
    "\n",
    "if dict_columns:\n",
    "    for col in dict_columns:\n",
    "        # Use apply with isinstance check to handle potential non-dict NaNs safely\n",
    "        df[col] = df[col].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "    print(\"  Converted dictionary columns to strings.\")\n",
    "else:\n",
    "    print(\"  No dictionary columns found or converted.\")\n",
    "\n",
    "\n",
    "# Drop duplicates again after type conversions\n",
    "initial_rows = df.shape[0]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "print(f\" (success) Block 3: Initial cleaning complete. Removed {initial_rows - df.shape[0]} duplicate rows. Dataset size: {len(df)}\")\n",
    "\n",
    "# Check for missing values after initial cleaning\n",
    "print(\"\\nMissing Values Summary after Block 3 Cleaning:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0-oC_Q8TyAyO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 4: Removing rows with missing critical values...\n",
      "Block 4: Removed 0 rows with missing critical values. Cleaned dataset size: 916230\n",
      "(Success) \n",
      "Missing Values Summary after Block 4 Cleaning (df_clean):\n",
      "measurement_title               0\n",
      "measurement_type                0\n",
      "measurement_medium              0\n",
      "measurement_time                0\n",
      "measurement_value               0\n",
      "units                           0\n",
      "units_abbreviation              0\n",
      "measurement_period_type         0\n",
      "data_stream_id                  0\n",
      "resource_id                     0\n",
      "measurement_id                  0\n",
      "record_id                       0\n",
      "latitude                        0\n",
      "longitude                       0\n",
      "measurement_description    204040\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Block 4: Handle Critical Missing Values ---\n",
    "\n",
    "print(\"\\nBlock 4: Removing rows with missing critical values...\")\n",
    "\n",
    "initial_rows = df.shape[0]\n",
    "df_clean = df.dropna(subset=['measurement_time', 'latitude', 'longitude', 'measurement_value', 'measurement_type']).reset_index(drop=True)\n",
    "print(f\"Block 4: Removed {initial_rows - df_clean.shape[0]} rows with missing critical values. Cleaned dataset size: {len(df_clean)}\")\n",
    "\n",
    "# Check for missing values again\n",
    "print(\"(Success) \\nMissing Values Summary after Block 4 Cleaning (df_clean):\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "if df_clean.empty:\n",
    "    raise Exception(\"Cleaned DataFrame is empty after removing critical missing values. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "--33CPBAyAvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 5: Applying temperature sensor corrections...\n",
      "\n",
      "Temperature ranges after correction:\n",
      "  Langley - Cumulus: Weather Station Air Temp: Min=0.00°C, Max=0.00°C, Mean=0.00°C\n",
      "  UI Labs Bioswale - Cumulus: Weather Station Air Temp: Min=0.00°C, Max=26.00°C, Mean=6.00°C\n",
      "  Argyle - Cumulus: Weather Station Air Temp: Min=0.00°C, Max=27.00°C, Mean=5.99°C\n",
      "  Langley - Thunder 1: TM1 Temp Sensor: Min=336.00°C, Max=537.00°C, Mean=438.66°C\n",
      "    WARNING: Temperatures for 'Langley - Thunder 1: TM1 Temp Sensor' may still need adjustment! Range (336.00, 537.00) seems extreme.\n",
      "  Langley - Thunder 1: MK-III Weather Station Temp: Min=0.00°C, Max=0.00°C, Mean=0.00°C\n",
      "  UI Labs Bioswale - Thunder 1: TM1 Temp Sensor: Min=327.00°C, Max=600.00°C, Mean=448.77°C\n",
      "    WARNING: Temperatures for 'UI Labs Bioswale - Thunder 1: TM1 Temp Sensor' may still need adjustment! Range (327.00, 600.00) seems extreme.\n",
      "  UI Labs Bioswale - Thunder 1: MK-III Weather Station Temp: Min=0.00°C, Max=100.00°C, Mean=35.48°C\n",
      "    WARNING: Temperatures for 'UI Labs Bioswale - Thunder 1: MK-III Weather Station Temp' may still need adjustment! Range (0.00, 100.00) seems extreme.\n",
      "  Argyle - Thunder 1: TM1 Temp Sensor: Min=0.00°C, Max=3298.00°C, Mean=3251.45°C\n",
      "    WARNING: Temperatures for 'Argyle - Thunder 1: TM1 Temp Sensor' may still need adjustment! Range (0.00, 3298.00) seems extreme.\n",
      "  Argyle - Thunder 1: MK-III Weather Station Temp: Min=0.00°C, Max=100.00°C, Mean=33.00°C\n",
      "    WARNING: Temperatures for 'Argyle - Thunder 1: MK-III Weather Station Temp' may still need adjustment! Range (0.00, 100.00) seems extreme.\n",
      "(Success) Block 5: Temperature values have been corrected based on sensor types.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 5: Temperature Sensor Correction ---\n",
    "\n",
    "print(\"\\nBlock 5: Applying temperature sensor corrections...\")\n",
    "\n",
    "def correct_temperature_values(df):\n",
    "    \"\"\"\n",
    "    Apply appropriate scaling/conversion to measurement values for Temperature data based on sensor title.\n",
    "    Heuristics and documented conversions are used to convert potential raw/scaled values to Celsius.\n",
    "    \"\"\"\n",
    "    df_corrected = df.copy()\n",
    "    # Ensure measurement_value is numeric before operations and drop NaNs if they appeared somehow\n",
    "    df_corrected['measurement_value'] = pd.to_numeric(df_corrected['measurement_value'], errors='coerce')\n",
    "    df_corrected = df_corrected.dropna(subset=['measurement_value', 'measurement_type']) # Ensure value and type exist\n",
    "\n",
    "    # Use .loc with boolean masks\n",
    "    temp_mask = df_corrected['measurement_type'] == 'Temperature'\n",
    "    # Create a temporary DataFrame for temperature data to avoid SettingWithCopyWarning on the original df_corrected slice\n",
    "    df_temp_only = df_corrected.loc[temp_mask].copy()\n",
    "\n",
    "    if not df_temp_only.empty:\n",
    "        # Ensure 'measurement_title' and 'units' are strings for comparison\n",
    "        df_temp_only['measurement_title'] = df_temp_only['measurement_title'].astype(str)\n",
    "        df_temp_only['units'] = df_temp_only['units'].astype(str)\n",
    "\n",
    "        # Process MK-III Weather Station Temp sensors (assuming scaling issues)\n",
    "        mk_mask = df_temp_only['measurement_title'].str.contains(\"MK-III Weather Station Temp\", na=False)\n",
    "        # Apply scaling heuristics based on value ranges\n",
    "        df_temp_only.loc[mk_mask & (df_temp_only['measurement_value'] > 10000), 'measurement_value'] /= 10000.0\n",
    "        df_temp_only.loc[mk_mask & (df_temp_only['measurement_value'] > 1000) & (df_temp_only['measurement_value'] <= 10000), 'measurement_value'] /= 1000.0\n",
    "        df_temp_only.loc[mk_mask & (df_temp_only['measurement_value'] > 100) & (df_temp_only['measurement_value'] <= 1000), 'measurement_value'] /= 10.0\n",
    "        # Also apply a division by 10 for values potentially indicating mV as per original Matplotlib code comment\n",
    "        # This might overlap with or replace the above heuristics depending on observed data patterns.\n",
    "        # Let's use the condition from the Matplotlib code's correction function, which was based on units containing 'mv'.\n",
    "        mkiii_mv_mask = mk_mask & df_temp_only['units'].str.lower().str.contains('mv', na=False)\n",
    "        # Apply *only* if it looks like an mV reading according to the Matplotlib code's logic\n",
    "        df_temp_only.loc[mkiii_mv_mask, 'measurement_value'] = df_temp_only.loc[mkiii_mv_mask, 'measurement_value'] / 10.0\n",
    "        if mkiii_mv_mask.any():\n",
    "             df_temp_only.loc[mkiii_mv_mask, 'units'] = 'Celsius (Corrected)' # Update units\n",
    "\n",
    "\n",
    "        # Process Cumulus Weather Station Air Temp sensors (assuming scaling issues)\n",
    "        cumulus_mask = df_temp_only['measurement_title'].str.contains(\"Cumulus: Weather Station Air Temp\", na=False)\n",
    "        df_temp_only.loc[cumulus_mask & (df_temp_only['measurement_value'] > 1000), 'measurement_value'] /= 1000.0\n",
    "        df_temp_only.loc[cumulus_mask & (df_temp_only['measurement_value'] > 100) & (df_temp_only['measurement_value'] <= 1000), 'measurement_value'] /= 10.0\n",
    "\n",
    "\n",
    "        # Process TM1 Temp Sensors (assuming mV and convert to Celsius)\n",
    "        tm1_mask = df_temp_only['measurement_title'].str.contains(\"TM1 Temp Sensor\", na=False)\n",
    "        tm1_mv_mask = tm1_mask & df_temp_only['units'].str.lower().str.contains('mv', na=False)\n",
    "        # Apply conversion formula if it looks like an mV reading based on units string\n",
    "        df_temp_only.loc[tm1_mv_mask, 'measurement_value'] = (df_temp_only.loc[tm1_mv_mask, 'measurement_value'] - 400.0) / 19.5\n",
    "        if tm1_mv_mask.any():\n",
    "             df_temp_only.loc[tm1_mv_mask, 'units'] = 'Celsius (Formula Applied)' # Update units\n",
    "\n",
    "        # Re-integrate corrected temperature data into the full DataFrame\n",
    "        df_corrected.loc[temp_mask, :] = df_temp_only.values\n",
    "\n",
    "    # Verify temperature corrections - print a summary after correction\n",
    "    temp_data_check = df_corrected[df_corrected['measurement_type'] == 'Temperature']\n",
    "    if not temp_data_check.empty:\n",
    "        print(\"\\nTemperature ranges after correction:\")\n",
    "        for title in temp_data_check['measurement_title'].unique():\n",
    "            # Ensure title is string for filtering and handle potential NaNs\n",
    "            title_str = str(title) if pd.notna(title) else ''\n",
    "            title_data = temp_data_check[temp_data_check['measurement_title'].astype(str) == title_str]\n",
    "            if not title_data.empty and pd.api.types.is_numeric_dtype(title_data['measurement_value']):\n",
    "                 min_temp = title_data['measurement_value'].min()\n",
    "                 max_temp = title_data['measurement_value'].max()\n",
    "                 mean_temp = title_data['measurement_value'].mean()\n",
    "                 print(f\"  {title_str}: Min={min_temp:.2f}°C, Max={max_temp:.2f}°C, Mean={mean_temp:.2f}°C\")\n",
    "                 if max_temp > 50 or min_temp < -40:\n",
    "                     print(f\"    WARNING: Temperatures for '{title_str}' may still need adjustment! Range ({min_temp:.2f}, {max_temp:.2f}) seems extreme.\")\n",
    "            elif pd.notna(title):\n",
    "                 print(f\"  {title_str}: No valid numeric temperature data after correction.\")\n",
    "    else:\n",
    "         print(\"\\nNo temperature data found after cleaning for correction check.\")\n",
    "\n",
    "\n",
    "    return df_corrected\n",
    "\n",
    "# Apply temperature scaling correction\n",
    "df_clean = correct_temperature_values(df_clean)\n",
    "print(\"(Success) Block 5: Temperature values have been corrected based on sensor types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "T1wXxU-FT-Nt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 6: Preparing spatial data and saving cleaned datasets...\n",
      "  Created GeoDataFrame with 916230 entries.\n",
      "  Saved cleaned data to data/chicago_environmental_data.geojson\n",
      "  Saved cleaned data to data/chicago_environmental_data_clean.csv\n",
      "(Success) Block 6: Spatial data prep and initial saving complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 6: Spatial Data Preparation and Saving Cleaned Data ---\n",
    "\n",
    "print(\"\\nBlock 6: Preparing spatial data and saving cleaned datasets...\")\n",
    "\n",
    "# Generate a GeoPandas DataFrame\n",
    "# Ensure latitude/longitude are numeric and not NaN\n",
    "df_geo_ready = df_clean.dropna(subset=['latitude', 'longitude']).copy()\n",
    "\n",
    "if not df_geo_ready.empty:\n",
    "    geometry = [Point(xy) for xy in zip(df_geo_ready['longitude'], df_geo_ready['latitude'])]\n",
    "    # Assuming original lat/lon are WGS84 (EPSG:4326)\n",
    "    gdf = gpd.GeoDataFrame(df_geo_ready, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    print(f\"  Created GeoDataFrame with {len(gdf)} entries.\")\n",
    "\n",
    "    # Save GeoDataFrame\n",
    "    try:\n",
    "        gdf.to_file('data/chicago_environmental_data.geojson', driver='GeoJSON')\n",
    "        print(\"  Saved cleaned data to data/chicago_environmental_data.geojson\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Could not save GeoDataFrame to file: {e}\")\n",
    "else:\n",
    "    print(\"  Skipping GeoDataFrame creation due to missing or invalid latitude/longitude in cleaned data.\")\n",
    "    gdf = None\n",
    "\n",
    "\n",
    "# Save the main cleaned data (CSV format is often useful)\n",
    "try:\n",
    "    df_clean.to_csv('data/chicago_environmental_data_clean.csv', index=False)\n",
    "    print(\"  Saved cleaned data to data/chicago_environmental_data_clean.csv\")\n",
    "except Exception as e:\n",
    "     print(f\"  Could not save cleaned DataFrame to CSV: {e}\")\n",
    "\n",
    "print(\"(Success) Block 6: Spatial data prep and initial saving complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWMUPYd8ZwWb"
   },
   "source": [
    " Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3OPExaQAyAtD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 7: Preparing data for Monthly Temperature Boxplots (V1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fj/whb8vkvn2zdd445kstpdx3fw0000gn/T/ipykernel_924/776797464.py:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[True False False ... True True True]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  range_mask[month_specific_mask] = temp_data_filtered_v1.loc[month_specific_mask, 'measurement_value'].between(min_t, max_t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Filtered temperature data for boxplot: removed 83553 outliers based on monthly ranges.\n",
      "  Sampling boxplot data from 277342 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fj/whb8vkvn2zdd445kstpdx3fw0000gn/T/ipykernel_924/776797464.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  temp_data_for_boxplot = temp_data_for_boxplot.groupby('month_name').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sampled down to 178906 rows.\n",
      "  Saved data for V1 (Monthly Boxplots) to data/monthly_temp_for_boxplot.json\n",
      "Block 7: Data prep for V1 complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 7: Data Prep for V1 (Monthly Temperature Boxplots) ---\n",
    "\n",
    "print(\"\\nBlock 7: Preparing data for Monthly Temperature Boxplots (V1)...\")\n",
    "\n",
    "import calendar\n",
    "month_names_map = {i: calendar.month_name[i] for i in range(1, 13)}\n",
    "\n",
    "# Stricter filtering based on *plausible* monthly ranges after sensor correction\n",
    "month_temp_ranges_strict = {\n",
    "    1: (-25.0, 10.0), 2: (-22.0, 15.0), 3: (-15.0, 25.0),\n",
    "    4: (-5.0, 30.0),  5: (0.0, 35.0),   6: (5.0, 38.0)\n",
    "}\n",
    "\n",
    "# Filter temperature data for relevant months\n",
    "temp_data_filtered_v1 = df_clean[\\\n",
    "    (df_clean['measurement_type'] == 'Temperature') &\\\n",
    "    (df_clean['measurement_medium'].astype(str).str.lower() == 'atmosphere') &\\\n",
    "    (df_clean['units'].astype(str).str.lower().str.contains('celsius', na=False)) &\\\n",
    "    (df_clean['measurement_time'].dt.month >= 3) &\\\n",
    "    (df_clean['measurement_time'].dt.month <= 6)\\\n",
    "].copy()\n",
    "\n",
    "if temp_data_filtered_v1.empty:\n",
    "     print(\"  No temperature data found for months 3-6 after cleaning. Skipping V1 prep.\")\n",
    "     temp_data_for_boxplot = pd.DataFrame()\n",
    "else:\n",
    "    temp_data_filtered_v1['month'] = temp_data_filtered_v1['measurement_time'].dt.month\n",
    "    temp_data_filtered_v1['month_name'] = temp_data_filtered_v1['month'].map(month_names_map)\n",
    "\n",
    "    # Apply stricter range filtering per month\n",
    "    initial_v1_rows = len(temp_data_filtered_v1)\n",
    "    valid_indices = []\n",
    "    # Apply filter using a boolean mask for performance\n",
    "    month_mask = temp_data_filtered_v1['month'].isin(month_temp_ranges_strict.keys())\n",
    "    # Create a range mask, default to True for months not in strict ranges\n",
    "    range_mask = pd.Series(True, index=temp_data_filtered_v1.index)\n",
    "\n",
    "    for m, (min_t, max_t) in month_temp_ranges_strict.items():\n",
    "        month_specific_mask = temp_data_filtered_v1['month'] == m\n",
    "        range_mask[month_specific_mask] = temp_data_filtered_v1.loc[month_specific_mask, 'measurement_value'].between(min_t, max_t)\n",
    "\n",
    "    # Combine filters\n",
    "    temp_data_for_boxplot = temp_data_filtered_v1.loc[range_mask].copy()\n",
    "\n",
    "    print(f\"  Filtered temperature data for boxplot: removed {initial_v1_rows - len(temp_data_for_boxplot)} outliers based on monthly ranges.\")\n",
    "\n",
    "    # Sample data for performance if necessary (Altair has limits on data size for inline/URL)\n",
    "    max_boxplot_points = 200000 # Set a threshold\n",
    "    if len(temp_data_for_boxplot) > max_boxplot_points:\n",
    "        print(f\"  Sampling boxplot data from {len(temp_data_for_boxplot)} rows...\")\n",
    "        # Stratified sample by month to keep distributions\n",
    "        temp_data_for_boxplot = temp_data_for_boxplot.groupby('month_name').apply(\n",
    "            lambda x: x.sample(min(len(x), int(max_boxplot_points / temp_data_for_boxplot['month_name'].nunique())), random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "        print(f\"  Sampled down to {len(temp_data_for_boxplot)} rows.\")\n",
    "\n",
    "\n",
    "# Save data for V1\n",
    "if not temp_data_for_boxplot.empty:\n",
    "    try:\n",
    "        # Only save the columns needed for the chart\n",
    "        temp_data_for_boxplot[['measurement_time', 'measurement_value', 'month_name']].to_json('data/monthly_temp_for_boxplot.json', orient='records')\n",
    "        print(\"  Saved data for V1 (Monthly Boxplots) to data/monthly_temp_for_boxplot.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save data for V1: {e}\")\n",
    "else:\n",
    "     print(\"  No data to save for V1 after filtering.\")\n",
    "\n",
    "\n",
    "print(\"Block 7: Data prep for V1 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRe02PG2aPrr"
   },
   "source": [
    "Create Vega-Lite Spatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WiAuxtETZo80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 8: Preparing data for Linked Choropleth & Bars (V2)...\n",
      "  Loaded local data/chicago_neighborhoods.json\n",
      "  Processing temperature data for neighborhoods...\n",
      "  Filtering temperature data from df_clean...\n",
      "  Found 411232 valid temperature readings\n",
      "  No temperature data for January\n",
      "  No temperature data for February\n",
      "  Processing 100622 readings for March\n",
      "  Added temperature data for 2 neighborhoods in March\n",
      "  Processing 113323 readings for April\n",
      "  Added temperature data for 2 neighborhoods in April\n",
      "  Processing 116866 readings for May\n",
      "  Added temperature data for 2 neighborhoods in May\n",
      "  Processing 80421 readings for June\n",
      "  Added temperature data for 2 neighborhoods in June\n",
      "  Successfully saved 8 records to data/neighborhood_temps.json\n",
      "  Prepared neighborhoods GeoJSON for Altair\n",
      "\n",
      "--- DEBUG: Final Status ---\n",
      "neighborhoods is None? False\n",
      "Number of neighborhoods: 77\n",
      "'community' in neighborhoods.columns? True\n",
      "df_clean is empty? False\n",
      "Number of temperature readings: 461364\n",
      "neighborhood_temps_df exists? True\n",
      "neighborhood_temps_df is empty? False\n",
      "Number of rows in neighborhood_temps_df: 8\n",
      "neighborhoods_geo_dict exists? True\n",
      "neighborhoods_geo_dict has features? 77\n",
      "  Final check: data/neighborhood_temps.json contains 8 records\n",
      "Block 8: Data prep for V2 complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 8: Data Prep for V2 (Linked Choropleth & Bars) ---\n",
    "\n",
    "print(\"\\nBlock 8: Preparing data for Linked Choropleth & Bars (V2)...\")\n",
    "\n",
    "# Load neighborhoods GeoJSON\n",
    "neighborhoods_geojson_path = \"data/chicago_neighborhoods.json\"\n",
    "try:\n",
    "    neighborhoods = gpd.read_file(neighborhoods_geojson_path)\n",
    "    print(f\"  Loaded local {neighborhoods_geojson_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not load local {neighborhoods_geojson_path}: {e}\")\n",
    "    neighborhoods = None\n",
    "\n",
    "neighborhood_temps_json_path = \"data/neighborhood_temps.json\"\n",
    "\n",
    "if neighborhoods is not None and 'community' in neighborhoods.columns and not df_clean.empty:\n",
    "    print(\"  Processing temperature data for neighborhoods...\")\n",
    "    \n",
    "    # Create simple list for neighborhood temperatures\n",
    "    all_neighborhood_temps = []\n",
    "    \n",
    "    # Define months for processing\n",
    "    months = {1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\"}\n",
    "    \n",
    "    # Filter temperature data - only include valid numeric measurements\n",
    "    print(\"  Filtering temperature data from df_clean...\")\n",
    "    temp_data = df_clean[\n",
    "        (df_clean['measurement_type'] == 'Temperature') & \n",
    "        (df_clean['measurement_medium'].astype(str).str.lower() == 'atmosphere') &\n",
    "        (df_clean['latitude'].notna()) & \n",
    "        (df_clean['longitude'].notna())\n",
    "    ].copy()\n",
    "    \n",
    "    temp_data['measurement_value'] = pd.to_numeric(temp_data['measurement_value'], errors='coerce')\n",
    "    temp_data = temp_data.dropna(subset=['measurement_value'])\n",
    "    \n",
    "    print(f\"  Found {len(temp_data)} valid temperature readings\")\n",
    "    \n",
    "    if not temp_data.empty:\n",
    "        # Convert measurement_time to datetime\n",
    "        temp_data['measurement_time'] = pd.to_datetime(temp_data['measurement_time'])\n",
    "        \n",
    "        # Create geometry for spatial join\n",
    "        temp_data['geometry'] = [Point(xy) for xy in zip(temp_data['longitude'], temp_data['latitude'])]\n",
    "        temp_gdf = gpd.GeoDataFrame(temp_data, geometry='geometry', crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Prepare neighborhoods for join\n",
    "        neighborhoods_crs = neighborhoods.to_crs(temp_gdf.crs)\n",
    "        \n",
    "        # Process each month\n",
    "        for month, month_name in months.items():\n",
    "            month_data = temp_gdf[temp_gdf['measurement_time'].dt.month == month]\n",
    "            \n",
    "            if len(month_data) > 0:\n",
    "                print(f\"  Processing {len(month_data)} readings for {month_name}\")\n",
    "                \n",
    "                try:\n",
    "                    joined = gpd.sjoin(month_data, neighborhoods_crs, how='inner', predicate='within')\n",
    "                    \n",
    "                    if not joined.empty:\n",
    "                        # Group by community and calculate statistics\n",
    "                        stats = joined.groupby('community')['measurement_value'].agg(['mean', 'min', 'max']).reset_index()\n",
    "                        stats.columns = ['community', 'mean_temp', 'min_temp', 'max_temp']\n",
    "                        \n",
    "                        stats['month'] = month\n",
    "                        stats['month_name'] = month_name\n",
    "                        \n",
    "                        stats['mean_temp'] = stats['mean_temp'].round(1)\n",
    "                        stats['min_temp'] = stats['min_temp'].round(1)\n",
    "                        stats['max_temp'] = stats['max_temp'].round(1)\n",
    "                        \n",
    "\n",
    "                        all_neighborhood_temps.extend(stats.to_dict('records'))\n",
    "                        print(f\"  Added temperature data for {len(stats)} neighborhoods in {month_name}\")\n",
    "                    else:\n",
    "                        print(f\"  No neighborhoods matched for {month_name} after spatial join\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in spatial join for {month_name}: {e}\")\n",
    "            else:\n",
    "                print(f\"  No temperature data for {month_name}\")\n",
    "    \n",
    "    # Save results\n",
    "    if all_neighborhood_temps:\n",
    "        try:\n",
    "            with open(neighborhood_temps_json_path, 'w') as f:\n",
    "                json.dump(all_neighborhood_temps, f)\n",
    "            print(f\"  Successfully saved {len(all_neighborhood_temps)} records to {neighborhood_temps_json_path}\")\n",
    "            \n",
    "            # Create DataFrame for future use\n",
    "            neighborhood_temps_df = pd.DataFrame(all_neighborhood_temps)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving temperature data: {e}\")\n",
    "            neighborhood_temps_df = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"  No neighborhood temperature data was generated\")\n",
    "        neighborhood_temps_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"  Missing neighborhoods data or df_clean is empty\")\n",
    "    neighborhood_temps_df = pd.DataFrame()\n",
    "\n",
    "# Prepare neighborhoods GeoJSON dictionary for Altair\n",
    "if neighborhoods is not None and not neighborhoods.empty:\n",
    "    try:\n",
    "        neighborhoods_geo_dict = json.loads(neighborhoods.to_json())\n",
    "        print(\"  Prepared neighborhoods GeoJSON for Altair\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to prepare neighborhoods GeoJSON dict: {e}\")\n",
    "        neighborhoods_geo_dict = None\n",
    "else:\n",
    "    neighborhoods_geo_dict = None\n",
    "\n",
    "# Print diagnostic information\n",
    "print(\"\\n--- DEBUG: Final Status ---\")\n",
    "print(f\"neighborhoods is None? {neighborhoods is None}\")\n",
    "if neighborhoods is not None:\n",
    "    print(f\"Number of neighborhoods: {len(neighborhoods)}\")\n",
    "    print(f\"'community' in neighborhoods.columns? {'community' in neighborhoods.columns}\")\n",
    "\n",
    "print(f\"df_clean is empty? {df_clean.empty}\")\n",
    "if not df_clean.empty:\n",
    "    temp_count = len(df_clean[df_clean['measurement_type'] == 'Temperature'])\n",
    "    print(f\"Number of temperature readings: {temp_count}\")\n",
    "\n",
    "print(f\"neighborhood_temps_df exists? {'neighborhood_temps_df' in locals()}\")\n",
    "if 'neighborhood_temps_df' in locals():\n",
    "    print(f\"neighborhood_temps_df is empty? {neighborhood_temps_df.empty}\")\n",
    "    if not neighborhood_temps_df.empty:\n",
    "        print(f\"Number of rows in neighborhood_temps_df: {len(neighborhood_temps_df)}\")\n",
    "\n",
    "print(f\"neighborhoods_geo_dict exists? {'neighborhoods_geo_dict' in locals()}\")\n",
    "if 'neighborhoods_geo_dict' in locals() and neighborhoods_geo_dict is not None:\n",
    "    print(f\"neighborhoods_geo_dict has features? {len(neighborhoods_geo_dict.get('features', []))}\")\n",
    "\n",
    "# Check if the JSON file was created\n",
    "try:\n",
    "    with open(neighborhood_temps_json_path, 'r') as f:\n",
    "        check_data = json.load(f)\n",
    "    print(f\"  Final check: {neighborhood_temps_json_path} contains {len(check_data)} records\")\n",
    "except Exception as e:\n",
    "    print(f\"  Final check: Could not read {neighborhood_temps_json_path}: {e}\")\n",
    "\n",
    "print(\"Block 8: Data prep for V2 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka6oM1wXaVN4"
   },
   "source": [
    " Create Linked View Implementation (Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HNchFVdcZ3Qb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 9: Preparing daily aggregated data (Temp, Humid, Precip, Soil) for V3, V5...\n",
      "  Scaling soil moisture down from max 638.5\n",
      "  Saved data for V3 (Temp/Humid/Precip/Scatter) to data/daily_env_march.json\n",
      "  Saved data for V5 (Soil/Precip Time Series) to data/daily_soil_precip.json\n",
      "Block 9: Daily aggregated data prep complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fj/whb8vkvn2zdd445kstpdx3fw0000gn/T/ipykernel_924/2048527475.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_env_combined['precipitation'].fillna(0, inplace=True) # Assume 0 precip if missing\n"
     ]
    }
   ],
   "source": [
    "# --- Block 9: Data Prep for Daily Aggregations (Temp, Humid, Precip, Soil) ---\n",
    "\n",
    "print(\"\\nBlock 9: Preparing daily aggregated data (Temp, Humid, Precip, Soil) for V3, V5...\")\n",
    "\n",
    "# Filter relevant data types from df_clean\n",
    "temp_data_daily = df_clean[df_clean['measurement_type'] == 'Temperature'].copy()\n",
    "humid_data_daily = df_clean[df_clean['measurement_type'] == 'RelativeHumidity'].copy()\n",
    "precip_data_daily = df_clean[df_clean['measurement_type'] == 'CumulativePrecipitation'].copy()\n",
    "soil_data_daily = df_clean[df_clean['measurement_type'] == 'SoilMoisture'].copy()\n",
    "\n",
    "# Ensure measurement_value is numeric for all relevant subsets before grouping\n",
    "for data_subset in [temp_data_daily, humid_data_daily, precip_data_daily, soil_data_daily]:\n",
    "     if not data_subset.empty:\n",
    "        data_subset['measurement_value'] = pd.to_numeric(data_subset['measurement_value'], errors='coerce')\n",
    "        data_subset.dropna(subset=['measurement_value'], inplace=True) # Drop rows where conversion failed or was NaN\n",
    "\n",
    "# Check if essential data subsets are empty\n",
    "if temp_data_daily.empty: print(\"  Warning: No Temperature data after filtering.\")\n",
    "if humid_data_daily.empty: print(\"  Warning: No RelativeHumidity data after filtering.\")\n",
    "if precip_data_daily.empty: print(\"  Warning: No CumulativePrecipitation data after filtering.\")\n",
    "if soil_data_daily.empty: print(\"  Warning: No SoilMoisture data after filtering.\")\n",
    "\n",
    "# Group by day - Use dt.normalize() for consistency (removes time part)\n",
    "if not temp_data_daily.empty: temp_data_daily['day'] = temp_data_daily['measurement_time'].dt.normalize()\n",
    "if not humid_data_daily.empty: humid_data_daily['day'] = humid_data_daily['measurement_time'].dt.normalize()\n",
    "if not precip_data_daily.empty: precip_data_daily['day'] = precip_data_daily['measurement_time'].dt.normalize()\n",
    "if not soil_data_daily.empty: soil_data_daily['day'] = soil_data_daily['measurement_time'].dt.normalize()\n",
    "\n",
    "# Aggregate daily values (mean) - handle potential empty dataframes\n",
    "daily_temp_agg = temp_data_daily.groupby('day')['measurement_value'].mean().reset_index() if not temp_data_daily.empty else pd.DataFrame(columns=['day', 'measurement_value'])\n",
    "daily_humid_agg = humid_data_daily.groupby('day')['measurement_value'].mean().reset_index() if not humid_data_daily.empty else pd.DataFrame(columns=['day', 'measurement_value'])\n",
    "daily_soil_agg = soil_data_daily.groupby('day')['measurement_value'].mean().reset_index() if not soil_data_daily.empty else pd.DataFrame(columns=['day', 'measurement_value'])\n",
    "\n",
    "# Precipitation Handling (Calculate daily change per sensor, then sum)\n",
    "if not precip_data_daily.empty:\n",
    "    sensor_daily_precip_agg = precip_data_daily.groupby(['data_stream_id', 'day'])['measurement_value'].max().reset_index()\n",
    "    sensor_daily_precip_agg = sensor_daily_precip_agg.sort_values(by=['data_stream_id', 'day'])\n",
    "    sensor_daily_precip_agg['daily_change'] = sensor_daily_precip_agg.groupby('data_stream_id')['measurement_value'].diff().fillna(0)\n",
    "    # Handle resets: Set negative diff values to 0 (assuming resets are to 0 or a low value)\n",
    "    sensor_daily_precip_agg.loc[sensor_daily_precip_agg['daily_change'] < 0, 'daily_change'] = 0\n",
    "\n",
    "    # Aggregate daily change across sensors\n",
    "    daily_precip_agg = sensor_daily_precip_agg.groupby('day')['daily_change'].sum().reset_index()\n",
    "    daily_precip_agg['daily_change'] = daily_precip_agg['daily_change'] / 25.4 # mm to inches\n",
    "    daily_precip_agg.loc[daily_precip_agg['daily_change'] > 3, 'daily_change'] = np.nan # Remove extreme spikes\n",
    "else:\n",
    "     daily_precip_agg = pd.DataFrame(columns=['day', 'daily_change'])\n",
    "     print(\"  Skipping daily precipitation aggregation due to no precipitation data.\")\n",
    "\n",
    "# Convert 'day' columns to datetime for merging (ensure consistency)\n",
    "daily_temp_agg['day'] = pd.to_datetime(daily_temp_agg['day'])\n",
    "daily_humid_agg['day'] = pd.to_datetime(daily_humid_agg['day'])\n",
    "daily_precip_agg['day'] = pd.to_datetime(daily_precip_agg['day'])\n",
    "daily_soil_agg['day'] = pd.to_datetime(daily_soil_agg['day'])\n",
    "\n",
    "# Merge the daily aggregated data into a single DataFrame\n",
    "daily_env_combined = pd.merge(daily_temp_agg, daily_humid_agg, on='day', how='outer', suffixes=('_temp', '_humid'))\n",
    "daily_env_combined = pd.merge(daily_env_combined, daily_precip_agg[['day', 'daily_change']], on='day', how='left')\n",
    "# Merge soil data, ensuring the 'measurement_value' column from soil is kept and renamed\n",
    "daily_env_combined = pd.merge(daily_env_combined, daily_soil_agg[['day', 'measurement_value']], on='day', how='left') # No suffixes needed if measurement_value is only column from soil_agg\n",
    "\n",
    "daily_env_combined.rename(columns={\n",
    "    'measurement_value_temp': 'temperature',\n",
    "    'measurement_value_humid': 'humidity',\n",
    "    'daily_change': 'precipitation',\n",
    "    'measurement_value': 'soil_moisture' # Renaming the measurement_value from soil_agg merge\n",
    "}, inplace=True)\n",
    "\n",
    "daily_env_combined.sort_values(by='day', inplace=True)\n",
    "\n",
    "# Handle potential missing values after outer merge\n",
    "daily_env_combined['precipitation'].fillna(0, inplace=True) # Assume 0 precip if missing\n",
    "\n",
    "# Handle soil moisture scaling/cleaning\n",
    "if 'soil_moisture' in daily_env_combined.columns and not daily_env_combined['soil_moisture'].dropna().empty:\n",
    "    # Optional: Scale down soil moisture if it exceeds 100 based on max observed value\n",
    "    max_soil_val = daily_env_combined['soil_moisture'].max()\n",
    "    if pd.notna(max_soil_val) and max_soil_val > 100:\n",
    "        print(f\"  Scaling soil moisture down from max {max_soil_val:.1f}\")\n",
    "        daily_env_combined['soil_moisture'] = daily_env_combined['soil_moisture'] * (100.0 / max_soil_val)\n",
    "else:\n",
    "    print(\"  Warning: No soil moisture data found for scaling.\")\n",
    "    if 'soil_moisture' not in daily_env_combined.columns: # Ensure column exists even if no data\n",
    "         daily_env_combined['soil_moisture'] = np.nan\n",
    "\n",
    "# Filter for March 2017 for V3 (Temp/Humid/Precip/Scatter)\n",
    "daily_env_march = daily_env_combined[\n",
    "    (daily_env_combined['day'] >= '2017-03-01') & (daily_env_combined['day'] <= '2017-03-31')\n",
    "].copy()\n",
    "\n",
    "# Save data for V3 (March Temp/Humid/Precip/Scatter)\n",
    "if not daily_env_march.empty:\n",
    "    try:\n",
    "        # Only save the columns needed for V3\n",
    "        daily_env_march[['day', 'temperature', 'humidity', 'precipitation']].to_json('data/daily_env_march.json', orient='records')\n",
    "        print(\"  Saved data for V3 (Temp/Humid/Precip/Scatter) to data/daily_env_march.json\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Could not save data for V3: {e}\")\n",
    "else:\n",
    "     print(\"  No data for March after aggregation. Skipping V3 data save.\")\n",
    "\n",
    "# Save data for V5 (Soil/Precip Time Series). Filter out days with no soil moisture data.\n",
    "daily_soil_precip_data = daily_env_combined[['day', 'soil_moisture', 'precipitation']].dropna(subset=['soil_moisture']).copy()\n",
    "if not daily_soil_precip_data.empty:\n",
    "    try:\n",
    "         daily_soil_precip_data.to_json('data/daily_soil_precip.json', orient='records')\n",
    "         print(\"  Saved data for V5 (Soil/Precip Time Series) to data/daily_soil_precip.json\")\n",
    "    except Exception as e:\n",
    "          print(f\"  Could not save data for V5: {e}\")\n",
    "else:\n",
    "     print(\"  No valid soil moisture data after aggregation. Skipping V5 Time Series data save.\")\n",
    "\n",
    "print(\"Block 9: Daily aggregated data prep complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "L5aG_wrCyAqM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 10: Preparing data for Daily Cycles & Trend (V4)...\n",
      "  Aggregated hourly temperature data for 96 month-hour pairs.\n",
      "  Aggregated daily temperature data for 106 days.\n",
      "  Saved data for V4 (Daily Cycles) to data/hourly_temp_cycles.json\n",
      "  Saved data for V4 (Daily Trend) to data/daily_temp_trend.json\n",
      "Block 10: Data prep for V4 complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 10: Data Prep for V4 (Daily & Hourly Temp Cycles & Trend) ---\n",
    "\n",
    "print(\"\\nBlock 10: Preparing data for Daily Cycles & Trend (V4)...\")\n",
    "\n",
    "# Filter Temperature data for months 3-6 from the original df_clean\n",
    "temp_data_v4 = df_clean[\n",
    "    (df_clean['measurement_type'] == 'Temperature') &\n",
    "    (df_clean['measurement_medium'].astype(str).str.lower() == 'atmosphere') &\n",
    "    (df_clean['units'].astype(str).str.lower().str.contains('celsius', na=False)) &\n",
    "    (df_clean['measurement_time'].dt.month >= 3) &\n",
    "    (df_clean['measurement_time'].dt.month <= 6)\n",
    "].copy()\n",
    "\n",
    "if temp_data_v4.empty:\n",
    "    print(\"  No temperature data found for months 3-6. Skipping V4 prep.\")\n",
    "    hourly_agg = pd.DataFrame()\n",
    "    daily_temp_trend_data = pd.DataFrame() # Ensure variables are defined\n",
    "else:\n",
    "    # Extract time fields\n",
    "    temp_data_v4['hour'] = temp_data_v4['measurement_time'].dt.hour\n",
    "    temp_data_v4['month'] = temp_data_v4['measurement_time'].dt.month\n",
    "    temp_data_v4['day'] = temp_data_v4['measurement_time'].dt.normalize() # Daily grouping\n",
    "\n",
    "    # Aggregate data: Calculate hourly mean and standard deviation by month for Cycles chart\n",
    "    hourly_agg = temp_data_v4.groupby(['month', 'hour'])['measurement_value'].agg(\n",
    "        mean_temp='mean',\n",
    "        std_temp='std'\n",
    "    ).reset_index()\n",
    "\n",
    "    if not hourly_agg.empty:\n",
    "        hourly_agg['std_temp'] = hourly_agg['std_temp'].fillna(0) # Replace NaN std (single measurement hour) with 0\n",
    "        hourly_agg['lower_band'] = hourly_agg['mean_temp'] - hourly_agg['std_temp']\n",
    "        hourly_agg['upper_band'] = hourly_agg['mean_temp'] + hourly_agg['std_temp']\n",
    "        hourly_agg['month_name'] = hourly_agg['month'].map(month_names_map)\n",
    "        print(f\"  Aggregated hourly temperature data for {len(hourly_agg)} month-hour pairs.\")\n",
    "    else:\n",
    "         print(\"  No data to aggregate hourly temperature cycles.\")\n",
    "\n",
    "    # Aggregate data: Calculate daily mean, min, max for Trend chart\n",
    "    # Using the same filtered temp_data_v4 as base\n",
    "    daily_temp_trend_data = temp_data_v4.groupby('day')['measurement_value'].agg(['mean', 'min', 'max']).reset_index()\n",
    "    if not daily_temp_trend_data.empty:\n",
    "        daily_temp_trend_data.columns = ['day', 'mean_temp', 'min_temp', 'max_temp']\n",
    "        daily_temp_trend_data['day'] = pd.to_datetime(daily_temp_trend_data['day'])\n",
    "        daily_temp_trend_data['month'] = daily_temp_trend_data['day'].dt.month # Add month for filtering later\n",
    "        daily_temp_trend_data['month_name'] = daily_temp_trend_data['month'].map(month_names_map)\n",
    "        print(f\"  Aggregated daily temperature data for {len(daily_temp_trend_data)} days.\")\n",
    "    else:\n",
    "        print(\"  No data to aggregate daily temperature trend.\")\n",
    "\n",
    "\n",
    "# Save data for V4 Cycles\n",
    "if 'hourly_agg' in globals() and not hourly_agg.empty:\n",
    "    try:\n",
    "        hourly_agg[['month', 'hour', 'mean_temp', 'std_temp', 'lower_band', 'upper_band', 'month_name']].to_json('data/hourly_temp_cycles.json', orient='records')\n",
    "        print(\"  Saved data for V4 (Daily Cycles) to data/hourly_temp_cycles.json\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Could not save data for V4 Daily Cycles: {e}\")\n",
    "else:\n",
    "     print(\"  No data to save for V4 Daily Cycles.\")\n",
    "\n",
    "# Save data for V4 Trend\n",
    "if 'daily_temp_trend_data' in globals() and not daily_temp_trend_data.empty:\n",
    "    try:\n",
    "        daily_temp_trend_data[['day', 'mean_temp', 'min_temp', 'max_temp', 'month_name']].to_json('data/daily_temp_trend.json', orient='records')\n",
    "        print(\"  Saved data for V4 (Daily Trend) to data/daily_temp_trend.json\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Could not save data for V4 Daily Trend: {e}\")\n",
    "else:\n",
    "     print(\"  No data to save for V4 Daily Trend.\")\n",
    "\n",
    "\n",
    "print(\"Block 10: Data prep for V4 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "HSydybHaQBEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 11: Preparing data for Soil Moisture Lag Correlation (V5 Part 2)...\n",
      "  Saved data for V5 Lag Correlation to data/soil_precip_lag_correlation.json\n",
      "Block 11: Data prep for V5 Lag Correlation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 11: Data Prep for V5 Lag Correlation ---\n",
    "\n",
    "print(\"\\nBlock 11: Preparing data for Soil Moisture Lag Correlation (V5 Part 2)...\")\n",
    "\n",
    "# Ensure daily_env_combined exists and has necessary columns\n",
    "if 'daily_env_combined' in globals() and \\\n",
    "   'soil_moisture' in daily_env_combined.columns and \\\n",
    "   'precipitation' in daily_env_combined.columns and \\\n",
    "   not daily_env_combined[['soil_moisture', 'precipitation']].dropna().empty:\n",
    "\n",
    "    max_lag = 7 # Define maximum lag in days\n",
    "\n",
    "    # Use a copy and ensure 'day' is datetime and set as index for reliable shifting\n",
    "    lag_corr_df_base = daily_env_combined[['day', 'soil_moisture', 'precipitation']].dropna(subset=['soil_moisture', 'precipitation']).copy()\n",
    "    if lag_corr_df_base.empty:\n",
    "        print(\"  No overlapping soil moisture and precipitation data for lag correlation.\")\n",
    "        lag_corr_plot_df = pd.DataFrame() # Ensure df is empty\n",
    "    else:\n",
    "        lag_corr_df_base['day'] = pd.to_datetime(lag_corr_df_base['day'])\n",
    "        lag_corr_df_base = lag_corr_df_base.set_index('day').sort_index()\n",
    "\n",
    "        lag_correlations = []\n",
    "        valid_lags = []\n",
    "\n",
    "        for lag in range(max_lag + 1):\n",
    "            # Shift precipitation by the lag amount\n",
    "            corr_df = lag_corr_df_base.copy()\n",
    "            corr_df['precip_lag'] = corr_df['precipitation'].shift(lag)\n",
    "\n",
    "            # Drop any rows that now have NaN due to shifting or original missing values\n",
    "            corr_df_cleaned = corr_df.dropna()\n",
    "\n",
    "            # Need at least 2 data points to calculate correlation\n",
    "            if len(corr_df_cleaned) > 1:\n",
    "                try:\n",
    "                    # Calculate Pearson correlation coefficient\n",
    "                    correlation = np.corrcoef(corr_df_cleaned['soil_moisture'], corr_df_cleaned['precip_lag'])[0, 1]\n",
    "                    lag_correlations.append(correlation)\n",
    "                    valid_lags.append(lag)\n",
    "                except Exception as e:\n",
    "                     print(f\"  Warning: Could not compute correlation for lag {lag}: {e}\")\n",
    "            else:\n",
    "                 # print(f\"  Not enough data points (>1) after cleaning to calculate correlation for lag {lag}. Found {len(corr_df_cleaned)}.\")\n",
    "                 pass # Skip lags with insufficient data silently unless debugging\n",
    "\n",
    "\n",
    "        # Create DataFrame for plotting correlations\n",
    "        lag_corr_plot_df = pd.DataFrame({\n",
    "            'lag': valid_lags,\n",
    "            'correlation': lag_correlations\n",
    "        }).dropna() # Drop any NaN correlations that might have slipped through (unlikely with np.corrcoef if input is valid)\n",
    "\n",
    "    # Save the correlation data\n",
    "    if 'lag_corr_plot_df' in globals() and not lag_corr_plot_df.empty:\n",
    "        try:\n",
    "            lag_corr_plot_df.to_json('data/soil_precip_lag_correlation.json', orient='records')\n",
    "            print(\"  Saved data for V5 Lag Correlation to data/soil_precip_lag_correlation.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not save data for V5 Lag Correlation: {e}\")\n",
    "    else:\n",
    "        print(\"  No data to save for V5 Lag Correlation after calculation.\")\n",
    "\n",
    "else:\n",
    "     print(\"Block 11: Skipping V5 Lag Correlation data prep due to missing daily_env_combined or essential columns/data.\")\n",
    "     lag_corr_plot_df = pd.DataFrame() # Ensure variable is defined\n",
    "\n",
    "print(\"Block 11: Data prep for V5 Lag Correlation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "KAVl48W7yAnh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 12: Preparing data for Temperature vs Wind Speed Scatter Plot (V6)...\n",
      "  Merged daily Temp and Wind data: 40 rows.\n",
      "  Removed 0 rows during IQR filtering.\n",
      "  Removed 40 rows outside March temp range (-8.3°C to 27.8°C).\n",
      "  No data remaining for March after filtering. Cannot proceed with V6 prep.\n",
      "Block 12: Data prep for V6 complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 12: Data Prep for V6 (Temp vs Wind Speed) ---\n",
    "\n",
    "print(\"\\nBlock 12: Preparing data for Temperature vs Wind Speed Scatter Plot (V6)...\")\n",
    "\n",
    "# Filter Wind Speed and Temperature data from df_clean\n",
    "wind_speed_v6 = df_clean[df_clean['measurement_type'] == 'WindSpeed'].copy()\n",
    "temp_data_v6 = df_clean[df_clean['measurement_type'] == 'Temperature'].copy()\n",
    "\n",
    "# Ensure measurement_value is numeric, coercing errors\n",
    "wind_speed_v6['measurement_value'] = pd.to_numeric(wind_speed_v6['measurement_value'], errors='coerce')\n",
    "temp_data_v6['measurement_value'] = pd.to_numeric(temp_data_v6['measurement_value'], errors='coerce')\n",
    "wind_speed_v6.dropna(subset=['measurement_value', 'latitude', 'longitude'], inplace=True) # Need location for grouping\n",
    "temp_data_v6.dropna(subset=['measurement_value', 'latitude', 'longitude'], inplace=True) # Need location for grouping\n",
    "\n",
    "if wind_speed_v6.empty: print(\"  Warning: No WindSpeed data after filtering/cleaning.\")\n",
    "if temp_data_v6.empty: print(\"  Warning: No Temperature data after filtering/cleaning for V6.\")\n",
    "\n",
    "# Group by day AND location (average daily values per sensor/location)\n",
    "if not wind_speed_v6.empty: wind_speed_v6['day'] = wind_speed_v6['measurement_time'].dt.normalize()\n",
    "if not temp_data_v6.empty: temp_data_v6['day'] = temp_data_v6['measurement_time'].dt.normalize()\n",
    "\n",
    "wind_speed_daily = wind_speed_v6.groupby(['day', 'latitude', 'longitude'])['measurement_value'].mean().reset_index() if not wind_speed_v6.empty else pd.DataFrame()\n",
    "temp_daily_v6 = temp_data_v6.groupby(['day', 'latitude', 'longitude'])['measurement_value'].mean().reset_index() if not temp_data_v6.empty else pd.DataFrame()\n",
    "\n",
    "# Convert 'day' columns to datetime\n",
    "if not wind_speed_daily.empty: wind_speed_daily['day'] = pd.to_datetime(wind_speed_daily['day'])\n",
    "if not temp_daily_v6.empty: temp_daily_v6['day'] = pd.to_datetime(temp_daily_v6['day'])\n",
    "\n",
    "# Merge the daily wind speed and temperature data by day and location\n",
    "# Use 'inner' merge to only keep days/locations with both measurements\n",
    "wind_temp = pd.merge(\n",
    "    wind_speed_daily,\n",
    "    temp_daily_v6,\n",
    "    on=['day', 'latitude', 'longitude'],\n",
    "    how='inner', # Ensure we have both temp and wind for each location on each day\n",
    "    suffixes=('_wind', '_temp')\n",
    ")\n",
    "\n",
    "if wind_temp.empty:\n",
    "    print(\"  DataFrame is empty after merging temperature and wind speed data. Cannot proceed with V6 prep.\")\n",
    "else:\n",
    "    print(f\"  Merged daily Temp and Wind data: {len(wind_temp)} rows.\")\n",
    "\n",
    "    # --- IQR Filtering on the Merged Data ---\n",
    "    initial_rows_wind_temp = len(wind_temp)\n",
    "    # Filter temperature outliers\n",
    "    if len(wind_temp['measurement_value_temp']) > 1: # Need at least 2 for quantile\n",
    "        Q1_temp = wind_temp['measurement_value_temp'].quantile(0.25)\n",
    "        Q3_temp = wind_temp['measurement_value_temp'].quantile(0.75)\n",
    "        IQR_temp = Q3_temp - Q1_temp\n",
    "        lower_bound_temp = Q1_temp - 1.5 * IQR_temp\n",
    "        upper_bound_temp = Q3_temp + 1.5 * IQR_temp\n",
    "        wind_temp = wind_temp[(wind_temp['measurement_value_temp'] >= lower_bound_temp) &\\\n",
    "                              (wind_temp['measurement_value_temp'] <= upper_bound_temp)].copy()\n",
    "    else:\n",
    "        print(\"  Not enough Temp data for IQR filtering.\")\n",
    "\n",
    "    # Filter wind speed outliers\n",
    "    if len(wind_temp['measurement_value_wind']) > 1: # Need at least 2 for quantile\n",
    "        Q1_wind = wind_temp['measurement_value_wind'].quantile(0.25)\n",
    "        Q3_wind = wind_temp['measurement_value_wind'].quantile(0.75)\n",
    "        IQR_wind = Q3_wind - Q1_wind\n",
    "        lower_bound_wind = Q1_wind - 1.5 * IQR_wind\n",
    "        upper_bound_wind = Q3_wind + 1.5 * IQR_wind\n",
    "        wind_temp = wind_temp[(wind_temp['measurement_value_wind'] >= lower_bound_wind) &\\\n",
    "                              (wind_temp['measurement_value_wind'] <= upper_bound_wind)].copy()\n",
    "    else:\n",
    "        print(\"  Not enough Wind data for IQR filtering.\")\n",
    "\n",
    "    print(f\"  Removed {initial_rows_wind_temp - len(wind_temp)} rows during IQR filtering.\")\n",
    "\n",
    "    # Check if data remains after IQR filtering\n",
    "    if wind_temp.empty:\n",
    "        print(\"  DataFrame is empty after IQR filtering. Cannot proceed with V6 prep.\")\n",
    "    else:\n",
    "        # --- Filter Data to Only Focus on March ---\n",
    "        wind_temp['month'] = wind_temp['day'].dt.month\n",
    "        wind_temp = wind_temp[wind_temp['month'] == 3].copy() # Filter for March (month 3)\n",
    "\n",
    "        # --- Add missing temperature range manually ---\n",
    "        march_temp_range = (-8.3, 27.8)  # from your previous sensor_ranges_agg for March\n",
    "\n",
    "        if march_temp_range:\n",
    "            min_temp, max_temp = march_temp_range\n",
    "            initial_rows_march = len(wind_temp)\n",
    "            wind_temp = wind_temp[(wind_temp['measurement_value_temp'] >= min_temp) &\\\n",
    "                                  (wind_temp['measurement_value_temp'] <= max_temp)].copy()\n",
    "            print(f\"  Removed {initial_rows_march - len(wind_temp)} rows outside March temp range ({min_temp}°C to {max_temp}°C).\")\n",
    "\n",
    "        # Check if data remains for March\n",
    "        if wind_temp.empty:\n",
    "            print(\"  No data remaining for March after filtering. Cannot proceed with V6 prep.\")\n",
    "        else:\n",
    "            # Create a field for the date labels directly in the dataframe\n",
    "            wind_temp['date_label'] = wind_temp['day'].dt.strftime('%m/%d')\n",
    "\n",
    "            # Save data for V6\n",
    "            try:\n",
    "                # Only save necessary columns\n",
    "                wind_temp[['day', 'latitude', 'longitude', 'measurement_value_temp', 'measurement_value_wind', 'date_label']].to_json('data/temp_wind_daily.json', orient='records')\n",
    "                print(\"  Saved data for V6 (Temp vs Wind) to data/temp_wind_daily.json\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not save data for V6: {e}\")\n",
    "\n",
    "print(\"Block 12: Data prep for V6 complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "w0lcpfirxWCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 13: Generating Altair spec for V1 (Monthly Boxplots)...\n",
      "  Saved V1 (Monthly Boxplot) spec to specs/monthly_boxplot.json\n",
      "Block 13: V1 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 13: Generate Altair Spec for V1 (Monthly Boxplots) ---\n",
    "\n",
    "print(\"\\nBlock 13: Generating Altair spec for V1 (Monthly Boxplots)...\")\n",
    "\n",
    "# Data is loaded in JS from 'data/monthly_temp_for_boxplot.json'\n",
    "# Define order and color scale based on *expected* months (3-6) for consistency,\n",
    "# but make sure they are present in the data if possible for the sort.\n",
    "# Using the list from Block 7's data prep.\n",
    "month_order_v1 = [month_names_map[m] for m in range(3, 7) if m in range(3,7)] # Define full range\n",
    "color_scheme_v1 = alt.Scale(domain=month_order_v1, range=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']) # Range length should match domain if possible\n",
    "\n",
    "# Create the boxplot specification using a data URL\n",
    "boxplot_v1 = alt.Chart(alt.Data(url='data/monthly_temp_for_boxplot.json')).mark_boxplot(\n",
    "    extent='min-max', # equivalent to showfliers=False\n",
    "    median={'color': 'white'} # White line for median\n",
    ").encode(\n",
    "    x=alt.X('month_name:N', title='Month', sort=month_order_v1), # Sort by defined order\n",
    "    y=alt.Y('measurement_value:Q', title='Temperature (°C)'),\n",
    "    color=alt.Color('month_name:N', scale=color_scheme_v1, legend=None), # Color by month, no legend needed\n",
    "    tooltip=[\n",
    "        alt.Tooltip('month_name:N', title='Month'),\n",
    "        # Note: Tooltips on boxplots in Vega-Lite often show aggregated stats implicitly or need explicit aggregation in transform\n",
    "        # Basic tooltip below might show raw value if data is not aggregated before plotting\n",
    "        alt.Tooltip('measurement_value:Q', title='Temperature (°C)', format='.1f')\n",
    "    ]\n",
    ").properties(\n",
    "    title='Monthly Temperature Distribution in Chicago (Mar-Jun 2017)'\n",
    ")\n",
    "\n",
    "# Save the chart specification\n",
    "try:\n",
    "    boxplot_v1.save('specs/monthly_boxplot.json')\n",
    "    print(\"  Saved V1 (Monthly Boxplot) spec to specs/monthly_boxplot.json\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not save V1 spec: {e}\")\n",
    "\n",
    "print(\"Block 13: V1 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "p2VNvdUUyAkX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 14: Generating Altair spec for V2 (Linked Choropleth/Bars)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fj/whb8vkvn2zdd445kstpdx3fw0000gn/T/ipykernel_924/1457717338.py:11: AltairDeprecationWarning: \n",
      "Deprecated since `altair=5.0.0`. Use selection_point instead.\n",
      "  selection_v2 = alt.selection_single(fields=[\"community\"], empty=\"none\", on=\"click\", name=\"sel_community_v2\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GetAttrExpression' object has no attribute 'month_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     11\u001b[39m selection_v2 = alt.selection_single(fields=[\u001b[33m\"\u001b[39m\u001b[33mcommunity\u001b[39m\u001b[33m\"\u001b[39m], empty=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m, on=\u001b[33m\"\u001b[39m\u001b[33mclick\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33msel_community_v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Choropleth Map (using InlineData for the GeoJSON features)\u001b[39;00m\n\u001b[32m     14\u001b[39m choropleth_v2 = alt.Chart(alt.InlineData(values=neighborhoods_geo_dict[\u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m])).mark_geoshape(\n\u001b[32m     15\u001b[39m     stroke=\u001b[33m'\u001b[39m\u001b[33mwhite\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     16\u001b[39m     strokeWidth=\u001b[32m0.5\u001b[39m\n\u001b[32m     17\u001b[39m ).encode(\n\u001b[32m     18\u001b[39m     color=alt.condition(\n\u001b[32m     19\u001b[39m         selection_v2, \u001b[38;5;66;03m# Condition based on the selection\u001b[39;00m\n\u001b[32m     20\u001b[39m         alt.Color(\u001b[33m'\u001b[39m\u001b[33mproperties.mean_temp:Q\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     21\u001b[39m                   title=\u001b[33m'\u001b[39m\u001b[33mMean Temperature (°C)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m                   \u001b[38;5;66;03m# Use a diverging scheme centered around a plausible April temp midpoint\u001b[39;00m\n\u001b[32m     23\u001b[39m                   scale=alt.Scale(scheme=\u001b[33m'\u001b[39m\u001b[33mredblue\u001b[39m\u001b[33m'\u001b[39m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m, domainMid=\u001b[32m10\u001b[39m)), \u001b[38;5;66;03m# Adjust midpoint as needed\u001b[39;00m\n\u001b[32m     24\u001b[39m         alt.value(\u001b[33m'\u001b[39m\u001b[33mlightgray\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Color for non-selected neighborhoods\u001b[39;00m\n\u001b[32m     25\u001b[39m     ),\n\u001b[32m     26\u001b[39m     opacity=alt.condition(selection_v2, alt.value(\u001b[32m1.0\u001b[39m), alt.value(\u001b[32m0.7\u001b[39m)), \u001b[38;5;66;03m# Highlight selected\u001b[39;00m\n\u001b[32m     27\u001b[39m     tooltip=[\n\u001b[32m     28\u001b[39m         alt.Tooltip(\u001b[33m'\u001b[39m\u001b[33mproperties.community:N\u001b[39m\u001b[33m'\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mNeighborhood\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     29\u001b[39m         alt.Tooltip(\u001b[33m'\u001b[39m\u001b[33mproperties.mean_temp:Q\u001b[39m\u001b[33m'\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mMean Temp (°C)\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33m.1f\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     30\u001b[39m         alt.Tooltip(\u001b[33m'\u001b[39m\u001b[33mproperties.min_temp:Q\u001b[39m\u001b[33m'\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mMin Temp (°C)\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33m.1f\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     31\u001b[39m         alt.Tooltip(\u001b[33m'\u001b[39m\u001b[33mproperties.max_temp:Q\u001b[39m\u001b[33m'\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mMax Temp (°C)\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33m.1f\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m     ]\n\u001b[32m     33\u001b[39m ).transform_lookup(\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Lookup from the *external* neighborhood_temps.json data source\u001b[39;00m\n\u001b[32m     35\u001b[39m     lookup=\u001b[33m'\u001b[39m\u001b[33mproperties.community\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;66;03m# Match lookup field in GeoJSON features\u001b[39;00m\n\u001b[32m     36\u001b[39m     from_=alt.LookupData(\n\u001b[32m     37\u001b[39m         data=alt.Data(url=\u001b[33m'\u001b[39m\u001b[33mdata/neighborhood_temps.json\u001b[39m\u001b[33m'\u001b[39m), \u001b[38;5;66;03m# Use data source URL\u001b[39;00m\n\u001b[32m     38\u001b[39m         key=\u001b[33m'\u001b[39m\u001b[33mcommunity\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;66;03m# Match key field in the lookup data\u001b[39;00m\n\u001b[32m     39\u001b[39m         fields=[\u001b[33m'\u001b[39m\u001b[33mmean_temp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmin_temp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmax_temp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmonth_name\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# Fields to bring into the GeoJSON data\u001b[39;00m\n\u001b[32m     40\u001b[39m     ),\n\u001b[32m     41\u001b[39m     as_=[\u001b[33m'\u001b[39m\u001b[33mproperties.mean_temp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproperties.min_temp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproperties.max_temp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproperties.month_name\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# How to name the new fields\u001b[39;00m\n\u001b[32m     42\u001b[39m ).transform_calculate(\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Create top-level field 'community' from properties for selection (Vega-Lite specific need for selections)\u001b[39;00m\n\u001b[32m     44\u001b[39m     community=\u001b[33m\"\u001b[39m\u001b[33mdatum.properties.community\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m ).transform_filter(\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Filter map to show only April data for this specific view (matches report's focus)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43malt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmonth_name\u001b[49m == \u001b[33m'\u001b[39m\u001b[33mApril\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     48\u001b[39m ).add_params(\n\u001b[32m     49\u001b[39m     selection_v2 \u001b[38;5;66;03m# Add the selection parameter to the chart\u001b[39;00m\n\u001b[32m     50\u001b[39m ).properties(\n\u001b[32m     51\u001b[39m     title=\u001b[33m'\u001b[39m\u001b[33mMean Temperature by Chicago Neighborhood (April)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     52\u001b[39m )\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Bar Chart (using the same lookup data source but filtered by selection)\u001b[39;00m\n\u001b[32m     56\u001b[39m bar_chart_v2 = alt.Chart(alt.Data(url=\u001b[33m'\u001b[39m\u001b[33mdata/neighborhood_temps.json\u001b[39m\u001b[33m'\u001b[39m)).transform_filter(\n\u001b[32m     57\u001b[39m      \u001b[38;5;66;03m# Filter data source by Month AND filter by map selection\u001b[39;00m\n\u001b[32m     58\u001b[39m     (alt.datum.month_name == \u001b[33m'\u001b[39m\u001b[33mApril\u001b[39m\u001b[33m'\u001b[39m) & selection_v2\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     )\n\u001b[32m     84\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Caskroom/miniforge/base/lib/python3.12/site-packages/altair/utils/schemapi.py:1089\u001b[39m, in \u001b[36mSchemaBase.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m   1088\u001b[39m     _getattr = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'GetAttrExpression' object has no attribute 'month_name'"
     ]
    }
   ],
   "source": [
    "# --- Block 14: Generate Altair Spec for V2 (Linked Choropleth & Bars) ---\n",
    "\n",
    "print(\"\\nBlock 14: Generating Altair spec for V2 (Linked Choropleth/Bars)...\")\n",
    "\n",
    "# Data sources are data/chicago_neighborhoods.json (for map shapes) and data/neighborhood_temps.json (for lookup)\n",
    "# neighborhoods_geo_dict was prepared in Block 8\n",
    "\n",
    "if 'neighborhoods_geo_dict' in globals() and neighborhoods_geo_dict and \\\n",
    "   'neighborhood_temps_df' in globals() and not neighborhood_temps_df.empty: # Ensure necessary data exists\n",
    "\n",
    "    selection_v2 = alt.selection_single(fields=[\"community\"], empty=\"none\", on=\"click\", name=\"sel_community_v2\")\n",
    "\n",
    "    # Choropleth Map (using InlineData for the GeoJSON features)\n",
    "    choropleth_v2 = alt.Chart(alt.InlineData(values=neighborhoods_geo_dict['features'])).mark_geoshape(\n",
    "        stroke='white',\n",
    "        strokeWidth=0.5\n",
    "    ).encode(\n",
    "        color=alt.condition(\n",
    "            selection_v2, # Condition based on the selection\n",
    "            alt.Color('properties.mean_temp:Q',\n",
    "                      title='Mean Temperature (°C)',\n",
    "                      # Use a diverging scheme centered around a plausible April temp midpoint\n",
    "                      scale=alt.Scale(scheme='redblue', reverse=True, domainMid=10)), # Adjust midpoint as needed\n",
    "            alt.value('lightgray') # Color for non-selected neighborhoods\n",
    "        ),\n",
    "        opacity=alt.condition(selection_v2, alt.value(1.0), alt.value(0.7)), # Highlight selected\n",
    "        tooltip=[\n",
    "            alt.Tooltip('properties.community:N', title='Neighborhood'),\n",
    "            alt.Tooltip('properties.mean_temp:Q', title='Mean Temp (°C)', format='.1f'),\n",
    "            alt.Tooltip('properties.min_temp:Q', title='Min Temp (°C)', format='.1f'),\n",
    "            alt.Tooltip('properties.max_temp:Q', title='Max Temp (°C)', format='.1f')\n",
    "        ]\n",
    "    ).transform_lookup(\n",
    "        # Lookup from the *external* neighborhood_temps.json data source\n",
    "        lookup='properties.community', # Match lookup field in GeoJSON features\n",
    "        from_=alt.LookupData(\n",
    "            data=alt.Data(url='data/neighborhood_temps.json'), # Use data source URL\n",
    "            key='community', # Match key field in the lookup data\n",
    "            fields=['mean_temp', 'min_temp', 'max_temp', 'month_name'] # Fields to bring into the GeoJSON data\n",
    "        ),\n",
    "        as_=['properties.mean_temp', 'properties.min_temp', 'properties.max_temp', 'properties.month_name'] # How to name the new fields\n",
    "    ).transform_calculate(\n",
    "        # Create top-level field 'community' from properties for selection (Vega-Lite specific need for selections)\n",
    "        community=\"datum.properties.community\"\n",
    "    ).transform_filter(\n",
    "        # Filter map to show only April data for this specific view (matches report's focus)\n",
    "        alt.datum.properties.month_name == 'April'\n",
    "    ).add_params(\n",
    "        selection_v2 # Add the selection parameter to the chart\n",
    "    ).properties(\n",
    "        title='Mean Temperature by Chicago Neighborhood (April)'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Bar Chart (using the same lookup data source but filtered by selection)\n",
    "    bar_chart_v2 = alt.Chart(alt.Data(url='data/neighborhood_temps.json')).transform_filter(\n",
    "         # Filter data source by Month AND filter by map selection\n",
    "        (alt.datum.month_name == 'April') & selection_v2\n",
    "    ).transform_fold(\n",
    "        fold=['min_temp', 'mean_temp', 'max_temp'], # Metrics to fold\n",
    "        as_=['temp_metric', 'temp_value'] # New columns for folded data\n",
    "    ).mark_bar().encode(\n",
    "        x=alt.X('temp_metric:N', title='Temperature Metric', sort=['min_temp', 'mean_temp', 'max_temp'], # Sort bars logically\n",
    "                axis=alt.Axis(labelAngle=0, labelExpr=\"replace(datum.value, '_temp', '')\")), # Clean up labels\n",
    "        y=alt.Y('temp_value:Q', title='Temperature (°C)', scale=alt.Scale(zero=False)), # Scale adapted to data range\n",
    "        color=alt.Color('temp_metric:N', legend=None, scale=alt.Scale(domain=['min_temp', 'mean_temp', 'max_temp'], range=['lightblue', 'orange', 'firebrick'])), # Specific colors\n",
    "        tooltip=[\n",
    "            alt.Tooltip('community:N', title='Neighborhood'), # 'community' field available after filter\n",
    "            alt.Tooltip('temp_metric:N', title='Metric'),\n",
    "            alt.Tooltip('temp_value:Q', title='Value (°C)', format='.1f')\n",
    "        ]\n",
    "    ).properties(\n",
    "        height=220,\n",
    "        # Dynamic title using expression from selection_v2 params\n",
    "         title=alt.TitleParams(\n",
    "            text=alt.expr(\n",
    "                 # Check if the selection is valid (a neighborhood is selected)\n",
    "                 'isValid(sel_community_v2.community) ? \"Temperature Metrics for \" + sel_community_v2.community[0] : \"Temperature Metrics (Select a Neighborhood)\"'\n",
    "            ),\n",
    "            subtitle=\"Min, Mean, and Max Temperature for April\",\n",
    "            anchor='middle',\n",
    "            offset=10\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Combine the charts vertically\n",
    "    linked_view_v2 = alt.vconcat(\n",
    "        choropleth_v2,\n",
    "        bar_chart_v2,\n",
    "        spacing=25 # Space between the two charts\n",
    "    ).resolve_scale(\n",
    "        color=\"independent\" # Allow each chart to manage its own color scale\n",
    "    ).configure_view(stroke=None) # Remove border around the concatenated view\n",
    "\n",
    "    # Save the chart specification\n",
    "    try:\n",
    "        linked_view_v2.save('specs/choropleth_linked_bars.json')\n",
    "        print(\"  Saved V2 (Linked Choropleth/Bars) spec to specs/choropleth_linked_bars.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V2 spec: {e}\")\n",
    "else:\n",
    "    print(\"Block 14: Skipping V2 spec generation due to missing data (neighborhoods GeoJSON or aggregated temperatures).\")\n",
    "\n",
    "print(\"Block 14: V2 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "MOMJS-UByAhr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 15: Generating Altair spec for V3 (Linked Time Series -> Scatter Filter)...\n",
      "  Saved V3 (Time Series Brush -> Scatter Filter) spec to specs/timeseries_scatter_linked.json\n",
      "Block 15: V3 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 15: Generate Altair Spec for V3 (Linked Time Series -> Scatter Filter) ---\n",
    "\n",
    "print(\"\\nBlock 15: Generating Altair spec for V3 (Linked Time Series -> Scatter Filter)...\")\n",
    "\n",
    "# Data source is data/daily_env_march.json (Daily data for March)\n",
    "# Ensure daily_env_march has data (checked in Block 9)\n",
    "\n",
    "# Define a selection that will capture a time interval on the x-axis\n",
    "time_brush_scatter_link_v3 = alt.selection_interval(encodings=['x'], name='select_time_for_scatter_v3')\n",
    "\n",
    "# Base chart for shared X-axis\n",
    "base_ts_scatter_link_v3 = alt.Chart(alt.Data(url='data/daily_env_march.json')).encode(\n",
    "    x=alt.X('day:T', title='Date (Brush to Select Range)', axis=alt.Axis(format='%a %d', labelAngle=0, grid=True))\n",
    ")\n",
    "\n",
    "# Temperature line (Left Y-axis, Red)\n",
    "temp_line_scatter_link_v3 = base_ts_scatter_link_v3.mark_line(point=False, strokeWidth=2, color='red').encode(\n",
    "    y=alt.Y('temperature:Q', title='Temperature (°C)', axis=alt.Axis(titleColor='red', titlePadding=10))\n",
    ").properties(height=150, title='Temperature and Humidity Over Time (March 2017)') # Add title here\n",
    "\n",
    "# Humidity line (Right Y-axis, Blue)\n",
    "humid_line_scatter_link_v3 = base_ts_scatter_link_v3.mark_line(point=False, strokeWidth=2, color='blue').encode(\n",
    "    y=alt.Y('humidity:Q', title='Relative Humidity (%)', axis=alt.Axis(orient='right', titleColor='blue', titlePadding=10))\n",
    ")\n",
    "\n",
    "# Layer the lines for the time series panel and add the brush selection\n",
    "time_series_panel_scatter_link_v3 = alt.layer(\n",
    "    temp_line_scatter_link_v3,\n",
    "    humid_line_scatter_link_v3\n",
    ").resolve_scale(\n",
    "    y='independent' # Independent Y-axes for temperature and humidity\n",
    ").add_params( # Add the interval selection parameter\n",
    "    time_brush_scatter_link_v3\n",
    ")\n",
    "\n",
    "# Scatter Plot (Temp vs Humidity), filtered by the brush selection\n",
    "scatter_panel_scatter_link_v3 = alt.Chart(alt.Data(url='data/daily_env_march.json')).mark_point(\n",
    "    opacity=0.6,\n",
    "    filled=True,\n",
    "    color='green' # Use a distinct color for scatter points\n",
    ").encode(\n",
    "    x=alt.X('temperature:Q', title='Temperature (°C)', scale=alt.Scale(zero=False)), # Scale adapted\n",
    "    y=alt.Y('humidity:Q', title='Relative Humidity (%)', scale=alt.Scale(zero=False)), # Scale adapted\n",
    "    tooltip=[ # Add tooltips for hovering\n",
    "        alt.Tooltip('day:T', format='%Y-%m-%d', title='Date'),\n",
    "        alt.Tooltip('temperature:Q', format='.1f', title='Temp (°C)'),\n",
    "        alt.Tooltip('humidity:Q', format='.0f', title='Humidity (%)'),\n",
    "        alt.Tooltip('precipitation:Q', format='.2f', title='Precip (in)') # Include precip info\n",
    "    ]\n",
    ").transform_filter( # Filter data based on the brush selection\n",
    "    time_brush_scatter_link_v3\n",
    ").properties(\n",
    "    height=300,\n",
    "    title='Temperature vs. Humidity Relationship (for selected period)' # Dynamic title could be added here based on selection\n",
    ")\n",
    "\n",
    "# Combine the Time Series and Scatter Plot vertically\n",
    "linked_view_v3 = alt.vconcat(\n",
    "    time_series_panel_scatter_link_v3,\n",
    "    scatter_panel_scatter_link_v3,\n",
    "    spacing=15 # Space between the charts\n",
    ").configure_axis( # Global axis configurations\n",
    "    grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    ").configure_title( # Chart title configuration\n",
    "    fontSize=14, anchor='middle'\n",
    ").configure_view( # Remove border around the concatenated view\n",
    "    stroke=None\n",
    ")\n",
    "\n",
    "# Save the chart specification\n",
    "try:\n",
    "    linked_view_v3.save('specs/timeseries_scatter_linked.json')\n",
    "    print(\"  Saved V3 (Time Series Brush -> Scatter Filter) spec to specs/timeseries_scatter_linked.json\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not save V3 spec: {e}\")\n",
    "\n",
    "print(\"Block 15: V3 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nWTEt3kyAfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 16: Generating Altair spec for V4 (Linked Daily Cycles & Trend)...\n",
      "  Saved V4 (Linked Cycles/Trend) spec to specs/cycles_trend_linked.json\n",
      "Block 16: V4 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 16: Generate Altair Spec for V4 (Linked Daily Cycles & Trend) ---\n",
    "\n",
    "print(\"\\nBlock 16: Generating Altair spec for V4 (Linked Daily Cycles & Trend)...\")\n",
    "\n",
    "# Data sources are data/hourly_temp_cycles.json and data/daily_temp_trend.json\n",
    "# Ensure hourly_agg and daily_temp_trend_data had data during prep (checked in Block 10)\n",
    "\n",
    "# Determine the order for months Mar-Jun based on expected months\n",
    "month_order_v4 = [month_names_map[m] for m in range(3, 7)]\n",
    "# Define color scheme for the months\n",
    "color_scheme_v4 = alt.Scale(domain=month_order_v4, range=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']) # Blue, Orange, Green, Red\n",
    "\n",
    "# Define the selection for clicking on a month in the cycles chart/legend\n",
    "month_cycle_select_v4 = alt.selection_point(fields=['month_name'], empty='all', on='click', name='select_month_v4')\n",
    "\n",
    "# Chart 1: Daily Cycles (shows average hourly patterns by month)\n",
    "base_cycle_v4 = alt.Chart(alt.Data(url='data/hourly_temp_cycles.json')).encode(\n",
    "    x=alt.X('hour:O', title='Hour of Day', # Treat hour as Ordinal for discrete steps\n",
    "            axis=alt.Axis(labelAngle=0, values=list(range(0, 24, 2)), grid=True)), # Show labels every 2 hours\n",
    "    color=alt.Color('month_name:N', # Color by month name\n",
    "                    scale=color_scheme_v4, # Use the defined color scheme\n",
    "                    sort=month_order_v4, # Ensure months are sorted correctly\n",
    "                    legend=alt.Legend(title=\"Month (Click Legend/Line)\")), # Add a legend with instructions\n",
    "    opacity=alt.condition(month_cycle_select_v4, alt.value(0.7), alt.value(0.2)) # Fade non-selected months\n",
    ")\n",
    "\n",
    "# Layer 1.1: Error Bands using mark_area (+/- 1 Std Dev)\n",
    "error_bands_cycle_v4 = base_cycle_v4.mark_area(opacity=0.2).encode(\n",
    "    y=alt.Y('lower_band:Q', title='Temperature (°C)', axis=alt.Axis(titlePadding=10)), # Y for lower bound\n",
    "    y2=alt.Y2('upper_band:Q') # Y2 for upper bound\n",
    ")\n",
    "\n",
    "# Layer 1.2: Mean Line\n",
    "mean_line_cycle_v4 = base_cycle_v4.mark_line(point=False, strokeWidth=2).encode( # Point=False to avoid default points\n",
    "    y=alt.Y('mean_temp:Q'), # Y for the mean line\n",
    "    strokeWidth=alt.condition(month_cycle_select_v4, alt.value(4), alt.value(2)), # Make selected line thicker\n",
    "    tooltip=[ # Tooltips for hovering over points/lines\n",
    "        alt.Tooltip('month_name:N', title='Month'),\n",
    "        alt.Tooltip('hour:O', title='Hour'),\n",
    "        alt.Tooltip('mean_temp:Q', format='.1f', title='Avg Temp (°C)'),\n",
    "        alt.Tooltip('std_temp:Q', format='.1f', title='Std Dev (°C)')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine layers for the first chart (Daily Cycles)\n",
    "daily_cycle_chart_linked_v4 = alt.layer(\n",
    "    error_bands_cycle_v4,\n",
    "    mean_line_cycle_v4\n",
    ").add_params( # Add the selection parameter to make this chart interactive\n",
    "    month_cycle_select_v4\n",
    ").properties(\n",
    "    height=300,\n",
    "    title='Daily Temperature Cycles by Month (Select a Month Below)'\n",
    ")\n",
    "\n",
    "\n",
    "# Chart 2: Daily Trend (shows daily averages over the study period)\n",
    "base_trend_filtered_v4 = alt.Chart(alt.Data(url='data/daily_temp_trend.json')).encode(\n",
    "    x=alt.X('day:T', title='Date', axis=alt.Axis(format='%b %d', labelAngle=-45, grid=True)) # Format date nicely\n",
    ").transform_filter( # Filter the data for this chart based on the selection from Chart 1\n",
    "    month_cycle_select_v4\n",
    ")\n",
    "\n",
    "# Layer 2.1: Mean Trend Line\n",
    "line_trend_filtered_v4 = base_trend_filtered_v4.mark_line(\n",
    "    color='#1A759F', # Consistent color for the trend line\n",
    "    strokeWidth=2\n",
    ").encode(\n",
    "    y=alt.Y('mean_temp:Q', title='Daily Avg Temp (°C)', axis=alt.Axis(titlePadding=10))\n",
    ")\n",
    "\n",
    "# Layer 2.2: Points on the Trend Line\n",
    "points_trend_filtered_v4 = base_trend_filtered_v4.mark_point(\n",
    "    filled=True, # Filled points\n",
    "    color='#1A759F', # Same color as the line\n",
    "    size=60 # Point size\n",
    ").encode(\n",
    "    y=alt.Y('mean_temp:Q'), # Y position matches the line\n",
    "    tooltip=[ # Tooltips for points\n",
    "        alt.Tooltip('day:T', title='Date', format='%b %d'),\n",
    "        alt.Tooltip('month_name:N', title='Month'),\n",
    "        alt.Tooltip('mean_temp:Q', title='Avg Temp (°C)', format='.1f'),\n",
    "        alt.Tooltip('min_temp:Q', title='Min Temp (°C)', format='.1f'),\n",
    "        alt.Tooltip('max_temp:Q', title='Max Temp (°C)', format='.1f')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine layers for the second chart (Daily Trend)\n",
    "daily_trend_chart_linked_v4 = alt.layer(\n",
    "    line_trend_filtered_v4,\n",
    "    points_trend_filtered_v4\n",
    ").properties(\n",
    "    height=250,\n",
    "    title='Daily Average Temperatures for Selected Month' # Static title, context from upper chart\n",
    ")\n",
    "\n",
    "# Combine the two charts vertically\n",
    "linked_view_v4 = alt.vconcat(\n",
    "    daily_cycle_chart_linked_v4,\n",
    "    daily_trend_chart_linked_v4,\n",
    "    spacing=20 # Space between charts\n",
    ").resolve_legend( # Ensure legends are handled correctly\n",
    "    color=\"independent\", # Color legend from the top chart\n",
    "    strokeWidth=\"independent\" # Stroke width legend (if used) is also independent\n",
    ").configure_axis( # Global axis configurations\n",
    "    grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    ").configure_title( # Global title configurations\n",
    "    fontSize=14, anchor='middle'\n",
    ").configure_view( # Remove border around the concatenated view\n",
    "    stroke=None\n",
    ").interactive() # Enable zooming/panning on the combined view\n",
    "\n",
    "\n",
    "# Save the chart specification\n",
    "try:\n",
    "    linked_view_v4.save('specs/cycles_trend_linked.json')\n",
    "    print(\"  Saved V4 (Linked Cycles/Trend) spec to specs/cycles_trend_linked.json\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not save V4 spec: {e}\")\n",
    "\n",
    "print(\"Block 16: V4 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bBfvaLmAD6C3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 17: Generating Altair spec for V5 Time Series (Focus + Context)...\n",
      "  Saved V5 (Soil/Precip Time Series) spec to specs/soil_precip_interactive.json\n",
      "Block 17: V5 Time Series spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 17: Generate Altair Spec for V5 Time Series (Focus + Context) ---\n",
    "\n",
    "print(\"\\nBlock 17: Generating Altair spec for V5 Time Series (Focus + Context)...\")\n",
    "\n",
    "# Data source is data/daily_soil_precip.json\n",
    "# Ensure daily_soil_precip_data had data during prep (checked in Block 9)\n",
    "\n",
    "# Define a selection that will capture a time interval on the x-axis of the context chart\n",
    "time_brush_v5 = alt.selection_interval(encodings=['x'], name='time_brush_v5')\n",
    "\n",
    "# Base chart for shared X-axis property (even though scales might resolve independently)\n",
    "base_v5_ts = alt.Chart(alt.Data(url='data/daily_soil_precip.json')).properties(width=700)\n",
    "\n",
    "# Lower Chart: Precipitation Context (with Brush)\n",
    "precip_context_v5 = base_v5_ts.mark_bar(color='steelblue', opacity=0.7).encode(\n",
    "    x=alt.X('day:T', title='Date (Brush to Select Range)', axis=alt.Axis(format='%b %d', grid=True)), # X-axis for context\n",
    "    y=alt.Y('precipitation:Q', title='Daily Precip (in)', axis=alt.Axis(titleColor='steelblue', titlePadding=10)), # Y-axis for precipitation\n",
    "    tooltip=[ # Tooltip for interaction\n",
    "        alt.Tooltip('day:T', format='%Y-%m-%d', title='Date'),\n",
    "        alt.Tooltip('precipitation:Q', format='.2f', title='Precip (in)')\n",
    "    ]\n",
    ").add_params( # Add the brush selection to this chart\n",
    "    time_brush_v5\n",
    ").properties(\n",
    "    height=80, # Make context chart shorter\n",
    "    title=\"Precipitation (Select Date Range Below)\"\n",
    ")\n",
    "\n",
    "# Upper Chart: Soil Moisture Detail (Filtered by Brush)\n",
    "soil_detail_v5 = base_v5_ts.mark_line(point=True, color='saddlebrown', strokeWidth=2).encode(\n",
    "    x=alt.X('day:T', title=None, axis=alt.Axis(labels=False, grid=True)), # Hide x-axis labels, but keep grid\n",
    "    y=alt.Y('soil_moisture:Q', title='Soil Moisture (% Max Scaled)', axis=alt.Axis(titleColor='saddlebrown', titlePadding=10)), # Y-axis for soil moisture\n",
    "    tooltip=[ # Tooltip for interaction\n",
    "        alt.Tooltip('day:T', format='%Y-%m-%d', title='Date'),\n",
    "        alt.Tooltip('soil_moisture:Q', format='.1f', title='Soil Moisture (%)')\n",
    "    ]\n",
    ").transform_filter( # Filter this chart based on the brush selection from the context chart\n",
    "    time_brush_v5\n",
    ").properties(\n",
    "    height=300,\n",
    "    title='Soil Moisture Response'\n",
    ")\n",
    "\n",
    "# Combine the Charts Vertically\n",
    "viz1_interactive_v5 = alt.vconcat(\n",
    "    soil_detail_v5,\n",
    "    precip_context_v5,\n",
    "    spacing=5 # Minimal spacing between focus and context\n",
    ").resolve_scale(\n",
    "    x='independent', # Allow independent x-axis zoom/pan (brush still links them)\n",
    "    y='independent' # Allow independent Y scales for soil and precip\n",
    ").configure_axis( # Global axis configuration\n",
    "    grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    ").configure_title( # Global title configuration\n",
    "    fontSize=14, anchor='middle'\n",
    ").configure_view( # Remove border around individual charts\n",
    "    stroke=None\n",
    ").interactive() # Enable interactivity (pan/zoom) on the combined view\n",
    "\n",
    "\n",
    "# Save the chart specification\n",
    "try:\n",
    "    viz1_interactive_v5.save('specs/soil_precip_interactive.json')\n",
    "    print(\"  Saved V5 (Soil/Precip Time Series) spec to specs/soil_precip_interactive.json\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not save V5 Time Series spec: {e}\")\n",
    "\n",
    "print(\"Block 17: V5 Time Series spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kweR1F9XJwY4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 18: Generating Altair spec for V5 Lag Correlation...\n",
      "  Peak correlation calculated: Peak correlation at 3 day lag: 0.239\n",
      "  Saved V5 (Lag Correlation) spec to specs/soil_precip_lag_correlation.json\n",
      "Block 18: V5 Lag Correlation spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 18: Generate Altair Spec for V5 Lag Correlation ---\n",
    "\n",
    "print(\"\\nBlock 18: Generating Altair spec for V5 Lag Correlation...\")\n",
    "\n",
    "# Data source is data/soil_precip_lag_correlation.json\n",
    "# Ensure lag_corr_plot_df had data during prep (checked in Block 11)\n",
    "\n",
    "if 'lag_corr_plot_df' in globals() and not lag_corr_plot_df.empty:\n",
    "\n",
    "    # Find peak correlation details for annotation\n",
    "    # Handle cases where max correlation might be negative (e.g., if most correlations are negative)\n",
    "    # Find the index of the maximum *absolute* correlation if direction doesn't matter,\n",
    "    # or just the maximum correlation value if positive correlation is expected.\n",
    "    # Let's stick to finding the index of the overall maximum correlation value.\n",
    "    if not lag_corr_plot_df.empty:\n",
    "        peak_idx_v5 = lag_corr_plot_df['correlation'].idxmax()\n",
    "        peak_lag_v5 = lag_corr_plot_df.loc[peak_idx_v5, 'lag']\n",
    "        peak_corr_v5 = lag_corr_plot_df.loc[peak_idx_v5, 'correlation']\n",
    "        peak_text_v5 = f'Peak correlation at {peak_lag_v5} day lag: {peak_corr_v5:.3f}'\n",
    "        print(f\"  Peak correlation calculated: {peak_text_v5}\")\n",
    "    else:\n",
    "        peak_lag_v5 = 0 # Default if no data\n",
    "        peak_corr_v5 = 0\n",
    "        peak_text_v5 = 'No correlation data available'\n",
    "        print(\"  No correlation data available to calculate peak.\")\n",
    "\n",
    "\n",
    "    # Base chart for the correlation plot\n",
    "    base_corr_v5 = alt.Chart(alt.Data(url='data/soil_precip_lag_correlation.json')).properties(\n",
    "         width=500, # Adjust width as needed\n",
    "         height=300, # Adjust height as needed\n",
    "         title='Correlation Between Soil Moisture and Lagged Precipitation'\n",
    "    )\n",
    "\n",
    "    # Line and points representing the correlation values by lag\n",
    "    line_corr_v5 = base_corr_v5.mark_line(point=True, color='#B5179E', strokeWidth=2, size=80).encode(\n",
    "        x=alt.X('lag:O', title='Lag (days)', axis=alt.Axis(labelAngle=0)), # Ordinal axis for discrete lags\n",
    "        y=alt.Y('correlation:Q', title='Correlation Coefficient', scale=alt.Scale(zero=True)), # Ensure scale includes zero\n",
    "        tooltip=[ # Tooltip for interaction\n",
    "            alt.Tooltip('lag:O', title='Lag (days)'),\n",
    "            alt.Tooltip('correlation:Q', format='.3f', title='Correlation')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Horizontal rule at y=0 for reference\n",
    "    zero_rule_v5 = alt.Chart(pd.DataFrame({'y': [0]})).mark_rule(color='black', strokeDash=[3,3], strokeWidth=1).encode(y='y')\n",
    "\n",
    "    # Annotation Text displaying the peak correlation info\n",
    "    # Create a dummy DataFrame for the annotation text position\n",
    "    # Position it relative to the data range or at a fixed point.\n",
    "    # Placing it slightly below the minimum correlation value seems reasonable.\n",
    "    annotation_y_pos = lag_corr_plot_df['correlation'].min() # Start slightly below the minimum\n",
    "    if annotation_y_pos > -0.5: annotation_y_pos = -0.5 # Ensure it's visible if all correlations are high\n",
    "\n",
    "    annotation_df_v5 = pd.DataFrame({'lag': [peak_lag_v5], 'correlation': [annotation_y_pos], 'text': [peak_text_v5]}) # Place near the peak lag, adjusted y\n",
    "\n",
    "    annotation_text_v5 = alt.Chart(annotation_df_v5).mark_text(\n",
    "        align='center', # Center text at the specified point\n",
    "        fontSize=11,\n",
    "        color='black',\n",
    "        dy=-10 # Nudge text up slightly from the point\n",
    "    ).encode(\n",
    "        x=alt.X('lag:O'), # Match encoding type of main plot's x-axis\n",
    "        y=alt.Y('correlation:Q'),\n",
    "        text='text:N'\n",
    "    )\n",
    "\n",
    "    # Layer the chart components\n",
    "    viz2_correlation_v5 = alt.layer(\n",
    "        line_corr_v5,\n",
    "        zero_rule_v5,\n",
    "        annotation_text_v5\n",
    "    ).configure_axis( # Configure axes\n",
    "        grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    "    ).configure_title( # Configure title\n",
    "        fontSize=14, anchor='middle'\n",
    "    ).configure_view( # Remove border\n",
    "        stroke=None\n",
    "    ).interactive() # Enable interaction (pan/zoom)\n",
    "\n",
    "\n",
    "    # Save the chart specification\n",
    "    try:\n",
    "        viz2_correlation_v5.save('specs/soil_precip_lag_correlation.json')\n",
    "        print(\"  Saved V5 (Lag Correlation) spec to specs/soil_precip_lag_correlation.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V5 Lag Correlation spec: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Block 18: Skipping V5 Lag Correlation spec generation due to no data.\")\n",
    "    viz2_correlation_v5 = None # Ensure variable is defined as None if skipped\n",
    "\n",
    "print(\"Block 18: V5 Lag Correlation spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 19: Generating Altair spec for V6 (Temp vs Wind Speed Scatter Plot)...\n",
      "Block 19: Skipping V6 spec generation due to empty wind_temp data after filtering.\n",
      "Block 19: V6 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 19: Generate Altair Spec for V6 (Temp vs Wind Speed Scatter Plot) ---\n",
    "\n",
    "print(\"\\nBlock 19: Generating Altair spec for V6 (Temp vs Wind Speed Scatter Plot)...\")\n",
    "\n",
    "# Data source is data/temp_wind_daily.json\n",
    "# Ensure wind_temp had data during prep (checked in Block 12)\n",
    "\n",
    "if 'wind_temp' in globals() and not wind_temp.empty:\n",
    "\n",
    "    # Calculate Regression and Correlation *before* building chart layers\n",
    "    # Use the data loaded from the JSON file path for consistency if running blocks out of order\n",
    "    try:\n",
    "        wind_temp_data_for_calcs = pd.read_json('data/temp_wind_daily.json')\n",
    "        if not wind_temp_data_for_calcs.empty and \\\n",
    "           pd.api.types.is_numeric_dtype(wind_temp_data_for_calcs['measurement_value_temp']) and \\\n",
    "           pd.api.types.is_numeric_dtype(wind_temp_data_for_calcs['measurement_value_wind']):\n",
    "\n",
    "            # Ensure there's variance for regression\n",
    "            if wind_temp_data_for_calcs['measurement_value_temp'].std() > 0 and wind_temp_data_for_calcs['measurement_value_wind'].std() > 0:\n",
    "                # Fit the linear regression using numpy\n",
    "                z_v6 = np.polyfit(wind_temp_data_for_calcs['measurement_value_temp'], wind_temp_data_for_calcs['measurement_value_wind'], 1)\n",
    "                slope_v6 = z_v6[0]\n",
    "                intercept_v6 = z_v6[1]\n",
    "                trend_text_v6 = f\"Trend: y={slope_v6:.2f}x+{intercept_v6:.2f}\"\n",
    "\n",
    "                # Calculate the correlation coefficient using pandas\n",
    "                correlation_v6 = wind_temp_data_for_calcs['measurement_value_temp'].corr(wind_temp_data_for_calcs['measurement_value_wind'])\n",
    "                corr_text_v6 = f'Corr: {correlation_v6:.2f}'\n",
    "                print(f\"  Calculated V6: {trend_text_v6}, {corr_text_v6}\")\n",
    "            else:\n",
    "                slope_v6, intercept_v6, correlation_v6 = np.nan, np.nan, np.nan\n",
    "                trend_text_v6 = \"Trend: No variance\"\n",
    "                corr_text_v6 = \"Corr: No variance\"\n",
    "                print(\"  Cannot calculate regression/correlation: insufficient variance.\")\n",
    "        else:\n",
    "            slope_v6, intercept_v6, correlation_v6 = np.nan, np.nan, np.nan\n",
    "            trend_text_v6 = \"Trend: N/A\"\n",
    "            corr_text_v6 = \"Corr: N/A\"\n",
    "            print(\"  Cannot calculate regression/correlation: data is not numeric or empty.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"  Data file data/temp_wind_daily.json not found for V6 calculations.\")\n",
    "        slope_v6, intercept_v6, correlation_v6 = np.nan, np.nan, np.nan\n",
    "        trend_text_v6 = \"Trend: Data not found\"\n",
    "        corr_text_v6 = \"Corr: Data not found\"\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not calculate regression/correlation for V6: {e}\")\n",
    "        slope_v6, intercept_v6, correlation_v6 = np.nan, np.nan, np.nan\n",
    "        trend_text_v6 = \"Trend: Error\"\n",
    "        corr_text_v6 = \"Corr: Error\"\n",
    "\n",
    "\n",
    "    # Base chart definition (using data URL)\n",
    "    base_v6 = alt.Chart(alt.Data(url='data/temp_wind_daily.json')).encode(\n",
    "        x=alt.X('measurement_value_temp:Q', title='Temperature (°C)'),\n",
    "        y=alt.Y('measurement_value_wind:Q', title='Wind Speed (m/s)')\n",
    "    )\n",
    "\n",
    "    # Layer 1: Scatter points\n",
    "    points_v6 = base_v6.mark_point(\n",
    "        size=80,\n",
    "        opacity=0.7,\n",
    "        filled=True,\n",
    "        color='darkgreen' # Set color directly\n",
    "    ).encode(\n",
    "        tooltip=[ # Add tooltips for interactivity\n",
    "            alt.Tooltip('day:T', format='%Y-%m-%d', title='Date'),\n",
    "            alt.Tooltip('latitude:Q', format='.2f'),\n",
    "            alt.Tooltip('longitude:Q', format='.2f'),\n",
    "            alt.Tooltip('measurement_value_temp:Q', format='.1f', title='Avg Temp (°C)'),\n",
    "            alt.Tooltip('measurement_value_wind:Q', format='.1f', title='Avg Wind (m/s)'),\n",
    "            alt.Tooltip('date_label:N', title='Date') # Use the pre-formatted label\n",
    "        ]\n",
    "    ).properties(\n",
    "         title='Temperature vs. Wind Speed in March' # Main chart title\n",
    "    )\n",
    "\n",
    "\n",
    "    # Layer 2: Regression line using transform_regression\n",
    "    # transform_regression works directly on the data source defined in the base chart\n",
    "    if pd.notna(slope_v6): # Only add the regression line if calculation was successful\n",
    "        regression_line_v6 = base_v6.mark_line(\n",
    "            color=\"red\",\n",
    "            strokeDash=[3,3], # Dashed line\n",
    "            strokeWidth=2\n",
    "        ).transform_regression(\n",
    "            'measurement_value_temp', # Independent variable (x)\n",
    "            'measurement_value_wind',  # Dependent variable (y)\n",
    "            method='linear'          # Regression method\n",
    "        )\n",
    "    else:\n",
    "        regression_line_v6 = alt.Chart(pd.DataFrame()) # Create an empty chart if regression failed\n",
    "\n",
    "\n",
    "    # Layer 3: Annotation text (Correlation and Trend)\n",
    "    # Position annotations near top-left based on data range from the loaded data\n",
    "    if 'wind_temp_data_for_calcs' in locals() and not wind_temp_data_for_calcs.empty:\n",
    "        temp_min_v6, temp_max_v6 = wind_temp_data_for_calcs['measurement_value_temp'].min(), wind_temp_data_for_calcs['measurement_value_temp'].max()\n",
    "        wind_min_v6, wind_max_v6 = wind_temp_data_for_calcs['measurement_value_wind'].min(), wind_temp_data_for_calcs['measurement_value_wind'].max()\n",
    "\n",
    "        # Calculate positions relative to data range\n",
    "        text_x_pos_v6 = temp_min_v6 + (temp_max_v6 - temp_min_v6) * 0.05 # 5% from left edge\n",
    "        # Position correlation text near the top\n",
    "        text_y_pos_corr_v6 = wind_max_v6 - (wind_max_v6 - wind_min_v6) * 0.05 # 5% from top edge\n",
    "        # Position trend text slightly below correlation text\n",
    "        text_y_pos_trend_v6 = wind_max_v6 - (wind_max_v6 - wind_min_v6) * 0.12 # 12% from top edge\n",
    "\n",
    "        annotation_data_v6 = pd.DataFrame([\n",
    "             {'x': text_x_pos_v6, 'y': text_y_pos_corr_v6, 'text': corr_text_v6},\n",
    "             # Only include trend text if calculated successfully\n",
    "             ] + ([{'x': text_x_pos_v6, 'y': text_y_pos_trend_v6, 'text': trend_text_v6}] if pd.notna(slope_v6) else [])\n",
    "        )\n",
    "\n",
    "        annotation_text_v6 = alt.Chart(annotation_data_v6).mark_text(\n",
    "            align='left', # Align text to the left of the (x,y) point\n",
    "            fontSize=12,\n",
    "            color='black' # Explicit text color for readability\n",
    "        ).encode(\n",
    "            x='x:Q',\n",
    "            y='y:Q',\n",
    "            text='text:N'\n",
    "        )\n",
    "    else:\n",
    "        annotation_text_v6 = alt.Chart(pd.DataFrame()) # Empty chart if no data\n",
    "\n",
    "    # Layer 4: Dummy layer for Legend\n",
    "    # Create data for legend items\n",
    "    legend_data_v6_list = [\n",
    "        {'label': 'March Daily Averages', 'color': 'darkgreen', 'shape': 'circle'},\n",
    "    ]\n",
    "    # Add regression line legend item only if regression succeeded\n",
    "    if pd.notna(slope_v6):\n",
    "         legend_data_v6_list.append({'label': trend_text_v6, 'color': 'red', 'shape': 'stroke'})\n",
    "\n",
    "    legend_data_v6 = pd.DataFrame(legend_data_v6_list)\n",
    "\n",
    "\n",
    "    # Need invisible marks linked to this data to generate the legend items\n",
    "    dummy_legend_v6 = alt.Chart(legend_data_v6).mark_point(\n",
    "        size=0, # Invisible points\n",
    "        opacity=0\n",
    "    ).encode(\n",
    "         # Use a dummy Y encoding that isn't displayed\n",
    "        y=alt.Y('label:N', axis=None),\n",
    "        # Use the specified color and shape directly in the legend definition\n",
    "        color=alt.Color('color:N', scale=None, legend=alt.Legend(title=None)), # Base legend config\n",
    "        shape=alt.Shape('shape:N', scale=alt.Scale(domain=['circle', 'stroke'], range=['circle', 'stroke']), # Define shape mapping\n",
    "                        legend=alt.Legend(title=None, symbolFillColor='color', stroke='color', symbolType='stroke')) # Customize legend appearance\n",
    "    )\n",
    "\n",
    "\n",
    "    # Combine all layers\n",
    "    # Order matters for visibility: points -> regression line -> annotations -> dummy legend\n",
    "    chart_layers_v6 = alt.layer(\n",
    "        points_v6,\n",
    "        regression_line_v6,\n",
    "        annotation_text_v6,\n",
    "        # dummy_legend_v6 # Layering dummy legends can be tricky. Often better managed with resolve_legend below.\n",
    "    ).resolve_legend(\n",
    "        color=alt.LegendResolveMap(color='independent', shape='independent') # Resolve color and shape legends independently\n",
    "        # Configure the legend appearance explicitly outside resolve_legend\n",
    "    ).properties(\n",
    "        width=700, # Adjust size as needed\n",
    "        height=500\n",
    "    ).configure_axis( # Global axis configurations\n",
    "        grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    "    ).configure_legend( # Configure legend appearance\n",
    "        orient='bottom', # Position legend at the bottom\n",
    "        padding=10,\n",
    "        labelFontSize=11,\n",
    "        titleFontSize=12,\n",
    "        # Create custom symbols if needed, or rely on the dummy layer approach\n",
    "        # Using the dummy layer approach for text labels in legend\n",
    "    )\n",
    "\n",
    "    # Add the dummy legend as a separate layer *after* the main layers if needed,\n",
    "    # but resolving legend should be sufficient if the main marks define the symbols.\n",
    "    # Let's try defining the legend properties directly in configure_legend first.\n",
    "    # If the dummy layer is needed to force legend items for things like regression lines:\n",
    "    # chart_layers_v6 = alt.layer(chart_layers_v6, dummy_legend_v6) # Layer the main chart with the dummy legend\n",
    "\n",
    "    # Enable interactivity (panning and zooming)\n",
    "    chart_v6 = chart_layers_v6.interactive()\n",
    "\n",
    "\n",
    "    # Save the chart specification\n",
    "    try:\n",
    "        chart_v6.save('specs/temp_wind_scatter.json')\n",
    "        print(\"  Saved V6 (Temp vs Wind Scatter) spec to specs/temp_wind_scatter.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V6 spec: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Block 19: Skipping V6 spec generation due to empty wind_temp data after filtering.\")\n",
    "    chart_v6 = None # Ensure variable is defined as None if skipped\n",
    "\n",
    "\n",
    "print(\"Block 19: V6 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTmA03HJFjJg"
   },
   "source": [
    "1.) Temperature Evolution During Winter-to-Spring Transition\n",
    "\n",
    "Daily temperature trend line with min/max range\n",
    "Monthly violin plots showing distribution changes\n",
    "\n",
    "\n",
    "2.) Spatial Temperature Variations\n",
    "\n",
    "Monthly temperature maps showing geographic patterns\n",
    "Temperature variability map highlighting areas with greatest fluctuations\n",
    "\n",
    "\n",
    "3.) Environmental Measurement Relationships\n",
    "\n",
    "Temperature-humidity correlation scatter plot with time progression\n",
    "Combined time series of temperature, humidity, and precipitation\n",
    "\n",
    "\n",
    "4.) Soil Moisture Response to Precipitation\n",
    "\n",
    "Time series showing precipitation events and soil moisture response\n",
    "Lag correlation analysis showing delayed response patterns\n",
    "\n",
    "\n",
    "5.) Wind Patterns and Correlations\n",
    "\n",
    "Wind rose diagrams for different Chicago locations\n",
    "Temperature-wind speed relationship scatter plot\n",
    "\n",
    "\n",
    "6.) Daily Cycles and Seasonal Changes\n",
    "\n",
    "Monthly comparison of daily temperature cycles\n",
    "Daily temperature range progression over the study period\n",
    "Hourly temperature heatmap showing daily and seasonal patterns"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "flooding-analysis",
   "language": "python",
   "name": "flooding-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
