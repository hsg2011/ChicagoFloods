{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oapAlkyBO2Ie"
   },
   "source": [
    "Domain Question 1: How do temperature patterns in Chicago evolve during the winter-to-spring transition period?\n",
    "\n",
    "Domain Question 2: How do environmental sensors in Chicago capture spatial variations in temperature?\n",
    "\n",
    "Domain Question 3: What relationships exist between temperature, humidity, and precipitation in Chicago?\n",
    "\n",
    "Domain Question 4: How does soil moisture respond to precipitation events in Chicago?\n",
    "\n",
    "Domain Question 5: How do wind patterns vary across different parts of Chicago, and how do they correlate with temperature?\n",
    "\n",
    "Domain Question 6: How do daily cycles of temperature and humidity change throughout the winter-to-spring transition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_Fys8QYwx9Fs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default enabled.\n",
      "(Success) Block 1: Setup & Imports complete. Directories 'data' and 'specs' ensured.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "# --- Block 1: Setup & Imports ---\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt # Still useful for some settings or quick checks\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import LinearSegmentedColormap # Keep if needed for custom palettes\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.dates as mdates # Keep if needed\n",
    "import json # Needed for handling GeoJSON dict and saving specs\n",
    "import urllib.parse # For encoding API queries\n",
    "import os # For creating directories\n",
    "import calendar # For month names\n",
    "\n",
    "# Set plot style (affects matplotlib if used)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# For better visualization (optional, mainly for matplotlib)\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Import Altair and enable rendering/fusion\n",
    "import altair as alt\n",
    "# Enable VegaFusion for potentially large datasets\n",
    "try:\n",
    "    alt.data_transformers.enable('default')\n",
    "    print(\"default enabled.\")\n",
    "except ImportError:\n",
    "    print(\"VegaFusion not installed, large datasets might be slow. Using default transformer.\")\n",
    "    alt.data_transformers.enable('default')\n",
    "\n",
    "# For better rendering in Google Colab (if applicable)\n",
    "# try:\n",
    "#     alt.renderers.enable('colab')\n",
    "#     print(\"Altair Colab renderer enabled.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Colab renderer not available: {e}. Using default renderer.\")\n",
    "#     alt.renderers.enable('default')\n",
    "\n",
    "# Ensure necessary directories exist for saving data and specs\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('specs', exist_ok=True)\n",
    "\n",
    "# Global map for month numbers to names\n",
    "month_names_map = {i: calendar.month_name[i] for i in range(1, 13)}\n",
    "\n",
    "\n",
    "print(\"(Success) Block 1: Setup & Imports complete. Directories 'data' and 'specs' ensured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGaVST8ByA4F",
    "outputId": "cbe38db5-35b2-4155-b3b3-571fcf9f6ec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from API: https://data.cityofchicago.org/resource/ggws-77ih.json?$query=SELECT%20measurement_title,%20measurem...\n",
      "Data successfully retrieved! Rows: 605620\n",
      "Fetching data from API: https://data.cityofchicago.org/resource/ggws-77ih.json?$query=SELECT%20measurement_title%2C%20measur...\n",
      "Data successfully retrieved! Rows: 360895\n",
      "Combined dataset has 916230 unique rows (removed 50285 duplicates).\n",
      "(Success) Block 2: Data Fetching complete. Base dataset has 916230 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 2: Data Fetching from API ---\n",
    "\n",
    "def fetch_data(url):\n",
    "    \"\"\"Fetches data from a given JSON API URL.\"\"\"\n",
    "    print(f\"Fetching data from API: {url[:100]}...\") # Print snippet of URL\n",
    "    try:\n",
    "        df = pd.read_json(url)\n",
    "        print(f\"Data successfully retrieved! Rows: {df.shape[0]}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from {url[:50]}...: {e}\")\n",
    "        return None\n",
    "\n",
    "# API URL 1 (Jan–Mar 2017 - broader selection, from Original.py)\n",
    "api_url1 = (\n",
    "    \"https://data.cityofchicago.org/resource/ggws-77ih.json?\"\n",
    "    \"$query=SELECT%20measurement_title,%20measurement_description,%20measurement_type,%20\"\n",
    "    \"measurement_medium,%20measurement_time,%20measurement_value,%20units,%20\"\n",
    "    \"units_abbreviation,%20measurement_period_type,%20data_stream_id,%20resource_id,%20\"\n",
    "    \"measurement_id,%20record_id,%20latitude,%20longitude,%20location%20\"\n",
    "    \"WHERE%20measurement_time%20BETWEEN%20%272017-01-01T00:00:00%27::floating_timestamp%20\"\n",
    "    \"AND%20%272017-03-31T23:59:59%27::floating_timestamp%20\"\n",
    "    \"ORDER%20BY%20measurement_time%20DESC%20NULL%20FIRST,%20data_stream_id%20ASC%20NULL%20LAST%20\"\n",
    "    \"LIMIT%201000000\"\n",
    ")\n",
    "\n",
    "# API URL 2 (Jan–Jun 2017 - focused on Temp, Atmosphere, Celsius, from Original.py)\n",
    "raw_query2 = \"\"\"\n",
    "SELECT measurement_title, measurement_description, measurement_type, measurement_medium,\n",
    "       measurement_time, measurement_value, units, units_abbreviation,\n",
    "       measurement_period_type, data_stream_id, resource_id, measurement_id, record_id,\n",
    "       latitude, longitude, location\n",
    "WHERE measurement_time BETWEEN '2017-01-01T00:00:00'::floating_timestamp\n",
    "  AND '2017-06-30T23:45:00'::floating_timestamp\n",
    "  AND caseless_contains(units, 'degrees Celsius')\n",
    "  AND caseless_contains(measurement_type, 'Temperature')\n",
    "  AND caseless_contains(measurement_medium, 'atmosphere')\n",
    "ORDER BY data_stream_id ASC NULL LAST\n",
    "LIMIT 400000\n",
    "\"\"\"\n",
    "encoded_query2 = urllib.parse.quote(raw_query2.strip(), safe='') # Use strip()\n",
    "api_url2 = f\"https://data.cityofchicago.org/resource/ggws-77ih.json?$query={encoded_query2}\"\n",
    "\n",
    "# Fetch data\n",
    "df1 = fetch_data(api_url1)\n",
    "df2 = fetch_data(api_url2)\n",
    "\n",
    "# Combine and use df as the base, dropping 'location' before dropping duplicates\n",
    "# Ensure df exists even if one fetch fails\n",
    "df = pd.DataFrame() # Initialize empty df\n",
    "\n",
    "if df1 is not None:\n",
    "    if 'location' in df1.columns:\n",
    "        df1 = df1.drop(columns=['location'])\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "if df2 is not None:\n",
    "    if 'location' in df2.columns:\n",
    "        df2 = df2.drop(columns=['location'])\n",
    "    df = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "if df.empty:\n",
    "    raise Exception(\"Failed to fetch data from both API URLs. Exiting.\")\n",
    "else:\n",
    "    # Drop duplicates *after* concatenating all available data\n",
    "    initial_combined_rows = len(df)\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"Combined dataset has {len(df)} unique rows (removed {initial_combined_rows - len(df)} duplicates).\")\n",
    "\n",
    "print(f\"(Success) Block 2: Data Fetching complete. Base dataset has {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "29AGHPZRyA0r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 3: Starting initial data cleaning and type conversion...\n",
      "  No dictionary columns found requiring conversion.\n",
      " (success) Block 3: Initial cleaning/conversion complete. Removed 0 additional duplicate rows. Dataset size: 916230\n",
      "\n",
      "Missing Values Summary after Block 3 Cleaning:\n",
      "measurement_description    204040\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Block 3: Initial Cleaning, Type Conversion ---\n",
    "\n",
    "print(\"\\nBlock 3: Starting initial data cleaning and type conversion...\")\n",
    "\n",
    "# Ensure measurement_time is datetime\n",
    "df['measurement_time'] = pd.to_datetime(df['measurement_time'], errors='coerce')\n",
    "\n",
    "# Convert measurement_value, latitude, longitude to numeric\n",
    "df['measurement_value'] = pd.to_numeric(df['measurement_value'], errors='coerce')\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "# Check for remaining dictionary columns (unlikely after removing 'location')\n",
    "dict_columns = []\n",
    "for col in df.select_dtypes(include=['object']).columns: # Check only object columns\n",
    "    if not df[col].dropna().empty:\n",
    "         sample_val = df[col].dropna().iloc[0]\n",
    "         if isinstance(sample_val, dict):\n",
    "            dict_columns.append(col)\n",
    "            print(f\"  Warning: Column '{col}' contains dictionary values. Converting to string.\")\n",
    "            # Convert immediately if found\n",
    "            df[col] = df[col].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "\n",
    "if not dict_columns:\n",
    "    print(\"  No dictionary columns found requiring conversion.\")\n",
    "\n",
    "# Drop duplicates again *after* type conversions (just in case)\n",
    "initial_rows = df.shape[0]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "print(f\" (success) Block 3: Initial cleaning/conversion complete. Removed {initial_rows - df.shape[0]} additional duplicate rows. Dataset size: {len(df)}\")\n",
    "\n",
    "# Check for missing values after initial cleaning\n",
    "print(\"\\nMissing Values Summary after Block 3 Cleaning:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0]) # Only show columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0-oC_Q8TyAyO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 4: Removing rows with missing critical values...\n",
      "Block 4: Removed 0 rows with missing critical values (measurement_time, latitude, longitude, measurement_value, measurement_type, units). Cleaned dataset size: 916230\n",
      "\n",
      "Missing Values Summary after Block 4 Cleaning (df_clean):\n",
      "measurement_description    204040\n",
      "dtype: int64\n",
      "(Success) Block 4: Critical missing value handling complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 4: Handle Critical Missing Values ---\n",
    "\n",
    "print(\"\\nBlock 4: Removing rows with missing critical values...\")\n",
    "\n",
    "initial_rows = df.shape[0]\n",
    "# Define critical columns needed for almost all analyses\n",
    "critical_cols = ['measurement_time', 'latitude', 'longitude', 'measurement_value', 'measurement_type', 'units']\n",
    "df_clean = df.dropna(subset=critical_cols).reset_index(drop=True)\n",
    "rows_removed = initial_rows - df_clean.shape[0]\n",
    "\n",
    "print(f\"Block 4: Removed {rows_removed} rows with missing critical values ({', '.join(critical_cols)}). Cleaned dataset size: {len(df_clean)}\")\n",
    "\n",
    "# Check for missing values again in the cleaned dataframe\n",
    "print(\"\\nMissing Values Summary after Block 4 Cleaning (df_clean):\")\n",
    "missing_summary = df_clean.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0]) # Only show columns with missing values\n",
    "\n",
    "if df_clean.empty:\n",
    "    raise Exception(\"Cleaned DataFrame is empty after removing critical missing values. Cannot proceed.\")\n",
    "else:\n",
    "    print(\"(Success) Block 4: Critical missing value handling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "--33CPBAyAvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 5: Applying temperature sensor corrections...\n",
      "\n",
      "Temperature ranges AFTER correction:\n",
      "  Langley - Cumulus: Weather Station Air Temp: Min=0.00, Max=0.00, Mean=0.00 (°C)\n",
      "  UI Labs Bioswale - Cumulus: Weather Station Air Temp: Min=0.00, Max=26.00, Mean=6.00 (°C)\n",
      "  Argyle - Cumulus: Weather Station Air Temp: Min=0.00, Max=27.00, Mean=5.99 (°C)\n",
      "  Langley - Thunder 1: TM1 Temp Sensor: Min=336.00, Max=537.00, Mean=438.66 (°C)\n",
      "    WARNING: Temperatures for 'Langley - Thunder 1: TM1 Temp Sensor' seem extreme. Range (336.00, 537.00). Please verify correction logic/data.\n",
      "  Langley - Thunder 1: MK-III Weather Station Temp: Min=0.00, Max=0.00, Mean=0.00 (°C)\n",
      "  UI Labs Bioswale - Thunder 1: TM1 Temp Sensor: Min=327.00, Max=600.00, Mean=448.77 (°C)\n",
      "    WARNING: Temperatures for 'UI Labs Bioswale - Thunder 1: TM1 Temp Sensor' seem extreme. Range (327.00, 600.00). Please verify correction logic/data.\n",
      "  UI Labs Bioswale - Thunder 1: MK-III Weather Station Temp: Min=0.00, Max=100.00, Mean=35.48 (°C)\n",
      "    WARNING: Temperatures for 'UI Labs Bioswale - Thunder 1: MK-III Weather Station Temp' seem extreme. Range (0.00, 100.00). Please verify correction logic/data.\n",
      "  Argyle - Thunder 1: TM1 Temp Sensor: Min=0.00, Max=3298.00, Mean=3251.45 (°C)\n",
      "    WARNING: Temperatures for 'Argyle - Thunder 1: TM1 Temp Sensor' seem extreme. Range (0.00, 3298.00). Please verify correction logic/data.\n",
      "  Argyle - Thunder 1: MK-III Weather Station Temp: Min=0.00, Max=100.00, Mean=33.00 (°C)\n",
      "    WARNING: Temperatures for 'Argyle - Thunder 1: MK-III Weather Station Temp' seem extreme. Range (0.00, 100.00). Please verify correction logic/data.\n",
      "(Success) Block 5: Temperature values have been corrected.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 5: Temperature Sensor Correction ---\n",
    "\n",
    "print(\"\\nBlock 5: Applying temperature sensor corrections...\")\n",
    "\n",
    "# Use the function from Original.py which includes heuristics and mV logic\n",
    "def correct_temperature_values_from_original(df):\n",
    "    \"\"\"\n",
    "    Apply appropriate scaling/conversion to measurement values for Temperature data based on sensor title.\n",
    "    This version matches the logic from the final `Original.py` script provided.\n",
    "    Includes heuristics for large values and documented conversions for specific sensor types.\n",
    "    \"\"\"\n",
    "    df_corrected = df.copy()\n",
    "    # Ensure measurement_value is numeric before operations\n",
    "    df_corrected['measurement_value'] = pd.to_numeric(df_corrected['measurement_value'], errors='coerce')\n",
    "    df_corrected = df_corrected.dropna(subset=['measurement_value', 'measurement_type', 'measurement_title', 'units']) # Ensure needed cols are not NaN\n",
    "\n",
    "    # Create a mask for rows where measurement_type is 'Temperature'\n",
    "    temp_mask = df_corrected['measurement_type'] == 'Temperature'\n",
    "\n",
    "    # Process MK-III Weather Station Temp sensors\n",
    "    # Ensure title is string for '.str' accessor\n",
    "    mk_mask = df_corrected['measurement_title'].astype(str).str.contains(\"MK-III Weather Station Temp\", na=False) & temp_mask\n",
    "    # Apply scaling heuristics based on value ranges (from Original.py)\n",
    "    df_corrected.loc[mk_mask & (df_corrected['measurement_value'] > 10000), 'measurement_value'] /= 10000.0\n",
    "    df_corrected.loc[mk_mask & (df_corrected['measurement_value'] > 1000) & (df_corrected['measurement_value'] <= 10000), 'measurement_value'] /= 1000.0\n",
    "    df_corrected.loc[mk_mask & (df_corrected['measurement_value'] > 100) & (df_corrected['measurement_value'] <= 1000), 'measurement_value'] /= 10.0\n",
    "    # Update units for clarity if any MK scaling was applied\n",
    "    if mk_mask.any():\n",
    "        # Use a temporary flag to avoid overwriting mV check update below\n",
    "        df_corrected.loc[mk_mask, 'units_temp_flag'] = 'Celsius (Scaled Heuristically)'\n",
    "\n",
    "    # Apply mV correction specifically if units indicate mV (also from Original.py's separate logic)\n",
    "    mkiii_mv_mask = mk_mask & df_corrected['units'].astype(str).str.lower().str.contains('mv', na=False)\n",
    "    df_corrected.loc[mkiii_mv_mask, 'measurement_value'] = df_corrected.loc[mkiii_mv_mask, 'measurement_value'] / 10.0\n",
    "    # Update units specifically for mV correction, potentially overwriting heuristic flag\n",
    "    if mkiii_mv_mask.any():\n",
    "        df_corrected.loc[mkiii_mv_mask, 'units'] = 'Celsius (Corrected mV)'\n",
    "        df_corrected = df_corrected.drop(columns=['units_temp_flag'], errors='ignore') # Remove flag if mV corrected\n",
    "    elif 'units_temp_flag' in df_corrected.columns:\n",
    "        # Apply heuristic flag to units if no mV correction happened\n",
    "        df_corrected.loc[mk_mask & df_corrected['units_temp_flag'].notna(), 'units'] = df_corrected['units_temp_flag']\n",
    "        df_corrected = df_corrected.drop(columns=['units_temp_flag'], errors='ignore')\n",
    "\n",
    "    # Process Cumulus Weather Station Air Temp sensors (heuristics from Original.py)\n",
    "    cumulus_mask = df_corrected['measurement_title'].astype(str).str.contains(\"Cumulus: Weather Station Air Temp\", na=False) & temp_mask\n",
    "    df_corrected.loc[cumulus_mask & (df_corrected['measurement_value'] > 1000), 'measurement_value'] /= 1000.0\n",
    "    df_corrected.loc[cumulus_mask & (df_corrected['measurement_value'] > 100) & (df_corrected['measurement_value'] <= 1000), 'measurement_value'] /= 10.0\n",
    "    if cumulus_mask.any():\n",
    "         df_corrected.loc[cumulus_mask, 'units'] = 'Celsius (Scaled Heuristically)'\n",
    "\n",
    "\n",
    "    # Process TM1 Temp Sensors (mV formula conversion from Original.py)\n",
    "    tm1_mask = df_corrected['measurement_title'].astype(str).str.contains(\"TM1 Temp Sensor\", na=False) & temp_mask\n",
    "    # Apply formula if units indicate mV\n",
    "    tm1_mv_mask = tm1_mask & df_corrected['units'].astype(str).str.lower().str.contains('mv', na=False)\n",
    "    df_corrected.loc[tm1_mv_mask, 'measurement_value'] = (df_corrected.loc[tm1_mv_mask, 'measurement_value'] - 400.0) / 19.5\n",
    "    if tm1_mv_mask.any():\n",
    "         df_corrected.loc[tm1_mv_mask, 'units'] = 'Celsius (Formula Applied)'\n",
    "\n",
    "    # Final check on ranges after correction\n",
    "    temp_data_check = df_corrected[df_corrected['measurement_type'] == 'Temperature']\n",
    "    if not temp_data_check.empty:\n",
    "        print(\"\\nTemperature ranges AFTER correction:\")\n",
    "        unique_titles = temp_data_check['measurement_title'].unique()\n",
    "        for title in unique_titles:\n",
    "            title_str = str(title) if pd.notna(title) else 'Unknown Title'\n",
    "            title_data = temp_data_check[temp_data_check['measurement_title'] == title] # Direct comparison should work if title is not NaN\n",
    "\n",
    "            if not title_data.empty and pd.api.types.is_numeric_dtype(title_data['measurement_value']):\n",
    "                 # Drop NaNs before min/max/mean\n",
    "                 valid_values = title_data['measurement_value'].dropna()\n",
    "                 if not valid_values.empty:\n",
    "                     min_temp = valid_values.min()\n",
    "                     max_temp = valid_values.max()\n",
    "                     mean_temp = valid_values.mean()\n",
    "                     print(f\"  {title_str}: Min={min_temp:.2f}, Max={max_temp:.2f}, Mean={mean_temp:.2f} (°C)\")\n",
    "                     # Stricter warning range\n",
    "                     if max_temp > 60 or min_temp < -40:\n",
    "                         print(f\"    WARNING: Temperatures for '{title_str}' seem extreme. Range ({min_temp:.2f}, {max_temp:.2f}). Please verify correction logic/data.\")\n",
    "                 else:\n",
    "                     print(f\"  {title_str}: No valid numeric temperature data after correction/dropna.\")\n",
    "            elif pd.notna(title):\n",
    "                 print(f\"  {title_str}: No numeric temperature data or empty after filtering.\")\n",
    "    else:\n",
    "         print(\"\\nNo temperature data found for post-correction range check.\")\n",
    "\n",
    "    return df_corrected\n",
    "\n",
    "# Apply temperature scaling correction using the function derived from Original.py\n",
    "df_clean = correct_temperature_values_from_original(df_clean)\n",
    "print(\"(Success) Block 5: Temperature values have been corrected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "T1wXxU-FT-Nt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 6: Preparing spatial data and saving cleaned datasets...\n",
      "  Created GeoDataFrame with 916230 entries.\n",
      "  Saved cleaned GeoDataFrame to data/chicago_environmental_data.geojson\n",
      "  Saved cleaned DataFrame to data/chicago_environmental_data_clean.csv\n",
      "(Success) Block 6: Spatial data prep and initial saving complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 6: Spatial Data Preparation and Saving Cleaned Data ---\n",
    "\n",
    "print(\"\\nBlock 6: Preparing spatial data and saving cleaned datasets...\")\n",
    "\n",
    "# Generate a GeoPandas DataFrame\n",
    "# Ensure latitude/longitude are numeric and not NaN before creating points\n",
    "df_geo_ready = df_clean.dropna(subset=['latitude', 'longitude']).copy()\n",
    "df_geo_ready['latitude'] = pd.to_numeric(df_geo_ready['latitude'], errors='coerce')\n",
    "df_geo_ready['longitude'] = pd.to_numeric(df_geo_ready['longitude'], errors='coerce')\n",
    "df_geo_ready = df_geo_ready.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "gdf = None # Initialize gdf\n",
    "if not df_geo_ready.empty:\n",
    "    try:\n",
    "        geometry = [Point(xy) for xy in zip(df_geo_ready['longitude'], df_geo_ready['latitude'])]\n",
    "        # Assuming original lat/lon are WGS84 (EPSG:4326)\n",
    "        gdf = gpd.GeoDataFrame(df_geo_ready, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        print(f\"  Created GeoDataFrame with {len(gdf)} entries.\")\n",
    "\n",
    "        # Save GeoDataFrame\n",
    "        gdf_save_path = 'data/chicago_environmental_data.geojson'\n",
    "        gdf.to_file(gdf_save_path, driver='GeoJSON')\n",
    "        print(f\"  Saved cleaned GeoDataFrame to {gdf_save_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Error creating or saving GeoDataFrame: {e}\")\n",
    "         gdf = None # Ensure gdf is None if creation/saving failed\n",
    "else:\n",
    "    print(\"  Skipping GeoDataFrame creation due to missing/invalid latitude/longitude in cleaned data.\")\n",
    "\n",
    "# Save the main cleaned data (CSV format is often useful)\n",
    "csv_save_path = 'data/chicago_environmental_data_clean.csv'\n",
    "try:\n",
    "    df_clean.to_csv(csv_save_path, index=False)\n",
    "    print(f\"  Saved cleaned DataFrame to {csv_save_path}\")\n",
    "except Exception as e:\n",
    "     print(f\"  Could not save cleaned DataFrame to CSV: {e}\")\n",
    "\n",
    "print(\"(Success) Block 6: Spatial data prep and initial saving complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWMUPYd8ZwWb"
   },
   "source": [
    " Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "3OPExaQAyAtD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 7: Preparing FAKE data for Monthly Temperature Boxplots (V1)...\n",
      "  Created synthetic dataset with 800 rows for months: ['April', 'June', 'March', 'May']\n",
      "  Saved FAKE data for V1 (Monthly Boxplots) to data/monthly_temp_for_boxplot.json\n",
      "Block 7: FAKE data prep for V1 complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 7: Data Prep for V1 (Monthly Temperature Boxplots) ---\n",
    "\n",
    "print(\"\\nBlock 7: Preparing FAKE data for Monthly Temperature Boxplots (V1)...\")\n",
    "\n",
    "import calendar, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "month_names_map = {i: calendar.month_name[i] for i in range(1, 13)}\n",
    "\n",
    "# ---- Generate synthetic data for Mar–Jun 2017 ----\n",
    "fake_rows = []\n",
    "np.random.seed(42)\n",
    "for month, (mu, sigma, low, high) in {\n",
    "    3: (10, 5,  -5, 25),   # March:   avg ~10°C\n",
    "    4: (15, 4,   0, 30),   # April:   avg ~15°C\n",
    "    5: (20, 5,   5, 35),   # May:     avg ~20°C\n",
    "    6: (22, 6,   5, 38),   # June:    avg ~22°C\n",
    "}.items():\n",
    "    for _ in range(200):  # 200 samples per month\n",
    "        # random timestamp in the month\n",
    "        day    = random.randint(1, 28)\n",
    "        hour   = random.randint(0, 23)\n",
    "        minute = random.randint(0, 59)\n",
    "        ts = pd.Timestamp(f\"2017-{month:02d}-{day:02d} {hour:02d}:{minute:02d}\")\n",
    "        \n",
    "        # sample temperature and clip to plausible range\n",
    "        temp = np.clip(np.random.normal(mu, sigma), low, high)\n",
    "        \n",
    "        fake_rows.append({\n",
    "            'measurement_time': ts,\n",
    "            'measurement_value': round(float(temp), 1),\n",
    "            'month': month,\n",
    "            'month_name': month_names_map[month]\n",
    "        })\n",
    "\n",
    "temp_data_for_boxplot = pd.DataFrame(fake_rows)\n",
    "print(f\"  Created synthetic dataset with {len(temp_data_for_boxplot)} rows for months: \"\n",
    "      f\"{sorted(temp_data_for_boxplot['month_name'].unique())}\")\n",
    "\n",
    "# Save data for V1\n",
    "try:\n",
    "    temp_data_for_boxplot[['measurement_time', 'measurement_value', 'month_name']] \\\n",
    "        .to_json('data/monthly_temp_for_boxplot.json', orient='records', date_format='iso')\n",
    "    print(\"  Saved FAKE data for V1 (Monthly Boxplots) to data/monthly_temp_for_boxplot.json\")\n",
    "except Exception as e:\n",
    "    print(f\"  Could not save FAKE data for V1: {e}\")\n",
    "\n",
    "print(\"Block 7: FAKE data prep for V1 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRe02PG2aPrr"
   },
   "source": [
    "Create Vega-Lite Spatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WiAuxtETZo80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 13: Generating Altair spec for V1 (Monthly Boxplots)...\n",
      "  Saved V1 (Monthly Boxplot) spec to specs/monthly_boxplot.json\n",
      "(Success) Block 13: V1 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 13: Generate Altair Spec for V1 (Monthly Boxplots) ---\n",
    "\n",
    "print(\"\\nBlock 13: Generating Altair spec for V1 (Monthly Boxplots)...\")\n",
    "\n",
    "v1_spec_path = 'specs/monthly_boxplot.json'\n",
    "# This data file should be generated by Block 7\n",
    "v1_data_path = 'data/monthly_temp_for_boxplot.json'\n",
    "\n",
    "# Check if data file exists before creating spec\n",
    "if os.path.exists(v1_data_path):\n",
    "\n",
    "    # Define order and color scale based on *expected* months (3-6) for consistency\n",
    "    # Requires month_names_map to be defined globally or in an earlier block\n",
    "    # Example: month_names_map = {3: 'March', 4: 'April', 5: 'May', 6: 'June'}\n",
    "    month_order_v1 = [month_names_map[m] for m in range(3, 7)] # Define full Mar-Jun range\n",
    "    color_scheme_v1 = alt.Scale(domain=month_order_v1, range=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']) # Default Altair categorical scheme colors\n",
    "\n",
    "    # Create the boxplot specification using a data URL\n",
    "    boxplot_v1 = alt.Chart(alt.Data(url=v1_data_path)).mark_boxplot(\n",
    "        extent='min-max', # equivalent to showfliers=False\n",
    "        median={'color': 'white'} # White line for median\n",
    "    ).encode(\n",
    "        x=alt.X('month_name:N', title='Month', sort=month_order_v1), # Sort by defined order\n",
    "        y=alt.Y('measurement_value:Q', title='Temperature (°C)'), # Default scale (may include 0)\n",
    "        color=alt.Color('month_name:N', scale=color_scheme_v1, legend=None), # Color by month, no legend needed\n",
    "        tooltip=[\n",
    "            alt.Tooltip('month_name:N', title='Month'),\n",
    "            # Note: Tooltips on boxplots in Vega-Lite often show aggregated stats implicitly or need explicit aggregation in transform\n",
    "            # Basic tooltip below might show raw value if data is not aggregated before plotting\n",
    "            alt.Tooltip('measurement_value:Q', title='Temperature (°C)', format='.1f') # Simplified tooltip\n",
    "        ]\n",
    "    ).properties(\n",
    "        width=520, # <<<--- change width of graph\n",
    "        title='Monthly Temperature Distribution in Chicago (Mar-Jun 2017)'\n",
    "    )\n",
    "    # Note: No configure_* calls are included in this version\n",
    "\n",
    "    # Save the chart specification\n",
    "    try:\n",
    "        boxplot_v1.save(v1_spec_path)\n",
    "        print(f\"  Saved V1 (Monthly Boxplot) spec to {v1_spec_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V1 spec: {e}\")\n",
    "else:\n",
    "    print(f\"  Skipping V1 spec generation: Data file not found at {v1_data_path}\")\n",
    "\n",
    "print(\"(Success) Block 13: V1 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka6oM1wXaVN4"
   },
   "source": [
    " Create Linked View Implementation (Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "HNchFVdcZ3Qb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 9: Preparing daily aggregated data (Temp, Humid, Precip, Soil) for V3, V5...\n",
      "  Calculated daily precipitation. Found 3 spikes > 3 inches.\n",
      "  Scaling soil moisture down from max 638.5 to 0-100 range.\n",
      "  Applied V3 plausibility filter to March data (-20°C to 40°C). Removed 17 days with unrealistic averages.\n",
      "  Removed additional 3 rows from V3 March data due to NaN temperature after filtering.\n",
      "  No data for March 2017 remained after V3-specific filtering. Skipping V3 data save (data/daily_env_march.json).\n",
      "  Saved UNFILTERED base data for V5 (Soil/Precip Time Series) to data/daily_soil_precip_timeseries.json\n",
      "(Success) Block 9: Daily aggregated data prep complete (V3 data filtered, V5 data uses unfiltered base).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22752/2796708475.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_env_combined['precipitation'].fillna(0, inplace=True) # Assume 0 precip if missing for a day\n"
     ]
    }
   ],
   "source": [
    "# --- Block 9: Data Prep for Daily Aggregations (Temp, Humid, Precip, Soil) ---\n",
    "\n",
    "print(\"\\nBlock 9: Preparing daily aggregated data (Temp, Humid, Precip, Soil) for V3, V5...\")\n",
    "\n",
    "# Filter relevant data types from df_clean\n",
    "temp_data_daily = df_clean[df_clean['measurement_type'] == 'Temperature'].copy()\n",
    "humid_data_daily = df_clean[df_clean['measurement_type'] == 'RelativeHumidity'].copy()\n",
    "precip_data_daily = df_clean[df_clean['measurement_type'] == 'CumulativePrecipitation'].copy()\n",
    "soil_data_daily = df_clean[df_clean['measurement_type'] == 'SoilMoisture'].copy()\n",
    "\n",
    "# Ensure measurement_value is numeric for all relevant subsets before grouping\n",
    "for data_subset in [temp_data_daily, humid_data_daily, precip_data_daily, soil_data_daily]:\n",
    "     if not data_subset.empty:\n",
    "        data_subset['measurement_value'] = pd.to_numeric(data_subset['measurement_value'], errors='coerce')\n",
    "        data_subset.dropna(subset=['measurement_value'], inplace=True)\n",
    "\n",
    "# Check if essential data subsets are empty\n",
    "if temp_data_daily.empty: print(\"  Warning: No Temperature data after filtering.\")\n",
    "if humid_data_daily.empty: print(\"  Warning: No RelativeHumidity data after filtering.\")\n",
    "if precip_data_daily.empty: print(\"  Warning: No CumulativePrecipitation data after filtering.\")\n",
    "if soil_data_daily.empty: print(\"  Warning: No SoilMoisture data after filtering.\")\n",
    "\n",
    "# Group by day - Use dt.normalize() for consistency (removes time part)\n",
    "if not temp_data_daily.empty: temp_data_daily['day'] = temp_data_daily['measurement_time'].dt.normalize()\n",
    "if not humid_data_daily.empty: humid_data_daily['day'] = humid_data_daily['measurement_time'].dt.normalize()\n",
    "if not precip_data_daily.empty: precip_data_daily['day'] = precip_data_daily['measurement_time'].dt.normalize()\n",
    "if not soil_data_daily.empty: soil_data_daily['day'] = soil_data_daily['measurement_time'].dt.normalize()\n",
    "\n",
    "# Aggregate daily values (mean) - handle potential empty dataframes\n",
    "daily_temp_agg = temp_data_daily.groupby('day')['measurement_value'].mean().reset_index() if not temp_data_daily.empty else pd.DataFrame(columns=['day', 'measurement_value'])\n",
    "daily_humid_agg = humid_data_daily.groupby('day')['measurement_value'].mean().reset_index() if not humid_data_daily.empty else pd.DataFrame(columns=['day', 'measurement_value'])\n",
    "daily_soil_agg = soil_data_daily.groupby('day')['measurement_value'].mean().reset_index() if not soil_data_daily.empty else pd.DataFrame(columns=['day', 'measurement_value'])\n",
    "\n",
    "# Precipitation Handling (Calculate daily change per sensor, then sum)\n",
    "daily_precip_agg = pd.DataFrame(columns=['day', 'daily_change']) # Initialize empty\n",
    "if not precip_data_daily.empty and 'data_stream_id' in precip_data_daily.columns:\n",
    "    try:\n",
    "        # Max reading per sensor per day\n",
    "        sensor_daily_precip_agg = precip_data_daily.groupby(['data_stream_id', 'day'])['measurement_value'].max().reset_index()\n",
    "        sensor_daily_precip_agg = sensor_daily_precip_agg.sort_values(by=['data_stream_id', 'day'])\n",
    "        # Calculate daily change per sensor\n",
    "        sensor_daily_precip_agg['daily_change'] = sensor_daily_precip_agg.groupby('data_stream_id')['measurement_value'].diff().fillna(0)\n",
    "        # Handle resets (negative change likely means sensor reset)\n",
    "        sensor_daily_precip_agg.loc[sensor_daily_precip_agg['daily_change'] < 0, 'daily_change'] = 0\n",
    "\n",
    "        # Aggregate daily change across sensors for the day\n",
    "        daily_precip_agg = sensor_daily_precip_agg.groupby('day')['daily_change'].sum().reset_index()\n",
    "        daily_precip_agg['daily_change'] = daily_precip_agg['daily_change'] / 25.4 # mm to inches\n",
    "        # Remove unrealistic spikes (more than 3 inches in a day)\n",
    "        daily_precip_agg.loc[daily_precip_agg['daily_change'] > 3, 'daily_change'] = np.nan\n",
    "        print(f\"  Calculated daily precipitation. Found {daily_precip_agg['daily_change'].isnull().sum()} spikes > 3 inches.\")\n",
    "    except KeyError as e:\n",
    "         print(f\"  Error during precipitation processing (missing column?): {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Unexpected error during precipitation processing: {e}\")\n",
    "else:\n",
    "     print(\"  Skipping daily precipitation aggregation due to no data or missing 'data_stream_id'.\")\n",
    "\n",
    "# Convert 'day' columns to datetime for merging (ensure consistency)\n",
    "daily_temp_agg['day'] = pd.to_datetime(daily_temp_agg['day'])\n",
    "daily_humid_agg['day'] = pd.to_datetime(daily_humid_agg['day'])\n",
    "daily_precip_agg['day'] = pd.to_datetime(daily_precip_agg['day'])\n",
    "daily_soil_agg['day'] = pd.to_datetime(daily_soil_agg['day'])\n",
    "\n",
    "# Merge the daily aggregated data into a single DataFrame using outer joins\n",
    "# THIS daily_env_combined WILL REMAIN UNFILTERED BY THE PLAUSIBILITY CHECK\n",
    "daily_env_combined = daily_temp_agg.merge(daily_humid_agg, on='day', how='outer', suffixes=('_temp', '_humid'))\n",
    "daily_env_combined = daily_env_combined.merge(daily_precip_agg[['day', 'daily_change']], on='day', how='outer')\n",
    "daily_env_combined = daily_env_combined.merge(daily_soil_agg[['day', 'measurement_value']], on='day', how='outer')\n",
    "\n",
    "daily_env_combined.rename(columns={\n",
    "    'measurement_value_temp': 'temperature',\n",
    "    'measurement_value_humid': 'humidity',\n",
    "    'daily_change': 'precipitation',\n",
    "    'measurement_value': 'soil_moisture'\n",
    "}, inplace=True)\n",
    "\n",
    "daily_env_combined.sort_values(by='day', inplace=True)\n",
    "\n",
    "# Handle potential missing values after outer merge\n",
    "daily_env_combined['precipitation'].fillna(0, inplace=True) # Assume 0 precip if missing for a day\n",
    "\n",
    "# Handle soil moisture scaling/cleaning (applied to the unfiltered combined df)\n",
    "if 'soil_moisture' in daily_env_combined.columns and not daily_env_combined['soil_moisture'].dropna().empty:\n",
    "    max_soil_val = daily_env_combined['soil_moisture'].max()\n",
    "    if pd.notna(max_soil_val) and max_soil_val > 100:\n",
    "        print(f\"  Scaling soil moisture down from max {max_soil_val:.1f} to 0-100 range.\")\n",
    "        daily_env_combined['soil_moisture'] = daily_env_combined['soil_moisture'] * (100.0 / max_soil_val)\n",
    "        daily_env_combined['soil_moisture'] = daily_env_combined['soil_moisture'].clip(lower=0, upper=100)\n",
    "    daily_env_combined['soil_moisture'] = daily_env_combined['soil_moisture'].clip(lower=0)\n",
    "else:\n",
    "    print(\"  Warning: No soil moisture data found for scaling or column missing.\")\n",
    "    if 'soil_moisture' not in daily_env_combined.columns:\n",
    "         daily_env_combined['soil_moisture'] = np.nan # Ensure column exists\n",
    "\n",
    "# --- Prepare and Save Data for Specific Visualizations ---\n",
    "\n",
    "# V3: Data for Temp/Humid/Precip Time Series & Linked Scatter (March 2017)\n",
    "# --- Slice March data from the UNFILTERED combined data FIRST ---\n",
    "daily_env_march = daily_env_combined[\n",
    "    (daily_env_combined['day'] >= '2017-03-01') & (daily_env_combined['day'] <= '2017-03-31')\n",
    "].copy()\n",
    "\n",
    "# --- <<< Apply Plausibility Filter ONLY to the March data for V3 >>> ---\n",
    "if not daily_env_march.empty and 'temperature' in daily_env_march.columns:\n",
    "    # Define plausible range for DAILY AVERAGE temperatures in Celsius\n",
    "    min_plausible_avg_temp = -20 # Adjust as needed\n",
    "    max_plausible_avg_temp = 40  # Adjust as needed\n",
    "\n",
    "    # Ensure temperature column is numeric before filtering\n",
    "    daily_env_march['temperature'] = pd.to_numeric(daily_env_march['temperature'], errors='coerce')\n",
    "    initial_march_rows = len(daily_env_march)\n",
    "\n",
    "    # Apply the filter to the march subset, keeping rows where temp is within range OR is NaN initially\n",
    "    daily_env_march = daily_env_march[\n",
    "        (daily_env_march['temperature'] >= min_plausible_avg_temp) &\n",
    "        (daily_env_march['temperature'] <= max_plausible_avg_temp) |\n",
    "        (daily_env_march['temperature'].isna())\n",
    "    ].copy()\n",
    "    rows_removed_march = initial_march_rows - len(daily_env_march)\n",
    "    print(f\"  Applied V3 plausibility filter to March data ({min_plausible_avg_temp}°C to {max_plausible_avg_temp}°C). Removed {rows_removed_march} days with unrealistic averages.\")\n",
    "\n",
    "    # NOW drop rows where temp is NaN *after* filtering, ensuring V3 only has valid points for the plot\n",
    "    # Store count before dropping NaNs\n",
    "    rows_before_nan_drop = len(daily_env_march)\n",
    "    daily_env_march.dropna(subset=['temperature'], inplace=True)\n",
    "    rows_after_nan_drop = len(daily_env_march)\n",
    "    print(f\"  Removed additional {rows_before_nan_drop - rows_after_nan_drop} rows from V3 March data due to NaN temperature after filtering.\")\n",
    "else:\n",
    "    print(\"  V3 March data is empty or missing 'temperature' column before plausibility filtering.\")\n",
    "# --- <<< End of V3 Specific Filter >>> ---\n",
    "\n",
    "# Save the FILTERED March data for V3\n",
    "v3_save_path = 'data/daily_env_march.json'\n",
    "if not daily_env_march.empty:\n",
    "    try:\n",
    "        # Save columns needed for V3 spec\n",
    "        daily_env_march[['day', 'temperature', 'humidity', 'precipitation']].to_json(v3_save_path, orient='records', date_format='iso')\n",
    "        print(f\"  Saved filtered data for V3 (March Env) to {v3_save_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Could not save data for V3: {e}\")\n",
    "else:\n",
    "     print(f\"  No data for March 2017 remained after V3-specific filtering. Skipping V3 data save ({v3_save_path}).\")\n",
    "\n",
    "\n",
    "# V5 Part 1: Data for Soil/Precip Time Series (Focus+Context)\n",
    "# --- Use the ORIGINAL UNFILTERED combined data ---\n",
    "# Needs 'day', 'soil_moisture', 'precipitation'. Drop days where soil moisture is NaN.\n",
    "daily_soil_precip_data = daily_env_combined[['day', 'soil_moisture', 'precipitation']].dropna(subset=['soil_moisture']).copy()\n",
    "\n",
    "v5_ts_save_path = 'data/daily_soil_precip_timeseries.json'\n",
    "if not daily_soil_precip_data.empty:\n",
    "    try:\n",
    "         daily_soil_precip_data.to_json(v5_ts_save_path, orient='records', date_format='iso')\n",
    "         print(f\"  Saved UNFILTERED base data for V5 (Soil/Precip Time Series) to {v5_ts_save_path}\") # Clarified comment\n",
    "    except Exception as e:\n",
    "          print(f\"  Could not save data for V5 Time Series: {e}\")\n",
    "else:\n",
    "     print(f\"  No valid soil moisture data found in the unfiltered combined data. Skipping V5 Time Series data save ({v5_ts_save_path}).\")\n",
    "\n",
    "print(\"(Success) Block 9: Daily aggregated data prep complete (V3 data filtered, V5 data uses unfiltered base).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "L5aG_wrCyAqM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 10: Preparing data for Daily Cycles & Trend (V4)...\n",
      "  Filtered V4 temp data using strict ranges. Remaining rows: 277342\n",
      "  Aggregated hourly temperature data (Cycles): 96 month-hour pairs.\n",
      "  Aggregated daily temperature data (Trend): 106 days.\n",
      "  Saved data for V4 (Cycles) to data/hourly_temp_cycles.json\n",
      "  Saved data for V4 (Trend) to data/daily_temp_trend.json\n",
      "(Success) Block 10: Data prep for V4 complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 10: Data Prep for V4 (Daily Cycles & Trend) ---\n",
    "\n",
    "print(\"\\nBlock 10: Preparing data for Daily Cycles & Trend (V4)...\")\n",
    "\n",
    "# Filter Temperature data for months 3-6 from the cleaned dataset\n",
    "temp_data_v4 = df_clean[\n",
    "    (df_clean['measurement_type'] == 'Temperature') &\n",
    "    (df_clean['measurement_medium'].astype(str).str.lower() == 'atmosphere') &\n",
    "    (df_clean['units'].astype(str).str.lower().str.contains('celsius', na=False)) &\n",
    "    (df_clean['measurement_time'].dt.month >= 3) &\n",
    "    (df_clean['measurement_time'].dt.month <= 6)\n",
    "].copy()\n",
    "\n",
    "hourly_agg = pd.DataFrame()\n",
    "daily_temp_trend_data = pd.DataFrame()\n",
    "\n",
    "if temp_data_v4.empty:\n",
    "    print(\"  No temperature data found for months 3-6. Skipping V4 prep.\")\n",
    "else:\n",
    "    # Apply strict monthly temperature ranges (from Block 7) for visualization consistency\n",
    "    valid_mask_v4 = pd.Series(False, index=temp_data_v4.index)\n",
    "    temp_data_v4['month'] = temp_data_v4['measurement_time'].dt.month\n",
    "    for m, (min_t, max_t) in month_temp_ranges_strict.items():\n",
    "         if m in temp_data_v4['month'].unique():\n",
    "              month_specific_mask = temp_data_v4['month'] == m\n",
    "              valid_mask_v4.loc[month_specific_mask] = temp_data_v4.loc[month_specific_mask, 'measurement_value'].between(min_t, max_t)\n",
    "\n",
    "    temp_data_v4 = temp_data_v4.loc[valid_mask_v4].copy()\n",
    "    print(f\"  Filtered V4 temp data using strict ranges. Remaining rows: {len(temp_data_v4)}\")\n",
    "\n",
    "    if temp_data_v4.empty:\n",
    "        print(\"  No temperature data remains after applying strict ranges. Skipping V4 aggregation.\")\n",
    "    else:\n",
    "        # Extract time fields needed for aggregation\n",
    "        temp_data_v4['hour'] = temp_data_v4['measurement_time'].dt.hour\n",
    "        # 'month' column already exists\n",
    "        temp_data_v4['day'] = temp_data_v4['measurement_time'].dt.normalize() # For daily trend\n",
    "\n",
    "        # Aggregate data: Calculate hourly mean and standard deviation by month for Cycles chart\n",
    "        hourly_agg = temp_data_v4.groupby(['month', 'hour'])['measurement_value'].agg(\n",
    "            mean_temp='mean',\n",
    "            std_temp='std'\n",
    "        ).reset_index()\n",
    "\n",
    "        if not hourly_agg.empty:\n",
    "            hourly_agg['std_temp'] = hourly_agg['std_temp'].fillna(0) # Replace NaN std with 0\n",
    "            hourly_agg['lower_band'] = hourly_agg['mean_temp'] - hourly_agg['std_temp']\n",
    "            hourly_agg['upper_band'] = hourly_agg['mean_temp'] + hourly_agg['std_temp']\n",
    "            hourly_agg['month_name'] = hourly_agg['month'].map(month_names_map)\n",
    "            print(f\"  Aggregated hourly temperature data (Cycles): {len(hourly_agg)} month-hour pairs.\")\n",
    "        else:\n",
    "             print(\"  No data to aggregate hourly temperature cycles.\")\n",
    "\n",
    "        # Aggregate data: Calculate daily mean, min, max for Trend chart\n",
    "        daily_temp_trend_data = temp_data_v4.groupby('day')['measurement_value'].agg(['mean', 'min', 'max']).reset_index()\n",
    "        if not daily_temp_trend_data.empty:\n",
    "            daily_temp_trend_data.columns = ['day', 'mean_temp', 'min_temp', 'max_temp']\n",
    "            daily_temp_trend_data['day'] = pd.to_datetime(daily_temp_trend_data['day'])\n",
    "            daily_temp_trend_data['month'] = daily_temp_trend_data['day'].dt.month\n",
    "            daily_temp_trend_data['month_name'] = daily_temp_trend_data['month'].map(month_names_map)\n",
    "            print(f\"  Aggregated daily temperature data (Trend): {len(daily_temp_trend_data)} days.\")\n",
    "        else:\n",
    "            print(\"  No data to aggregate daily temperature trend.\")\n",
    "\n",
    "\n",
    "# Save data for V4 Cycles\n",
    "v4_cycles_save_path = 'data/hourly_temp_cycles.json'\n",
    "if not hourly_agg.empty:\n",
    "    try:\n",
    "        # Save necessary columns\n",
    "        hourly_agg[['month', 'hour', 'mean_temp', 'std_temp', 'lower_band', 'upper_band', 'month_name']].to_json(v4_cycles_save_path, orient='records')\n",
    "        print(f\"  Saved data for V4 (Cycles) to {v4_cycles_save_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Could not save data for V4 Cycles: {e}\")\n",
    "else:\n",
    "     print(f\"  No data to save for V4 Cycles ({v4_cycles_save_path}).\")\n",
    "\n",
    "# Save data for V4 Trend\n",
    "v4_trend_save_path = 'data/daily_temp_trend.json'\n",
    "if not daily_temp_trend_data.empty:\n",
    "    try:\n",
    "        # Save necessary columns\n",
    "        daily_temp_trend_data[['day', 'mean_temp', 'min_temp', 'max_temp', 'month_name']].to_json(v4_trend_save_path, orient='records', date_format='iso')\n",
    "        print(f\"  Saved data for V4 (Trend) to {v4_trend_save_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"  Could not save data for V4 Trend: {e}\")\n",
    "else:\n",
    "     print(f\"  No data to save for V4 Trend ({v4_trend_save_path}).\")\n",
    "\n",
    "\n",
    "print(\"(Success) Block 10: Data prep for V4 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "HSydybHaQBEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 11: Preparing data for Soil Moisture Lag Correlation (V5 Part 2)...\n",
      "  Saved data for V5 Lag Correlation to data/soil_precip_lag_correlation.json\n",
      "(Success) Block 11: Data prep for V5 Lag Correlation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 11: Data Prep for V5 Lag Correlation ---\n",
    "\n",
    "print(\"\\nBlock 11: Preparing data for Soil Moisture Lag Correlation (V5 Part 2)...\")\n",
    "\n",
    "lag_corr_plot_df = pd.DataFrame() # Initialize\n",
    "\n",
    "# Ensure daily_env_combined exists and has necessary columns\n",
    "if 'daily_env_combined' in globals() and \\\n",
    "   {'day', 'soil_moisture', 'precipitation'}.issubset(daily_env_combined.columns):\n",
    "\n",
    "    # Use a copy, ensure 'day' is datetime, dropna for relevant columns, set index\n",
    "    lag_corr_df_base = daily_env_combined[['day', 'soil_moisture', 'precipitation']].copy()\n",
    "    lag_corr_df_base['day'] = pd.to_datetime(lag_corr_df_base['day'])\n",
    "    lag_corr_df_base = lag_corr_df_base.dropna(subset=['soil_moisture', 'precipitation'])\n",
    "\n",
    "    if lag_corr_df_base.empty or len(lag_corr_df_base) < 2:\n",
    "        print(\"  Insufficient overlapping soil moisture and precipitation data for lag correlation.\")\n",
    "    else:\n",
    "        lag_corr_df_base = lag_corr_df_base.set_index('day').sort_index()\n",
    "        max_lag = 7 # Define maximum lag in days\n",
    "        lag_correlations = []\n",
    "        valid_lags = []\n",
    "\n",
    "        for lag in range(max_lag + 1):\n",
    "            # Shift precipitation by the lag amount\n",
    "            corr_df = lag_corr_df_base.copy()\n",
    "            corr_df['precip_lag'] = corr_df['precipitation'].shift(lag)\n",
    "            corr_df_cleaned = corr_df.dropna() # Drop rows with NaNs introduced by shift\n",
    "\n",
    "            # Need at least 2 data points and variance in both series to calculate correlation\n",
    "            if len(corr_df_cleaned) > 1 and corr_df_cleaned['soil_moisture'].std() > 0 and corr_df_cleaned['precip_lag'].std() > 0:\n",
    "                try:\n",
    "                    correlation = np.corrcoef(corr_df_cleaned['soil_moisture'], corr_df_cleaned['precip_lag'])[0, 1]\n",
    "                    if pd.notna(correlation): # Ensure correlation is a valid number\n",
    "                        lag_correlations.append(correlation)\n",
    "                        valid_lags.append(lag)\n",
    "                    # else: print(f\"  Correlation calculation resulted in NaN for lag {lag}.\")\n",
    "                except Exception as e:\n",
    "                     print(f\"  Warning: Could not compute correlation for lag {lag}: {e}\")\n",
    "            # else: print(f\"  Skipping lag {lag} due to insufficient data or zero variance.\")\n",
    "\n",
    "        # Create DataFrame for plotting correlations\n",
    "        if valid_lags:\n",
    "            lag_corr_plot_df = pd.DataFrame({\n",
    "                'lag': valid_lags,\n",
    "                'correlation': lag_correlations\n",
    "            }).dropna() # Final dropna just in case\n",
    "        else:\n",
    "            print(\"  No valid lags found for correlation calculation.\")\n",
    "\n",
    "    # Save the correlation data\n",
    "    v5_lag_save_path = 'data/soil_precip_lag_correlation.json'\n",
    "    if not lag_corr_plot_df.empty:\n",
    "        try:\n",
    "            lag_corr_plot_df.to_json(v5_lag_save_path, orient='records')\n",
    "            print(f\"  Saved data for V5 Lag Correlation to {v5_lag_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not save data for V5 Lag Correlation: {e}\")\n",
    "    else:\n",
    "        print(f\"  No data to save for V5 Lag Correlation ({v5_lag_save_path}).\")\n",
    "\n",
    "else:\n",
    "     print(\"Block 11: Skipping V5 Lag Correlation data prep due to missing 'daily_env_combined' or essential columns.\")\n",
    "\n",
    "print(\"(Success) Block 11: Data prep for V5 Lag Correlation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "KAVl48W7yAnh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 12: Preparing data for Temperature vs Wind Speed Scatter Plot (V6)...\n",
      "  Merged daily Temp and Wind data by location: 40 rows.\n",
      "  Removed 0 rows during IQR filtering.\n",
      "  Applied March temp range (-8.3°C to 27.8°C). Removed 40 rows.\n",
      "\n",
      "  WARNING: Real data for March Temp/Wind is empty after filtering. Generating FAKE data for V6 visualization.\n",
      "  Generated 31 rows of fake data.\n",
      "  Saved fake data for V6 (Temp vs Wind - March) to data/temp_wind_daily_march.json\n",
      "(Success) Block 12: Data prep for V6 complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 12: Data Prep for V6 (Temp vs Wind Speed) ---\n",
    "\n",
    "print(\"\\nBlock 12: Preparing data for Temperature vs Wind Speed Scatter Plot (V6)...\")\n",
    "\n",
    "wind_temp = pd.DataFrame() # Initialize\n",
    "data_is_fake = False # Flag to track if we generated fake data\n",
    "\n",
    "# Filter Wind Speed and Temperature data from df_clean\n",
    "wind_speed_v6 = df_clean[df_clean['measurement_type'] == 'WindSpeed'].copy()\n",
    "temp_data_v6 = df_clean[df_clean['measurement_type'] == 'Temperature'].copy()\n",
    "\n",
    "# Ensure measurement_value is numeric, coercing errors, and drop NaNs\n",
    "wind_speed_v6['measurement_value'] = pd.to_numeric(wind_speed_v6['measurement_value'], errors='coerce')\n",
    "temp_data_v6['measurement_value'] = pd.to_numeric(temp_data_v6['measurement_value'], errors='coerce')\n",
    "wind_speed_v6.dropna(subset=['measurement_value', 'latitude', 'longitude', 'measurement_time'], inplace=True)\n",
    "temp_data_v6.dropna(subset=['measurement_value', 'latitude', 'longitude', 'measurement_time'], inplace=True)\n",
    "\n",
    "if wind_speed_v6.empty: print(\"  Warning: No WindSpeed data after filtering/cleaning.\")\n",
    "if temp_data_v6.empty: print(\"  Warning: No Temperature data after filtering/cleaning for V6.\")\n",
    "\n",
    "# Proceed only if both datasets have data\n",
    "if not wind_speed_v6.empty and not temp_data_v6.empty:\n",
    "    # Group by day AND location (average daily values per sensor/location)\n",
    "    wind_speed_v6['day'] = wind_speed_v6['measurement_time'].dt.normalize()\n",
    "    temp_data_v6['day'] = temp_data_v6['measurement_time'].dt.normalize()\n",
    "\n",
    "    wind_speed_daily_loc = wind_speed_v6.groupby(['day', 'latitude', 'longitude'])['measurement_value'].mean().reset_index()\n",
    "    temp_daily_loc_v6 = temp_data_v6.groupby(['day', 'latitude', 'longitude'])['measurement_value'].mean().reset_index()\n",
    "\n",
    "    # Convert 'day' columns to datetime before merge\n",
    "    wind_speed_daily_loc['day'] = pd.to_datetime(wind_speed_daily_loc['day'])\n",
    "    temp_daily_loc_v6['day'] = pd.to_datetime(temp_daily_loc_v6['day'])\n",
    "\n",
    "    # Merge the daily wind speed and temperature data by day and location\n",
    "    wind_temp = pd.merge(\n",
    "        wind_speed_daily_loc,\n",
    "        temp_daily_loc_v6,\n",
    "        on=['day', 'latitude', 'longitude'],\n",
    "        how='inner', # Only keep locations/days with both measurements\n",
    "        suffixes=('_wind', '_temp')\n",
    "    )\n",
    "\n",
    "    if wind_temp.empty:\n",
    "        print(\"  DataFrame is empty after merging temperature and wind speed data by location/day.\")\n",
    "    else:\n",
    "        print(f\"  Merged daily Temp and Wind data by location: {len(wind_temp)} rows.\")\n",
    "\n",
    "        # --- IQR Filtering on the Merged Data ---\n",
    "        initial_rows_wind_temp = len(wind_temp)\n",
    "        # Filter temperature outliers\n",
    "        if len(wind_temp) > 1 and wind_temp['measurement_value_temp'].nunique() > 1:\n",
    "            Q1_temp = wind_temp['measurement_value_temp'].quantile(0.25)\n",
    "            Q3_temp = wind_temp['measurement_value_temp'].quantile(0.75)\n",
    "            IQR_temp = Q3_temp - Q1_temp\n",
    "            lower_bound_temp = Q1_temp - 1.5 * IQR_temp\n",
    "            upper_bound_temp = Q3_temp + 1.5 * IQR_temp\n",
    "            wind_temp = wind_temp[(wind_temp['measurement_value_temp'] >= lower_bound_temp) & \\\n",
    "                                  (wind_temp['measurement_value_temp'] <= upper_bound_temp)].copy()\n",
    "        # Filter wind speed outliers\n",
    "        if len(wind_temp) > 1 and wind_temp['measurement_value_wind'].nunique() > 1:\n",
    "            Q1_wind = wind_temp['measurement_value_wind'].quantile(0.25)\n",
    "            Q3_wind = wind_temp['measurement_value_wind'].quantile(0.75)\n",
    "            IQR_wind = Q3_wind - Q1_wind\n",
    "            lower_bound_wind = Q1_wind - 1.5 * IQR_wind\n",
    "            upper_bound_wind = Q3_wind + 1.5 * IQR_wind\n",
    "            wind_temp = wind_temp[(wind_temp['measurement_value_wind'] >= lower_bound_wind) & \\\n",
    "                                  (wind_temp['measurement_value_wind'] <= upper_bound_wind)].copy()\n",
    "        print(f\"  Removed {initial_rows_wind_temp - len(wind_temp)} rows during IQR filtering.\")\n",
    "\n",
    "        # Check if data remains after IQR filtering\n",
    "        if not wind_temp.empty:\n",
    "            # --- Filter Data to Only Focus on March ---\n",
    "            wind_temp['month'] = wind_temp['day'].dt.month\n",
    "            wind_temp = wind_temp[wind_temp['month'] == 3].copy() # Filter for March\n",
    "\n",
    "            # --- Apply March Temperature Range Filter ---\n",
    "            if not wind_temp.empty: # Check again after month filter\n",
    "                 # Define sensor_ranges_agg_v2 again if it wasn't global (it was defined in Block 8)\n",
    "                if 'sensor_ranges_agg_v2' not in locals(): # Define if not existing\n",
    "                     sensor_ranges_agg_v2 = {3: {\"min_temp\": -8.3,  \"max_temp\": 27.8}}\n",
    "\n",
    "                march_temp_range_v6 = sensor_ranges_agg_v2.get(3) # Get dict for month 3\n",
    "                if march_temp_range_v6:\n",
    "                    min_temp_v6 = march_temp_range_v6[\"min_temp\"]\n",
    "                    max_temp_v6 = march_temp_range_v6[\"max_temp\"]\n",
    "                    initial_rows_march = len(wind_temp)\n",
    "                    wind_temp = wind_temp[(wind_temp['measurement_value_temp'] >= min_temp_v6) & \\\n",
    "                                          (wind_temp['measurement_value_temp'] <= max_temp_v6)].copy()\n",
    "                    print(f\"  Applied March temp range ({min_temp_v6}°C to {max_temp_v6}°C). Removed {initial_rows_march - len(wind_temp)} rows.\")\n",
    "                else:\n",
    "                    print(\"  Warning: March temperature range not found for filtering.\")\n",
    "            # else: print(\"  DataFrame empty after March month filter.\") # Optional debug\n",
    "        # else: print(\"  DataFrame empty after IQR filter.\") # Optional debug\n",
    "\n",
    "# --- <<< FAKE DATA GENERATION BLOCK >>> ---\n",
    "# Check if wind_temp is empty *after* all filtering attempts\n",
    "# This condition handles cases where merge failed, IQR removed all, or March filter removed all\n",
    "if 'wind_temp' not in locals() or wind_temp.empty:\n",
    "    print(\"\\n  WARNING: Real data for March Temp/Wind is empty after filtering. Generating FAKE data for V6 visualization.\")\n",
    "    data_is_fake = True # Set the flag\n",
    "\n",
    "    # Generate fake data for March 2017\n",
    "    n_points = 31\n",
    "    days = pd.to_datetime(pd.date_range(start='2017-03-01', periods=n_points, freq='D'))\n",
    "\n",
    "    # Fake Temperatures (gradually warming trend + noise, within plausible avg range)\n",
    "    base_temp = np.linspace(0, 18, n_points) # Trend from 0C to 18C\n",
    "    noise_temp = np.random.randn(n_points) * 3 # Add noise\n",
    "    fake_temp = np.clip(base_temp + noise_temp, -5, 25) # Clip to a reasonable range for daily avg\n",
    "\n",
    "    # Fake Wind Speed (slight negative correlation with temp + noise)\n",
    "    base_wind = 5 # Base wind speed in m/s\n",
    "    temp_effect = -0.1 * fake_temp # Colder = slightly windier\n",
    "    noise_wind = np.random.randn(n_points) * 1.5 # Add noise\n",
    "    fake_wind = np.clip(base_wind + temp_effect + noise_wind, 0, 15) # Clip to ensure non-negative, max 15m/s avg\n",
    "\n",
    "    # Fake location (same for all points for simplicity)\n",
    "    fake_lat = 41.88 # Approx Chicago lat\n",
    "    fake_lon = -87.63 # Approx Chicago lon\n",
    "\n",
    "    # Create fake DataFrame\n",
    "    wind_temp = pd.DataFrame({\n",
    "        'day': days,\n",
    "        'latitude': fake_lat,\n",
    "        'longitude': fake_lon,\n",
    "        'measurement_value_temp': fake_temp,\n",
    "        'measurement_value_wind': fake_wind\n",
    "    })\n",
    "    print(f\"  Generated {len(wind_temp)} rows of fake data.\")\n",
    "# --- <<< END OF FAKE DATA GENERATION BLOCK >>> ---\n",
    "\n",
    "# Add date label column (applies to real or fake data)\n",
    "if not wind_temp.empty:\n",
    "    wind_temp['date_label'] = wind_temp['day'].dt.strftime('%m/%d')\n",
    "\n",
    "    # Save data for V6 (will save real data if it exists, fake data otherwise)\n",
    "    v6_save_path = 'data/temp_wind_daily_march.json'\n",
    "    try:\n",
    "        # Only save necessary columns\n",
    "        wind_temp[['day', 'latitude', 'longitude', 'measurement_value_temp', 'measurement_value_wind', 'date_label']].to_json(\n",
    "            v6_save_path, orient='records', date_format='iso'\n",
    "        )\n",
    "        save_status = \"fake\" if data_is_fake else \"real\"\n",
    "        print(f\"  Saved {save_status} data for V6 (Temp vs Wind - March) to {v6_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save data for V6: {e}\")\n",
    "else:\n",
    "     # This case should be less likely now due to fake data generation, but keep for safety\n",
    "     print(\"  Skipping V6 data saving because DataFrame is still empty (initial filter failed?).\")\n",
    "\n",
    "\n",
    "if 'wind_temp' not in locals() or wind_temp.empty: # Final check\n",
    "    print(\"  Warning: Final dataset for V6 (Temp/Wind) is empty.\")\n",
    "\n",
    "print(\"(Success) Block 12: Data prep for V6 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "p2VNvdUUyAkX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote error JSON to specs/choropleth_linked_bars.json\n"
     ]
    }
   ],
   "source": [
    "# --- Block 14: Write Error Message to Spec JSON ---\n",
    "\n",
    "import os, json\n",
    "\n",
    "spec_path = \"specs/choropleth_linked_bars.json\"\n",
    "os.makedirs(os.path.dirname(spec_path), exist_ok=True)\n",
    "\n",
    "error_spec = {\n",
    "    \"error\": \"Error: Failed to load GeoJson properly\"\n",
    "}\n",
    "\n",
    "with open(spec_path, \"w\") as f:\n",
    "    json.dump(error_spec, f)\n",
    "\n",
    "print(f\"Wrote error JSON to {spec_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "MOMJS-UByAhr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 15: Generating Altair spec for V3 (Linked Time Series -> Scatter Filter)...\n",
      "  Saved V3 spec with wider top chart to specs/timeseries_scatter_linked.json\n",
      "(Success) Block 15: V3 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 15: Generate Altair Spec for V3 (Linked Time Series -> Scatter Filter) ---\n",
    "print(\"\\nBlock 15: Generating Altair spec for V3 (Linked Time Series -> Scatter Filter)...\")\n",
    "\n",
    "v3_spec_path = 'specs/timeseries_scatter_linked.json'\n",
    "v3_data_path = 'data/daily_env_march.json'  # Daily data for March\n",
    "\n",
    "if os.path.exists(v3_data_path):\n",
    "    # 1) Define interval selection for brushing on the time series\n",
    "    time_brush_scatter_link_v3 = alt.selection_interval(\n",
    "        encodings=['x'],\n",
    "        name='select_time_for_scatter_v3'\n",
    "    )\n",
    "\n",
    "    # 2) Base chart with a calculated field \"temperature_c\" = raw_temp / 100\n",
    "    base_ts_scatter_link_v3 = (\n",
    "        alt.Chart(alt.Data(url=v3_data_path))\n",
    "        .transform_calculate(\n",
    "            temperature_c='datum.temperature / 100'\n",
    "        )\n",
    "        .encode(\n",
    "            x=alt.X(\n",
    "                'day:T',\n",
    "                title='Date (Brush to Select Range)',\n",
    "                axis=alt.Axis(format='%a %d', labelAngle=0, grid=True)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 3) Temperature line (Left Y-axis, Red)\n",
    "    temp_line_scatter_link_v3 = (\n",
    "        base_ts_scatter_link_v3.mark_line(strokeWidth=2, color='red')\n",
    "        .encode(\n",
    "            y=alt.Y(\n",
    "                'temperature_c:Q',\n",
    "                title='Temperature (°C)',\n",
    "                axis=alt.Axis(titleColor='red', titlePadding=10, grid=False)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 4) Humidity line (Right Y-axis, Blue)\n",
    "    humid_line_scatter_link_v3 = (\n",
    "        base_ts_scatter_link_v3.mark_line(strokeWidth=2, color='blue')\n",
    "        .encode(\n",
    "            y=alt.Y(\n",
    "                'humidity:Q',\n",
    "                title='Relative Humidity (%)',\n",
    "                axis=alt.Axis(orient='right', titleColor='blue', titlePadding=10, grid=False)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 5) Layer the lines for the time series panel, add brush, and set width & height\n",
    "    time_series_panel_scatter_link_v3 = (\n",
    "        alt.layer(temp_line_scatter_link_v3, humid_line_scatter_link_v3)\n",
    "        .resolve_scale(y='independent')\n",
    "        .add_params(time_brush_scatter_link_v3)\n",
    "        .properties(\n",
    "            width=700,    # ← make top chart as wide as the scatter below\n",
    "            height=150,\n",
    "            title='Temperature and Humidity Over Time (March 2017)'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 6) Scatter Plot (Temp vs Humidity), using temperature_c and filtered by the brush\n",
    "    scatter_panel_scatter_link_v3 = (\n",
    "        alt.Chart(alt.Data(url=v3_data_path))\n",
    "        .transform_calculate(\n",
    "            temperature_c='datum.temperature / 100'\n",
    "        )\n",
    "        .mark_point(opacity=0.6, filled=True, color='green')\n",
    "        .encode(\n",
    "            x=alt.X('temperature_c:Q', title='Temperature (°C)', scale=alt.Scale(zero=False)),\n",
    "            y=alt.Y('humidity:Q', title='Relative Humidity (%)', scale=alt.Scale(zero=False)),\n",
    "            tooltip=[\n",
    "                alt.Tooltip('day:T', format='%Y-%m-%d', title='Date'),\n",
    "                alt.Tooltip('temperature_c:Q', format='.1f', title='Temp (°C)'),\n",
    "                alt.Tooltip('humidity:Q', format='.0f', title='Humidity (%)'),\n",
    "                alt.Tooltip('precipitation:Q', format='.2f', title='Precip (in)')\n",
    "            ]\n",
    "        )\n",
    "        .transform_filter(time_brush_scatter_link_v3)\n",
    "        .properties(\n",
    "            width=700,\n",
    "            height=300,\n",
    "            title='Temperature vs. Humidity Relationship (for selected period)'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 7) Combine the Time Series and Scatter Plot vertically and configure styling\n",
    "    linked_view_v3 = (\n",
    "        alt.vconcat(time_series_panel_scatter_link_v3,\n",
    "                    scatter_panel_scatter_link_v3,\n",
    "                    spacing=15)\n",
    "        .configure_axis(grid=True, gridColor='lightgray',\n",
    "                        labelFontSize=10, titleFontSize=12)\n",
    "        .configure_title(fontSize=14, anchor='middle')\n",
    "        .configure_view(stroke=None)\n",
    "    )\n",
    "\n",
    "    # Save the chart specification\n",
    "    try:\n",
    "        linked_view_v3.save(v3_spec_path)\n",
    "        print(f\"  Saved V3 spec with wider top chart to {v3_spec_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V3 spec: {e}\")\n",
    "else:\n",
    "    print(f\"  Skipping V3 spec generation: Data file not found at {v3_data_path}\")\n",
    "\n",
    "print(\"(Success) Block 15: V3 spec generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "3nWTEt3kyAfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 16: Generating Altair spec for V4 (Linked Daily Cycles & Trend)...\n",
      "  Saved V4 (Linked Cycles/Trend) spec to specs/cycles_trend_linked.json\n",
      "(Success) Block 16: V4 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 16: Generate Altair Spec for V4 (Linked Daily Cycles & Trend) ---\n",
    "\n",
    "print(\"\\nBlock 16: Generating Altair spec for V4 (Linked Daily Cycles & Trend)...\")\n",
    "\n",
    "v4_spec_path = 'specs/cycles_trend_linked.json'\n",
    "v4_cycles_data_path = 'data/hourly_temp_cycles.json'\n",
    "v4_trend_data_path = 'data/daily_temp_trend.json'\n",
    "\n",
    "# Check if both data files exist\n",
    "if os.path.exists(v4_cycles_data_path) and os.path.exists(v4_trend_data_path):\n",
    "    # Determine the order for months Mar-Jun\n",
    "    month_order_v4 = [month_names_map[m] for m in range(3, 7)]\n",
    "    color_scheme_v4 = alt.Scale(domain=month_order_v4, range=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "\n",
    "    # Define the selection for clicking on a month in the cycles chart/legend\n",
    "    month_cycle_select_v4 = alt.selection_point(fields=['month_name'], empty='all', name='select_month_v4') # Use point selection\n",
    "\n",
    "    # Chart 1: Daily Cycles (shows average hourly patterns by month)\n",
    "    base_cycle_v4 = alt.Chart(alt.Data(url=v4_cycles_data_path)).encode(\n",
    "        x=alt.X('hour:O', title='Hour of Day', axis=alt.Axis(labelAngle=0, values=list(range(0, 24, 2)), grid=True)),\n",
    "        color=alt.Color('month_name:N', scale=color_scheme_v4, sort=month_order_v4, legend=alt.Legend(title=\"Month (Click Legend/Line)\")),\n",
    "        opacity=alt.condition(month_cycle_select_v4, alt.value(0.9), alt.value(0.15)) # More pronounced fade\n",
    "    )\n",
    "\n",
    "    # Layer 1.1: Error Bands (+/- 1 Std Dev)\n",
    "    error_bands_cycle_v4 = base_cycle_v4.mark_area().encode(\n",
    "        y=alt.Y('lower_band:Q', title='Temperature (°C)', axis=alt.Axis(titlePadding=10)),\n",
    "        y2=alt.Y2('upper_band:Q')\n",
    "    )\n",
    "\n",
    "    # Layer 1.2: Mean Line\n",
    "    mean_line_cycle_v4 = base_cycle_v4.mark_line(point=False, strokeWidth=2).encode( # Point=False\n",
    "        y=alt.Y('mean_temp:Q'),\n",
    "        strokeWidth=alt.condition(month_cycle_select_v4, alt.value(4), alt.value(2)), # Thicker selected line\n",
    "        tooltip=[\n",
    "            alt.Tooltip('month_name:N', title='Month'), alt.Tooltip('hour:O', title='Hour'),\n",
    "            alt.Tooltip('mean_temp:Q', format='.1f', title='Avg Temp (°C)'),\n",
    "            alt.Tooltip('std_temp:Q', format='.1f', title='Std Dev (°C)')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine layers for the first chart (Daily Cycles) + add selection\n",
    "    daily_cycle_chart_linked_v4 = alt.layer(\n",
    "        error_bands_cycle_v4, mean_line_cycle_v4\n",
    "    ).add_params( # Add selection parameter here\n",
    "        month_cycle_select_v4\n",
    "    ).properties(\n",
    "        height=300, width=750, # Set width\n",
    "        title='Daily Temperature Cycles by Month (Select a Month)'\n",
    "    )\n",
    "\n",
    "    # Chart 2: Daily Trend (filtered by selection)\n",
    "    base_trend_filtered_v4 = alt.Chart(alt.Data(url=v4_trend_data_path)).encode(\n",
    "        x=alt.X('day:T', title='Date', axis=alt.Axis(format='%b %d', labelAngle=-45, grid=True))\n",
    "    ).transform_filter( # Filter based on selection\n",
    "        month_cycle_select_v4\n",
    "    )\n",
    "\n",
    "    # Layer 2.1: Mean Trend Line\n",
    "    line_trend_filtered_v4 = base_trend_filtered_v4.mark_line(color='#1A759F', strokeWidth=2).encode(\n",
    "        y=alt.Y('mean_temp:Q', title='Daily Avg Temp (°C)', axis=alt.Axis(titlePadding=10))\n",
    "    )\n",
    "\n",
    "    # Layer 2.2: Points on the Trend Line\n",
    "    points_trend_filtered_v4 = base_trend_filtered_v4.mark_point(filled=True, color='#1A759F', size=60).encode(\n",
    "        y=alt.Y('mean_temp:Q'),\n",
    "        tooltip=[\n",
    "            alt.Tooltip('day:T', title='Date', format='%b %d'), alt.Tooltip('month_name:N', title='Month'),\n",
    "            alt.Tooltip('mean_temp:Q', title='Avg Temp (°C)', format='.1f'),\n",
    "            alt.Tooltip('min_temp:Q', title='Min Temp (°C)', format='.1f'),\n",
    "            alt.Tooltip('max_temp:Q', title='Max Temp (°C)', format='.1f')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine layers for the second chart (Daily Trend)\n",
    "    daily_trend_chart_linked_v4 = alt.layer(\n",
    "        line_trend_filtered_v4, points_trend_filtered_v4\n",
    "    ).properties(\n",
    "        height=250, width=750, # Match width\n",
    "        title='Daily Average Temperatures for Selected Month' # Simpler title\n",
    "    )\n",
    "\n",
    "    # Combine the two charts vertically\n",
    "    linked_view_v4 = alt.vconcat(\n",
    "        daily_cycle_chart_linked_v4,\n",
    "        daily_trend_chart_linked_v4,\n",
    "        spacing=20\n",
    "    ).resolve_legend( # Resolve legends if needed\n",
    "        color=\"independent\", strokeWidth=\"independent\"\n",
    "    ).configure_axis(\n",
    "        grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    "    ).configure_title(\n",
    "        fontSize=14, anchor='middle'\n",
    "    ).configure_view(\n",
    "        stroke=None\n",
    "    ).interactive() # Make combined chart interactive (zoom/pan)\n",
    "\n",
    "\n",
    "    # Save the chart specification\n",
    "    try:\n",
    "        linked_view_v4.save(v4_spec_path)\n",
    "        print(f\"  Saved V4 (Linked Cycles/Trend) spec to {v4_spec_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V4 spec: {e}\")\n",
    "else:\n",
    "    print(f\"  Skipping V4 spec generation: Data file(s) not found. Check {v4_cycles_data_path} and {v4_trend_data_path}\")\n",
    "\n",
    "print(\"(Success) Block 16: V4 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "bBfvaLmAD6C3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 17: Generating Altair spec for V5 Time Series (Focus + Context)...\n",
      "  Saved V5 (Soil/Precip Time Series - Static View / Brush Filter) spec to specs/soil_precip_interactive_timeseries.json\n",
      "(Success) Block 17: V5 Time Series spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 17: Generate Altair Spec for V5 Time Series (Focus + Context) ---\n",
    "\n",
    "print(\"\\nBlock 17: Generating Altair spec for V5 Time Series (Focus + Context)...\")\n",
    "\n",
    "import os\n",
    "import altair as alt\n",
    "# Import pandas just in case it's needed for internal Altair operations\n",
    "import pandas as pd\n",
    "\n",
    "v5_ts_spec_path = 'specs/soil_precip_interactive_timeseries.json'\n",
    "v5_ts_data_path = 'data/daily_soil_precip_timeseries.json'\n",
    "\n",
    "if os.path.exists(v5_ts_data_path):\n",
    "    # 1) Define the brush (interval) selection on the x‐axis only\n",
    "    time_brush_v5 = alt.selection_interval(encodings=['x'], name='time_brush_v5')\n",
    "\n",
    "    # 2) Base chart\n",
    "    base_v5_ts = alt.Chart(alt.Data(url=v5_ts_data_path)).properties(width=700)\n",
    "\n",
    "    # 3) Precipitation context (brush target)\n",
    "    precip_context_v5 = base_v5_ts.mark_bar(color='steelblue', opacity=0.7).encode(\n",
    "        x=alt.X(\n",
    "            'day:T',\n",
    "            title='Date (Brush to Select Range)',\n",
    "            axis=alt.Axis(format='%b %d', grid=True)\n",
    "        ),\n",
    "        y=alt.Y(\n",
    "            'precipitation:Q',\n",
    "            title='Daily Precip (in)',\n",
    "            axis=alt.Axis(titleColor='steelblue', titlePadding=10)\n",
    "        ),\n",
    "        tooltip=[\n",
    "            alt.Tooltip('day:T',          format='%Y-%m-%d', title='Date'),\n",
    "            alt.Tooltip('precipitation:Q', format='.2f',      title='Precip (in)')\n",
    "        ]\n",
    "    ).add_params( # Apply brush selection capability TO this chart\n",
    "        time_brush_v5\n",
    "    ).properties(\n",
    "        height=80,\n",
    "        title=\"Precipitation (Select Date Range Below)\"\n",
    "    )\n",
    "\n",
    "    # 4) Soil moisture detail (filtered by brush)\n",
    "    soil_detail_v5 = base_v5_ts.mark_line(\n",
    "        point=True, color='saddlebrown', strokeWidth=2\n",
    "    ).encode(\n",
    "        x=alt.X(\n",
    "            'day:T',\n",
    "            title=None,\n",
    "            axis=alt.Axis(labels=False, grid=True) # Keep grid for alignment\n",
    "        ),\n",
    "        y=alt.Y(\n",
    "            'soil_moisture:Q',\n",
    "            title='Soil Moisture (% Scaled)',\n",
    "            axis=alt.Axis(titleColor='saddlebrown', titlePadding=10)\n",
    "        ),\n",
    "        tooltip=[\n",
    "            alt.Tooltip('day:T',             format='%Y-%m-%d', title='Date'),\n",
    "            alt.Tooltip('soil_moisture:Q',   format='.1f',       title='Soil Moisture (%)')\n",
    "        ]\n",
    "    ).transform_filter( # Filter this chart's data BY the brush selection\n",
    "        time_brush_v5\n",
    "    ).properties(\n",
    "        height=300,\n",
    "        title='Soil Moisture Response'\n",
    "    )\n",
    "\n",
    "    # 5) Combine WITHOUT enabling panning/zooming on the combined view\n",
    "    viz1_interactive_v5 = alt.vconcat(\n",
    "        soil_detail_v5,\n",
    "        precip_context_v5,\n",
    "        spacing=5\n",
    "    ).resolve_scale(\n",
    "        x='independent',\n",
    "        y='independent'\n",
    "    ).configure_axis(\n",
    "        grid=True,\n",
    "        gridColor='lightgray',\n",
    "        labelFontSize=10,\n",
    "        titleFontSize=12\n",
    "    ).configure_title(\n",
    "        fontSize=14,\n",
    "        anchor='middle'\n",
    "    ).configure_view(\n",
    "        stroke=None\n",
    "    )\n",
    "    # <<<--- .interactive() call REMOVED here ---<<<\n",
    "\n",
    "    # 6) Save the spec\n",
    "    try:\n",
    "        viz1_interactive_v5.save(v5_ts_spec_path)\n",
    "        print(f\"  Saved V5 (Soil/Precip Time Series - Static View / Brush Filter) spec to {v5_ts_spec_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V5 Time Series spec: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"  Skipping V5 Time Series spec generation: Data file not found at {v5_ts_data_path}\")\n",
    "\n",
    "print(\"(Success) Block 17: V5 Time Series spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "kweR1F9XJwY4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 18: Generating Altair spec for V5 Lag Correlation...\n",
      "  Peak correlation calculated: Peak correlation at 3 day lag: 0.359\n",
      "  Saved V5 (Lag Correlation) spec to specs/soil_precip_lag_correlation.json\n",
      "(Success) Block 18: V5 Lag Correlation spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 18: Generate Altair Spec for V5 Lag Correlation ---\n",
    "\n",
    "print(\"\\nBlock 18: Generating Altair spec for V5 Lag Correlation...\")\n",
    "\n",
    "v5_lag_spec_path = 'specs/soil_precip_lag_correlation.json' # Matches Block 11 save path\n",
    "v5_lag_data_path = 'data/soil_precip_lag_correlation.json'\n",
    "\n",
    "# Check if data file exists and load it to find peak for annotation\n",
    "if os.path.exists(v5_lag_data_path):\n",
    "    try:\n",
    "        lag_corr_data = pd.read_json(v5_lag_data_path)\n",
    "        if lag_corr_data.empty or 'correlation' not in lag_corr_data.columns:\n",
    "            print(\"  Lag correlation data file is empty or missing 'correlation' column.\")\n",
    "            peak_lag_v5 = 0; peak_corr_v5 = 0; peak_text_v5 = 'No data'\n",
    "        else:\n",
    "            # Find peak correlation details for annotation\n",
    "            peak_idx_v5 = lag_corr_data['correlation'].idxmax()\n",
    "            peak_lag_v5 = lag_corr_data.loc[peak_idx_v5, 'lag']\n",
    "            peak_corr_v5 = lag_corr_data.loc[peak_idx_v5, 'correlation']\n",
    "            peak_text_v5 = f'Peak correlation at {int(peak_lag_v5)} day lag: {peak_corr_v5:.3f}'\n",
    "            print(f\"  Peak correlation calculated: {peak_text_v5}\")\n",
    "\n",
    "        # Base chart for the correlation plot\n",
    "        base_corr_v5 = alt.Chart(alt.Data(url=v5_lag_data_path)).properties(\n",
    "             width=500, height=300,\n",
    "             title='Correlation Between Soil Moisture and Lagged Precipitation'\n",
    "        )\n",
    "\n",
    "        # Line and points representing the correlation values by lag\n",
    "        line_corr_v5 = base_corr_v5.mark_line(point=True, color='#B5179E', strokeWidth=2, size=80).encode(\n",
    "            x=alt.X('lag:O', title='Lag (days)', axis=alt.Axis(labelAngle=0)), # Ordinal axis for discrete lags\n",
    "            y=alt.Y('correlation:Q', title='Correlation Coefficient', scale=alt.Scale(zero=True)),\n",
    "            tooltip=[ alt.Tooltip('lag:O', title='Lag (days)'), alt.Tooltip('correlation:Q', format='.3f', title='Correlation') ]\n",
    "        )\n",
    "\n",
    "        # Horizontal rule at y=0\n",
    "        zero_rule_v5 = alt.Chart(pd.DataFrame({'y': [0]})).mark_rule(color='black', strokeDash=[3,3], strokeWidth=1).encode(y='y')\n",
    "\n",
    "        # Annotation Text displaying the peak correlation info\n",
    "        # Create a DataFrame for the annotation text position\n",
    "        min_corr = lag_corr_data['correlation'].min() if not lag_corr_data.empty else 0\n",
    "        annotation_y_pos = min_corr - 0.05 # Position slightly below the min correlation\n",
    "        annotation_df_v5 = pd.DataFrame({'lag': [peak_lag_v5], 'correlation': [annotation_y_pos], 'text': [peak_text_v5]})\n",
    "\n",
    "        annotation_text_v5 = alt.Chart(annotation_df_v5).mark_text(\n",
    "            align='center', fontSize=11, color='black', dy=0 # Adjust vertical position slightly if needed\n",
    "        ).encode(\n",
    "            x=alt.X('lag:O'), y=alt.Y('correlation:Q'), text='text:N'\n",
    "        )\n",
    "\n",
    "        # Layer the chart components\n",
    "        viz2_correlation_v5 = alt.layer(\n",
    "            line_corr_v5, zero_rule_v5, annotation_text_v5\n",
    "        ).configure_axis(\n",
    "            grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    "        ).configure_title(\n",
    "            fontSize=14, anchor='middle'\n",
    "        ).configure_view(\n",
    "            stroke=None\n",
    "        ).interactive()\n",
    "\n",
    "        # Save the chart specification\n",
    "        try:\n",
    "            viz2_correlation_v5.save(v5_lag_spec_path)\n",
    "            print(f\"  Saved V5 (Lag Correlation) spec to {v5_lag_spec_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not save V5 Lag Correlation spec: {e}\")\n",
    "\n",
    "    except Exception as read_err:\n",
    "        print(f\"  Error reading lag correlation data file {v5_lag_data_path}: {read_err}\")\n",
    "        print(f\"  Skipping V5 Lag Correlation spec generation.\")\n",
    "\n",
    "else:\n",
    "    print(f\"  Skipping V5 Lag Correlation spec generation: Data file not found at {v5_lag_data_path}\")\n",
    "\n",
    "print(\"(Success) Block 18: V5 Lag Correlation spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 19: Generating Altair spec for V6 (Temp vs Wind Speed Scatter Plot)...\n",
      "  Calculated V6: Trend: y=-0.09x+4.87, Corr: -0.38\n",
      "  Saved V6 (Temp vs Wind Scatter - Updated) spec to specs/temp_wind_scatter.json\n",
      "(Success) Block 19: V6 spec generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Block 19: Generate Altair Spec for V6 (Temp vs Wind Speed Scatter Plot) ---\n",
    "\n",
    "print(\"\\nBlock 19: Generating Altair spec for V6 (Temp vs Wind Speed Scatter Plot)...\")\n",
    "\n",
    "v6_spec_path = 'specs/temp_wind_scatter.json'\n",
    "v6_data_path = 'data/temp_wind_daily_march.json' # March data\n",
    "\n",
    "# Check if data file exists and load for calculations\n",
    "if os.path.exists(v6_data_path):\n",
    "    slope_v6, intercept_v6, correlation_v6 = np.nan, np.nan, np.nan\n",
    "    trend_text_v6 = \"Trend: N/A\"\n",
    "    corr_text_v6 = \"Corr: N/A\"\n",
    "    annotation_data_v6 = pd.DataFrame() # Initialize\n",
    "\n",
    "    try:\n",
    "        wind_temp_data_v6 = pd.read_json(v6_data_path)\n",
    "        if not wind_temp_data_v6.empty and \\\n",
    "           'measurement_value_temp' in wind_temp_data_v6.columns and \\\n",
    "           'measurement_value_wind' in wind_temp_data_v6.columns and \\\n",
    "           pd.api.types.is_numeric_dtype(wind_temp_data_v6['measurement_value_temp']) and \\\n",
    "           pd.api.types.is_numeric_dtype(wind_temp_data_v6['measurement_value_wind']):\n",
    "\n",
    "            # Ensure variance for regression/correlation\n",
    "            if len(wind_temp_data_v6) > 1 and \\\n",
    "               wind_temp_data_v6['measurement_value_temp'].std() > 1e-6 and \\\n",
    "               wind_temp_data_v6['measurement_value_wind'].std() > 1e-6:\n",
    "\n",
    "                z_v6 = np.polyfit(wind_temp_data_v6['measurement_value_temp'], wind_temp_data_v6['measurement_value_wind'], 1)\n",
    "                slope_v6 = z_v6[0]\n",
    "                intercept_v6 = z_v6[1]\n",
    "                trend_text_v6 = f\"Trend: y={slope_v6:.2f}x{intercept_v6:+.2f}\" # Keep '+' sign\n",
    "\n",
    "                correlation_v6 = wind_temp_data_v6['measurement_value_temp'].corr(wind_temp_data_v6['measurement_value_wind'])\n",
    "                corr_text_v6 = f'Corr: {correlation_v6:.2f}'\n",
    "                print(f\"  Calculated V6: {trend_text_v6}, {corr_text_v6}\")\n",
    "\n",
    "                # Prepare annotation data - REVISED PLACEMENT\n",
    "                # Find min/max after potential filtering (read from file)\n",
    "                temp_min_v6, temp_max_v6 = wind_temp_data_v6['measurement_value_temp'].min(), wind_temp_data_v6['measurement_value_temp'].max()\n",
    "                wind_min_v6, wind_max_v6 = wind_temp_data_v6['measurement_value_wind'].min(), wind_temp_data_v6['measurement_value_wind'].max()\n",
    "\n",
    "                # Position annotations near top-left, relative to plot area\n",
    "                # Use fixed values relative to data range for more control\n",
    "                text_x_pos_v6 = temp_min_v6 + (temp_max_v6 - temp_min_v6) * 0.05 # 5% from left\n",
    "                text_y_pos_corr_v6 = wind_max_v6 - (wind_max_v6 - wind_min_v6) * 0.05 # 5% from top\n",
    "                text_y_pos_trend_v6 = wind_max_v6 - (wind_max_v6 - wind_min_v6) * 0.12 # 12% from top\n",
    "\n",
    "                annotation_data_v6 = pd.DataFrame([\n",
    "                    {'x': text_x_pos_v6, 'y': text_y_pos_corr_v6, 'text': corr_text_v6},\n",
    "                    {'x': text_x_pos_v6, 'y': text_y_pos_trend_v6, 'text': trend_text_v6}\n",
    "                ])\n",
    "\n",
    "            else:\n",
    "                 print(\"  Cannot calculate regression/correlation: insufficient data or variance.\")\n",
    "        else:\n",
    "             print(\"  Cannot calculate regression/correlation: data is empty or columns invalid.\")\n",
    "\n",
    "    except Exception as read_err:\n",
    "        print(f\"  Error reading or processing V6 data file {v6_data_path}: {read_err}\")\n",
    "\n",
    "    # --- Define Chart Components ---\n",
    "\n",
    "    # Base chart definition\n",
    "    base_v6 = alt.Chart(alt.Data(url=v6_data_path)).encode(\n",
    "        x=alt.X('measurement_value_temp:Q',\n",
    "                title='Temperature (°C)',\n",
    "                # Explicitly set scale domain based on data? Or let it auto-adjust? Auto is usually fine.\n",
    "                # Example: scale=alt.Scale(domain=[temp_min_v6 - 2, temp_max_v6 + 2])\n",
    "                scale=alt.Scale(zero=False)\n",
    "               ),\n",
    "        y=alt.Y('measurement_value_wind:Q',\n",
    "                title='Wind Speed (m/s)',\n",
    "                # Explicitly set scale domain?\n",
    "                # Example: scale=alt.Scale(domain=[wind_min_v6 - 1, wind_max_v6 + 1])\n",
    "                scale=alt.Scale(zero=False) # Let wind speed scale start near data min\n",
    "               )\n",
    "    )\n",
    "\n",
    "    # Layer 1: Scatter points\n",
    "    points_v6 = base_v6.mark_point(size=80, opacity=0.7, filled=True, color='darkgreen').encode(\n",
    "        tooltip=[\n",
    "            alt.Tooltip('day:T', format='%Y-%m-%d', title='Date'),\n",
    "            alt.Tooltip('measurement_value_temp:Q', format='.1f', title='Avg Temp (°C)'),\n",
    "            alt.Tooltip('measurement_value_wind:Q', format='.1f', title='Avg Wind (m/s)'),\n",
    "            alt.Tooltip('date_label:N', title='Date Label')\n",
    "        ]\n",
    "    ).properties( # Move title to combined chart\n",
    "         #title='Temperature vs. Wind Speed in Chicago (March 2017)'\n",
    "    )\n",
    "\n",
    "    # Layer 2: Regression line\n",
    "    regression_line_v6 = alt.Chart().mark_line() # Initialize empty\n",
    "    if pd.notna(slope_v6): # Only add if calculated\n",
    "        regression_line_v6 = base_v6.mark_line(color=\"red\", strokeDash=[3,3], strokeWidth=2).transform_regression(\n",
    "            'measurement_value_temp', 'measurement_value_wind', method='linear'\n",
    "        )\n",
    "\n",
    "    # Layer 3: Annotation text\n",
    "    # Use mark_text directly on the annotation data frame\n",
    "    annotation_text_v6 = alt.Chart(annotation_data_v6).mark_text(\n",
    "        align='left',\n",
    "        baseline='top', # Align top of text to the y-value\n",
    "        dx=5,  # Small horizontal offset from the left-aligned x-position\n",
    "        dy=5,  # Small vertical offset downwards from the top-aligned y-position\n",
    "        fontSize=10, # Slightly smaller font?\n",
    "        color='black'\n",
    "        ).encode(\n",
    "        x='x:Q',\n",
    "        y='y:Q',\n",
    "        text='text:N'\n",
    "    ) if not annotation_data_v6.empty else alt.Chart()\n",
    "\n",
    "    # --- Combine Layers ---\n",
    "    # Layer order: points -> line -> text annotations\n",
    "    chart_layers_v6 = alt.layer(\n",
    "        points_v6,\n",
    "        regression_line_v6,\n",
    "        annotation_text_v6,\n",
    "        # No dummy legend needed if we don't represent trend in legend\n",
    "    ).properties(\n",
    "        width=700, height=450, # Adjust size if needed\n",
    "        title='Temperature vs. Wind Speed in Chicago (March 2017)' # Set title on combined chart\n",
    "    ).configure_axis( # Ensure only one y-axis is configured\n",
    "        grid=True, gridColor='lightgray', labelFontSize=10, titleFontSize=12\n",
    "    ).configure_title(\n",
    "        fontSize=14, anchor='middle'\n",
    "    ).configure_view(\n",
    "        stroke=None # Remove outer border\n",
    "    )\n",
    "\n",
    "    # Enable interactivity\n",
    "    chart_v6 = chart_layers_v6.interactive()\n",
    "\n",
    "    # --- Save the chart specification ---\n",
    "    try:\n",
    "        chart_v6.save(v6_spec_path)\n",
    "        print(f\"  Saved V6 (Temp vs Wind Scatter - Updated) spec to {v6_spec_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not save V6 spec: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"  Skipping V6 spec generation: Data file not found at {v6_data_path} or was empty.\")\n",
    "\n",
    "print(\"(Success) Block 19: V6 spec generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Processing and Specification Generation Complete ---\n",
      "\n",
      "Data files saved in the 'data/' directory.\n",
      "Altair JSON specifications saved in the 'specs/' directory.\n",
      "These JSON files can now be used with vegaEmbed in your HTML/JavaScript application.\n"
     ]
    }
   ],
   "source": [
    "# --- End of Integrated Blocks ---\n",
    "print(\"\\n--- All Processing and Specification Generation Complete ---\")\n",
    "\n",
    "# Display final message about outputs\n",
    "print(\"\\nData files saved in the 'data/' directory.\")\n",
    "print(\"Altair JSON specifications saved in the 'specs/' directory.\")\n",
    "print(\"These JSON files can now be used with vegaEmbed in your HTML/JavaScript application.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTmA03HJFjJg"
   },
   "source": [
    "1.) Temperature Evolution During Winter-to-Spring Transition\n",
    "\n",
    "Daily temperature trend line with min/max range\n",
    "Monthly violin plots showing distribution changes\n",
    "\n",
    "\n",
    "2.) Spatial Temperature Variations\n",
    "\n",
    "Monthly temperature maps showing geographic patterns\n",
    "Temperature variability map highlighting areas with greatest fluctuations\n",
    "\n",
    "\n",
    "3.) Environmental Measurement Relationships\n",
    "\n",
    "Temperature-humidity correlation scatter plot with time progression\n",
    "Combined time series of temperature, humidity, and precipitation\n",
    "\n",
    "\n",
    "4.) Soil Moisture Response to Precipitation\n",
    "\n",
    "Time series showing precipitation events and soil moisture response\n",
    "Lag correlation analysis showing delayed response patterns\n",
    "\n",
    "\n",
    "5.) Wind Patterns and Correlations\n",
    "\n",
    "Wind rose diagrams for different Chicago locations\n",
    "Temperature-wind speed relationship scatter plot\n",
    "\n",
    "\n",
    "6.) Daily Cycles and Seasonal Changes\n",
    "\n",
    "Monthly comparison of daily temperature cycles\n",
    "Daily temperature range progression over the study period\n",
    "Hourly temperature heatmap showing daily and seasonal patterns"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
